<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<meta name="DC.Type" content="topic" />
<meta name="DC.Title" content="HPE Express Containers with Docker Enterprise Edition on HPE SimpliVity" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="prodname" content="HPE Enterprise Containers as a Service with Docker Enterprise Edition" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="version" content="1.0" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="copyright" content="HPE 2018" type="primary" />
<meta name="DC.Rights.Owner" content="HPE 2018" type="primary" />
<meta name="DC.Format" content="XHTML" />
<meta name="DC.Identifier" content="enterprise-caas" />
<link rel="stylesheet" type="text/css" href="commonltr.css" />
</head>
<body>
<div class="nested0" aria-labelledby="ariaid-title1" id="enterprise-caas">
<h1 class="title topictitle1" id="ariaid-title1"><span class="ph"><span class="keyword">HPE Express Containers with Docker Enterprise Edition on HPE SimpliVity</span></span></h1>

<div class="body">
<ul class="ul">

<li class="li"><a class="xref" href="#introduction">Executive Summary</a>

</li>


<li class="li"><a class="xref" href="#release-notes-syn-mvi2">Release Notes</a>
<ul class="ul">
<li class="li"><a class="xref" href="#playbook-updates-svt">Playbook updates</a></li>

<li class="li"><a class="xref" href="#configuration-updates">Configuration updates</a></li>

<li class="li"><a class="xref" href="#fixed-release-svt">Fixed in this release</a></li>



</ul>

</li>


<li class="li"><a class="xref" href="#architecture">Solution overview</a>
<ul class="ul">
<li class="li"><a class="xref" href="#solution-config">Solution configuration</a>
<ul class="ul">
<li class="li"><a class="xref" href="#svt-config">HPE SimpliVity configuration</a></li>

<li class="li"><a class="xref" href="#linux-config">Linux-only VM configuration</a></li>

<li class="li"><a class="xref" href="#hybrid-config">Hybrid VM configuration (Windows and Linux)</a></li>

</ul>

</li>

<li class="li"><a class="xref" href="#ha">High availability</a></li>

<li class="li"><a class="xref" href="#sizing">Sizing considerations</a></li>

<li class="li"><a class="xref" href="#disaster-recovery">Disaster Recovery</a></li>

<li class="li"><a class="xref" href="#security">Security</a></li>

</ul>

</li>


<li class="li"><a class="xref" href="#solution-components">Solution components</a>

<ul class="ul">
<li class="li"><a class="xref" href="#hardware-svt">Hardware</a>
<ul class="ul">
<li class="li"><a class="xref" href="#simplivity">About HPE SimpliVity</a></li>

</ul>

</li>

<li class="li"><a class="xref" href="#software">Software</a>
<ul class="ul">
<li class="li"><a class="xref" href="#about-ansible">About Ansible</a></li>

<li class="li"><a class="xref" href="#docker-ee">About Docker Enterprise Edition</a></li>

</ul>

</li>


<li class="li"><a class="xref" href="#application-software">Application software</a>
<ul class="ul">
<li class="li"><a class="xref" href="#architecture-splunk-sysdig">Monitoring with Splunk and Sysdig</a></li>

<li class="li"><a class="xref" href="#architecture-promgraf">Monitoring with Prometheus and Grafana</a></li>

</ul>

</li>

</ul>

</li>


<li class="li"><a class="xref" href="#provisioning">Preparing the environment</a>
<ul class="ul">


<li class="li"><a class="xref" href="#verify-prereqs">Verify prerequisites</a></li>

<li class="li"><a class="xref" href="#vsphere-ha">Enable vSphere High Availability (HA)</a></li>

<li class="li"><a class="xref" href="#install-vdvs">Install vSphere Docker Volume Service driver on all ESXi hosts</a></li>

<li class="li"><a class="xref" href="#create-ansible-node">Create the Ansible node</a></li>

<li class="li"><a class="xref" href="#create-rhel-template">Create the Red Hat Linux template</a></li>






</ul>

</li>


<li class="li"><a class="xref" href="#edit-group-var">Configuring the core components</a>
<ul class="ul">
<li class="li"><a class="xref" href="#ansible-config">Ansible configuration</a></li>

<li class="li"><a class="xref" href="#edit-inventory">Editing the inventory</a></li>

<li class="li"><a class="xref" href="#vmware-config">VMware configuration</a></li>

<li class="li"><a class="xref" href="#svt-variables-config">HPE SimpliVity configuration</a></li>

<li class="li"><a class="xref" href="#svt-backup-config">HPE SimpliVity backup configuration</a></li>

<li class="li"><a class="xref" href="#network-config">Networking configuration</a></li>

<li class="li"><a class="xref" href="#environment-config">Environment configuration</a></li>

<li class="li"><a class="xref" href="#docker-config">Docker configuration</a></li>

<li class="li"><a class="xref" href="#orchestrator-config">Orchestrator configuration</a></li>

<li class="li"><a class="xref" href="#k8s-config">Kubernetes configuration</a>
<ul class="ul">
<li class="li"><a class="xref" href="#k8s-persistent-volume-config">Kubernetes Persistent Volume configuration</a></li>

</ul>

</li>

<li class="li"><a class="xref" href="#edit-vault">Protecting sensitive information</a></li>

<li class="li"><a class="xref" href="#inventory-group-variables">Inventory group variables</a></li>

</ul>

</li>


<li class="li"><a class="xref" href="#playbooks-overview">Overview of the playbooks</a>
<ul class="ul">
<li class="li"><a class="xref" href="#core-infrastructure">Core components</a></li>

<li class="li"><a class="xref" href="#optional-playbooks">Optional components</a></li>

<li class="li"><a class="xref" href="#backup-restore-playbooks">Backup and restore playbooks</a></li>

<li class="li"><a class="xref" href="#convenience-playbooks">Convenience playbooks</a></li>

<li class="li"><a class="xref" href="#convenience-scripts">Convenience scripts</a></li>

</ul>

</li>


<li class="li"><a class="xref" href="#running-playbooks">Deploying the core components</a>
<ul class="ul">
<li class="li"><a class="xref" href="#create-rhel-vms">Provisioning RHEL VMs</a></li>

<li class="li"><a class="xref" href="#unique_2072037671">Provisioning load balancers for UCP and DTR</a></li>

<li class="li"><a class="xref" href="#install-docker-ucp-dtr">Installing Docker UCP and DTR on RHEL VMs</a></li>

<li class="li"><a class="xref" href="#add-rhel-workers">Deploying RHEL workers</a></li>

<li class="li"><a class="xref" href="#svt-backup-playbooks">HPE SimpliVity backup playbooks</a></li>

</ul>

</li>


<li class="li"><a class="xref" href="#deploying-windows-workers">Deploying Windows workers</a>
<ul class="ul">
<li class="li"><a class="xref" href="#create-template-win">Create the Windows Template</a></li>

<li class="li"><a class="xref" href="#add-windows-workers">Playbooks for adding Windows workers</a></li>

<li class="li"><a class="xref" href="#windows-config">Windows configuration</a></li>

<li class="li"><a class="xref" href="#windows-proxy">Deploying Windows workers behind a proxy</a></li>

<li class="li"><a class="xref" href="#lifecycle-windows">Windows operating system and Docker EE</a></li>

</ul>

</li>


<li class="li"><a class="xref" href="#deploying-sysdig">Deploying Sysdig monitoring</a>
<ul class="ul">
<li class="li"><a class="xref" href="#sysdig">Monitoring with Sysdig</a></li>

<li class="li"><a class="xref" href="#install-sysdig">Playbooks for installing Sysdig on RHEL</a></li>

<li class="li"><a class="xref" href="#monitoring-config-sysdig">Sysdig configuration</a></li>

<li class="li"><a class="xref" href="#sysdig-trial">Registering for Sysdig trial</a></li>

<li class="li"><a class="xref" href="#deploying-sysdig-k8s">Deploying Sysdig monitoring on Kubernetes</a></li>

<li class="li"><a class="xref" href="#deploying-sysdig-swarm">Deploying Sysdig monitoring on Docker Swarm</a></li>

</ul>

</li>


<li class="li"><a class="xref" href="#deploying-splunk">Deploying Splunk</a>
<ul class="ul">
<li class="li"><a class="xref" href="#splunk">Monitoring with Splunk</a></li>

<li class="li"><a class="xref" href="#install-splunk">Playbooks for installing Splunk</a></li>

<li class="li"><a class="xref" href="#monitoring-config-splunk">Splunk configuration</a></li>

<li class="li"><a class="xref" href="#splunk-ui">Accessing Splunk UI</a></li>

<li class="li"><a class="xref" href="#redeploying-splunk-demo">Redeploying Splunk demo</a></li>

</ul>

</li>


<li class="li"><a class="xref" href="#prometheus-grafana">Deploying Prometheus and Grafana on Docker RHEL nodes</a>
<ul class="ul">
<li class="li"><a class="xref" href="#promgraf-overview">Monitoring with Prometheus and Grafana</a></li>

<li class="li"><a class="xref" href="#prometheus-grafana-playbooks">Playbooks for installing Prometheus and Grafana</a></li>

<li class="li"><a class="xref" href="#monitoring-config-promgraf">Prometheus and Grafana configuration</a></li>

<li class="li"><a class="xref" href="#using-promgraf-swarm">Accessing Grafana UI</a></li>

</ul>

</li>


<li class="li"><a class="xref" href="#post-deployment">Post deployment</a>
<ul class="ul">
<li class="li"><a class="xref" href="#install-kubectl">Installing kubectl</a></li>

<li class="li"><a class="xref" href="#k8s-example-guestbook">Kubernetes guestbook example with Redis</a></li>

<li class="li"><a class="xref" href="#k8s-nfs-provisioner">Deploying the NFS provisioner for Kubernetes</a>
<ul class="ul">
<li class="li"><a class="xref" href="#k8s-nfs-provisioner-vm">Using NFS VM for post-deployment verification</a></li>


</ul>

</li>

</ul>

</li>







<li class="li"><a class="xref" href="#backup-restore">Backup and restore</a>
<ul class="ul">
<li class="li"><a class="xref" href="#backup-restore-ucp-dtr">Backup and restore UCP and DTR</a></li>


<li class="li"><a class="xref" href="#backup-restore-docker-volumes-svt">Backup and restore Docker persistent volumes with HPE SimpliVity</a></li>


</ul>

</li>


<li class="li"><a class="xref" href="#lifecycle">Solution lifecycle management</a>
<ul class="ul">
<li class="li"><a class="xref" href="#lifecycle-svt">HPE SimpliVity environment</a></li>

<li class="li"><a class="xref" href="#lifecycle-vdvs">vSphere Docker Volume Service Plug-in</a></li>

<li class="li"><a class="xref" href="#lifecycle-rhel">Red Hat Enterprise Linux operating system</a></li>

<li class="li"><a class="xref" href="#lifecycle-docker-ee">Docker EE Environment</a></li>

<li class="li"><a class="xref" href="#lifecycle-monitoring">Monitoring Tools</a></li>

</ul>

</li>


<li class="li"><a class="xref" href="#summary">Summary</a></li>


<li class="li"><a class="xref" href="#bom">Appendix A</a></li>

<li class="li"><a class="xref" href="#customer-certs">Appendix B: Using customer supplied certificates for UCP and DTR</a></li>

<li class="li"><a class="xref" href="#splunk-ssl">Appendix C: Enabling SSL between the universal forwarders and the Splunk indexers using your certificates</a></li>

<li class="li"><a class="xref" href="#post-deployment-check-certs">Appendix D: How to check that certs were deployed correctly</a></li>

<li class="li"><a class="xref" href="#resources-links">Resources and additional links</a></li>

</ul>


</div>

</div>
<div class="topic nested0" aria-labelledby="ariaid-title2" id="introduction">
<h1 class="title topictitle1" id="ariaid-title2">Executive Summary</h1>

<div class="body">
    
 
    
<div class="section" id="introduction__introduction-section">
<p class="p"><span class="ph"><span class="keyword">HPE Express Containers with Docker Enterprise Edition on HPE SimpliVity</span></span> is a
complete solution from Hewlett Packard Enterprise that includes all the hardware,
software, professional services, and support you need to deploy a
Containers-as-a-Service (CaaS) platform, allowing you to get up and running quickly
and efficiently. The solution takes <span class="ph"><span class="keyword">HPE SimpliVity</span></span> infrastructure and combines it
with Docker’s enterprise-grade container platform, popular open source tools, along
with deployment and advisory services from HPE Pointnext. </p>


<p class="p"><span class="ph"><span class="keyword">HPE Express Containers with Docker Enterprise Edition on HPE SimpliVity</span></span> is ideal for customers
migrating legacy applications to containers, transitioning to a container DevOps
development model or needing a hybrid environment to support container and
non-containerized applications on a common VM platform. This Reference Configuration
provides a solution for IT operations, addressing the need 
for a production-ready environment that is easy to deploy and manage.  </p>

    
<p class="p">This release supports Kubernetes via Docker Enterprise Edition (EE) 2.0, which is the only platform that manages and secures applications on Kubernetes 
  in multi-Linux, multi-OS and multi-cloud customer environments. This document describes the best practices for deploying and operating HPE Enterprise
Containers as a Service with Docker Enterprise Edition (EE). It shows how to automate the
provisioning of the environment using a set of Ansible playbooks. It also outlines a
set of manual steps to harden, secure and audit the overall status of the system. </p>


</div>

</div>

<div class="topic nested1" aria-labelledby="ariaid-title3" id="audience-assumptions">
<h2 class="title topictitle2" id="ariaid-title3"></h2>

<div class="body">
<p class="p"><strong class="ph b">Target Audience: </strong>This document is primarily aimed at technical individuals working in the
operations side of the software pipeline, such as infrastructure architects, system administrators and infrastructure engineers,
but anybody with an interest in automating the provisioning of virtual servers and containers
may find this document useful. 
</p>

<p class="p"><strong class="ph b">Assumptions: </strong>The present document assumes a minimum
understanding in concepts such as virtualization and containerization and also some knowledge
around Linux®, Microsoft Windows® and VMware® technologies.</p>

</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title4" id="release-notes-syn-mvi2">
<h1 class="title topictitle1" id="ariaid-title4">Release Notes</h1>

<div class="body">
    
<div class="section"><h2 class="title sectiontitle">Critical security issue in the Kubernetes API Server</h2>
    

<p class="p">To address <a class="xref" href="https://nvd.nist.gov/vuln/detail/CVE-2018-1002105" target="_blank">CVE-2018-1002105</a>, 
    a critical security issue in the Kubernetes API Server, 
    Docker is using a custom build of Kubernetes 1.8.15 for UCP 3.0.7.</p>

    
<p class="p">It is recommened that you specify <code class="ph codeph">ucp_version: 3.0.7</code> in your Docker configuration - 
    see the section <a class="xref" href="#docker-config">Docker configuration</a> for more information.</p>



</div>
    
    
<div class="section"><h2 class="title sectiontitle">New features</h2>


<p class="p">This release of the <span class="ph"><span class="keyword">Docker-SimpliVity</span></span> playbooks is  based on the recent release of the corresponding 
<span class="ph"><span class="keyword">Docker-Synergy</span></span> playbooks with some specific additions:
</p>
 
    
<ul class="ul">
<li class="li"><strong class="ph b">Load balancers:</strong> The playbooks now support load balancers based on VRRP, using <code class="ph codeph">HAproxy</code> and <code class="ph codeph">keepalived</code>.
The solution can be deployed using these loadbalancers, external load balancers, no load balancers or 
the legacy version of standalone load balancers. 
</li>
  
    
<li class="li"><strong class="ph b">Default storage driver:</strong> The default storage driver is now <code class="ph codeph">overlay2</code>, as recommended by Docker. Previously, the
<code class="ph codeph">devicemapper</code> was configured in <code class="ph codeph">direct-lvm</code> mode.</li>
    
    
<li class="li"><strong class="ph b">Windows workers behind proxy:</strong> Windows worker nodes can now be deployed behind a proxy. Simple HTTP proxy support is provided and the playbooks 
do not support any type of authentication with the proxy (on either RHEL or Windows nodes).</li>
    

<li class="li"><strong class="ph b">Windows boot drive:</strong> The OS partition of Windows worker nodes is now resized to occupy all the boot disk, whereas, previously, there would have been some
unallocated space.
</li>

</ul>
    
</div>

    
<div class="section"><h2 class="title sectiontitle">Features taken from <span class="ph"><span class="keyword">Docker-Synergy</span></span> release</h2>
  
    
<p class="p">The principal features inherited from this release include:</p>


<ul class="ul">
<li class="li">Docker 2.0 with Kubernetes 1.8</li>
    
<li class="li">Splunk support for monitoring Kubernetes logs and metrics</li>

<li class="li">Sysdig support for monitoring Kubernetes clusters</li>
  
<li class="li">Updated versions of Prometheus and Grafana</li>
    
    
</ul>



</div>
    
    
    
  
</div>

<div class="topic nested1" aria-labelledby="playbook-updates-svt__changes-playbooks-svt" id="playbook-updates-svt">
<h2 class="title topictitle2" id="playbook-updates-svt__changes-playbooks-svt">Playbook updates</h2>
  
<div class="body">
 
<p class="p">An overview of all the playbooks is available <a class="xref" href="#playbooks-overview">here</a>. This section outlines the changes 
    from the previous release of the playbooks.</p>

 
<p class="p">The playbooks that were available previously as part of the <code class="ph codeph">Dev Edition</code> have been removed in this release, leaving a 
single codebase that currently focuses on operations functionality. Some of the development-specific stacks may be reinstated in a future release.</p>

<p class="p"></p>
 
    
<p class="p">Windows-specific playbooks have now been merged into the core, so running <code class="ph codeph">site.yml</code> will automatically install
Windows workers, if they are declared in the inventory. Non-core playbooks such as those for the monitoring stacks will automatically install
any required Windows software.
</p>
    
    
<p class="p">The playbooks that are specific to <span class="ph"><span class="keyword">HPE SimpliVity</span></span> and related to backup have been redesigned:</p>

    
<ul class="ul">
<li class="li"><code class="ph codeph">playbooks/config_dummy_vms_for_docker_volumes_backup.yml</code> now uses the same mechanism as the 
    <code class="ph codeph">playbooks/create_vms.yml</code> to deploy the dummy VM.</li>
   
    
<li class="li"><code class="ph codeph">playbooks/config_simplivity_backups.yml</code> has been refactored to be more modular, with separate plays for
creating backup policies, populating policies with rules, and assigning backup policies to VMs. The reliability of the 
playbook has been improved by minimizing the number of concurrent operations to the least number of nodes possible. The playbook 
creates the policies based on the <code class="ph codeph">backup_policies</code> data structure in the <code class="ph codeph">group_vars/vars</code> file.
Any exisiting policies will not be over-written when running the playbook, but rules attached to the policies will be updated
based on the information supplied in the variables file. If you omit the <code class="ph codeph">node_policy</code> variable for a group of nodes,
 no policy will be assigned by default.
</li>

</ul>

       
    
<p class="p"><code class="ph codeph">playbooks/config_storage_driver.yml</code> prepares drives for local Docker volumes and container images
    and configures the Docker storage driver based on the value of the <code class="ph codeph">docker_storage_driver</code> variable
    in <code class="ph codeph">group_vars/vars</code>.
</p>
    
  
<p class="p">The <code class="ph codeph">scale_worker.sh</code> utility script has been updated to support the new load balancers as well as the legacy ones.</p>
  
</div>

</div>
<div class="topic nested1" aria-labelledby="configuration-updates__changes-mvi2-configuration" id="configuration-updates">
<h2 class="title topictitle2" id="configuration-updates__changes-mvi2-configuration">Configuration updates</h2>
  
<div class="body">
<p class="p">New variables and configuration files introduced in this release of  <span class="ph"><span class="keyword">Docker-SimpliVity</span></span>:</p>


<ul class="ul">
<li class="li"><code class="ph codeph">docker_ee_reponame</code> variable in <code class="ph codeph">group_vars/vars</code> replaces <code class="ph codeph">docker_ee_version</code> - see <a class="xref" href="#docker-config">Docker configuration</a> </li>

<li class="li"><code class="ph codeph">docker_ee_version_windows</code> variable in <code class="ph codeph">group_vars/vars</code> - see <a class="xref" href="#windows-config">Windows configuration</a></li>
 
<li class="li"><code class="ph codeph">docker_storage_driver</code> variable in <code class="ph codeph">group_vars/vars</code> - see <a class="xref" href="#docker-config">Docker configuration</a> </li>
    
<li class="li"><code class="ph codeph">windows_update</code> variable in <code class="ph codeph">group_vars/vars</code> - see <a class="xref" href="#windows-config">Windows configuration</a></li>
 
<li class="li"><code class="ph codeph">windows_winrm_script</code> variable in <code class="ph codeph">group_vars/vars</code> - see <a class="xref" href="#windows-config">Windows configuration</a></li>

</ul>

    
    

<p class="p">Recently added variables and configuration files inherited from the underlying <span class="ph"><span class="keyword">Docker-Synergy</span></span> release:</p>

<ul class="ul">
<li class="li"><code class="ph codeph">splunk_uf_password</code> variable in <code class="ph codeph">group_vars/vault</code> - see <a class="xref" href="#splunk-pre-deploy">Splunk prerequisites</a></li>
  
<li class="li"><code class="ph codeph">orchestrator</code> variable in <code class="ph codeph">vm_hosts</code> - see <a class="xref" href="#orchestrator-config">Orchestrator configuration</a></li>
  
<li class="li"><code class="ph codeph">k8s_pod_cidr</code> variable in <code class="ph codeph">group_vars/vars</code> - see <a class="xref" href="#k8s-config">Kubernetes configuration</a></li>
 
<li class="li">Additional configuration files for each group in the inventory including <code class="ph codeph">group_vars/vms.yml</code>,
<code class="ph codeph">group_vars/ucp.yml</code>, <code class="ph codeph">group_vars/dtr.yml</code>, <code class="ph codeph">group_vars/worker.yml</code> 
and <code class="ph codeph">group_vars/nfs.yml</code> - see <a class="xref" href="#inventory-group-variables">Inventory group variables</a>.
</li>

</ul>



</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title7" id="fixed-release-svt">
<h2 class="title topictitle2" id="ariaid-title7">Fixed in this release</h2>

<div class="body">
<p class="p">Port 12388 is now opened on UCP nodes to facilitate access to the Kubernetes API from withing the cluster.</p>

  
<p class="p">When deploying the Splunk Universal Forwarder on Windows worker nodes, port 1514 is now opened on logger VM, as opposed to
previously where the playbook incorrectly opened it on the Ansible box.</p>

  
  
</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title8" id="architecture">
<h1 class="title topictitle1" id="ariaid-title8">Solution overview</h1>

<div class="body">

<p class="p">The <span class="ph"><span class="keyword">HPE Express Containers with Docker Enterprise Edition on HPE SimpliVity</span></span> solution consists of a set of Ansible
            playbooks that run on top of a VMware virtualization platform on <span class="ph"><span class="keyword">HPE SimpliVity</span></span> hardware. The solution allows you to configure
            a flexible OS environment (with both RHEL and Windows workers) providing built-in high
            availability (HA), container monitoring and security, and backup and restore
            functionality. </p>




<div class="section">



<div class="fig fignone" id="architecture__d23e17">

<br /><img class="image" src="svt/../media/overview-graphic-svt.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 1. </b></span>Solution overview</span></div><p> </p></div>
  
    
<p class="p"><a class="xref" href="svt/architecture-svt.html#architecture-svt__overview-graphic">Figure 1</a> provides an overview of the steps used to deploy the solution. 
   Deploying your hardware and <span class="ph"><span class="keyword">HPE SimpliVity</span></span> is specific to your environment and is not covered here.
   This document shows you how to:    
</p>
  
</div>

   
<ul class="ul">
<li class="li">Prepare the VM templates</li>

<li class="li">Create the Ansible host</li>

<li class="li">Configure the Ansible parameters</li>

<li class="li">Run the Ansible playbooks</li>
        
</ul>
    
 
<p class="p">Once you are up and running, you should regularly backup the system using the scripts provided as part of this solution.</p>
 
 
</div>

<div class="topic nested1" aria-labelledby="ariaid-title9" id="solution-config">
<h2 class="title topictitle2" id="ariaid-title9">Solution configuration</h2>

<div class="body">
    
</div>

<div class="topic nested2" aria-labelledby="ariaid-title10" id="svt-config">
<h3 class="title topictitle3" id="ariaid-title10"><span class="ph"><span class="keyword">HPE SimpliVity</span></span> configuration</h3>

<div class="body">
<p class="p">The Ansible playbooks are available to download at 
<span class="ph"><a class="xref" href="https://github.com/HewlettPackard/Docker-SimpliVity" target="_blank"><span class="keyword">https://github.com/HewlettPackard/Docker-SimpliVity</span></a></span>. 
By default, the  playbooks  are configured  to set up a 3 node environment. This is the 
minimal starter configuration recommended by HPE and Docker for 
production. 
</p>


<p class="p">The Operations environment is comprised of three <span class="ph"><span class="keyword">HPE SimpliVity</span></span> 380 Gen10 servers. 
    HPE recommends dual socket <span class="ph"><span class="keyword">HPE SimpliVity</span></span> systems with at least 14 CPU cores per socket (28 total cores per system) 
    for optimal performance and support during HA failover scenarios. Since the <span class="ph"><span class="keyword">HPE SimpliVity</span></span> technology 
    relies on VMware virtualization, the servers are managed using vCenter. 

</p>

    
</div>

</div>
<div class="topic nested2" aria-labelledby="ariaid-title11" id="linux-config">
<h3 class="title topictitle3" id="ariaid-title11">Linux-only VM configuration</h3>
    
<div class="body">  

<ul class="ul">
<li class="li">3 Docker Universal Control Plane (UCP) VM nodes for HA and cluster management </li>

<li class="li">3 Docker Trusted Registry (DTR) VM nodes for HA of the container registry </li>

</ul>


<p class="p">The Docker UCP and DTR nodes are spread across 3 physical nodes, with one on each physical node. An odd number of manager nodes is
recommended to avoid split-brain issues. It is possible to restrict the deployment to 1 UCP and 1 DTR, or to expand to more than 3, but the recommended minimum for 
an enterprise production deployment is 3 UCPs and 3 DTRs.
</p>


<ul class="ul">  
<li class="li">3 Docker Linux worker VM nodes for container workloads - Kubernetes or Docker swarm or a mix</li>

</ul>


<p class="p">The Docker worker nodes will be co-located with the UCP and DTR nodes in a 3 physical node deployment. 
Where more than 3 physical nodes are available, the worker nodes will typically be separated onto the extra nodes. 
It is possible to specify that more than one worker node is deployed per physical node but 
this decision will depend on the requirements of your applications.</p>


<ul class="ul">  
<li class="li">1 Docker UCP load balancer VM to ensure access to UCP in the event of a node
failure </li>

<li class="li">1 Docker DTR load balancer VM to ensure access to DTR in the event of a node
failure </li>


</ul>


<p class="p">By default, two load balancers are deployed to increase availability of UCP and DTR and these are placed on separate physical nodes.
    Load balancing for applications running on worker nodes can achieved by using the playbooks to deploy
additional load balancers, or by manually configuring the existing two to support your applications in addition to supporting UCP and DTR. 
</p>


<ul class="ul">  
<li class="li">1 Logging server VM for central logging </li>

<li class="li">1 NFS server VM for storage of Docker DTR images </li>

</ul>


<p class="p">With the addition of the NFS and logging VMs, a total of 13 VMs are created for the default Linux-only deployment.  In addition to these VMs, the playbooks also set up the Docker persistent storage plug-in from VMware.  
The vSphere Docker volume plug-in  facilitates the storage of data in a shared datastore that can be accessed from any machine in the cluster.</p>





</div>

</div>
<div class="topic nested2" aria-labelledby="ariaid-title12" id="hybrid-config">
<h3 class="title topictitle3" id="ariaid-title12">Hybrid VM configuration (Windows and Linux)</h3>
    
<div class="body">
    
<ul class="ul">
<li class="li">3 Docker swarm Windows worker VM nodes for container workloads (optional). Kubernetes is not yet supported for Windows workers. </li>
     
</ul>

   
<p class="p">The hybrid deployment will typically add 3 Windows worker nodes to the above configuration, co-located with the Linux workers.  </p>
   

<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> 
Some of the application 
    software supported by this configuration does not currently run on Windows, for example, the Sysdig Software Agent (see the 
    section <a class="xref" href="#sysdig">Monitoring with Sysdig</a>).    
</div><hr />

    

</div>

</div>
</div>
<div class="topic nested1" aria-labelledby="ariaid-title13" id="ha">
<h2 class="title topictitle2" id="ariaid-title13">High availability</h2>

<div class="body">
<p class="p">Uptime is paramount for  businesses implementing Docker containers in business critical
environments. The <span class="ph"><span class="keyword">HPE Express Containers with Docker Enterprise Edition on HPE SimpliVity</span></span> solution offers various levels of high
availability (HA) to support continuous availability. The Docker EE system components 
run on multiple manager nodes in the cluster. The management plane continues to 
operate even in the event of a manager node failure. Application containers can be 
protected through the use of <code class="ph codeph">services</code> running on top of swarm. The swarm orchestrator 
works to maintain the number of containers declared as part of the service. 
The Ansible playbooks can be modified to fit your environment and
your high availability (HA) needs.</p>

<p class="p"></p>

</div>


<div class="topic nested2" aria-labelledby="ariaid-title14" id="load-balancers">
<h3 class="title topictitle3" id="ariaid-title14">Load Balancers</h3>


<div class="body">
<p class="p">This solution also deploys load balancers in the system to help with
container traffic management. There are two load balancer VMs – the UCP load balancer and
DTR load balancer. The playbooks can be configured to deploy one or more worker load
balancers depending on the requirements of your applications. A typical load balancer architecture for
applications running on Docker EE is shown in <a class="xref" href="#load-balancers__lbs">Figure 2</a>. 
The playbooks now support load balancers based on VRRP, using <code class="ph codeph">HAproxy</code> and <code class="ph codeph">keepalived</code>.
The solution can be deployed using these loadbalancers, or external load balancers, or no load balancers or 
the legacy version of standalone load balancers.             
</p>

    

<div class="fig fignone" id="load-balancers__lbs">


<img class="image" src="media/load-balancers.png" />

<div><span class="figcap"><span class="fig--title-label"><b>Figure 2. </b></span>Load balancer architecture</span></div><p> </p></div>



</div>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title15" id="sizing">
<h2 class="title topictitle2" id="ariaid-title15">Sizing considerations</h2>

<div class="body">
<p class="p">A node is a machine in the cluster (virtual or physical) with Docker Engine running on
it. There are two types of nodes: managers and workers. UCP will run on the manager nodes. 
Although DTR runs on a worker node, Docker does not recommend running other 
application containers on these nodes.</p>

<p class="p">To decide what size the node should be in terms of CPU, RAM, and storage resources,
consider the following:</p>

<ol class="ol">
<li class="li">All nodes should at least fulfil the minimal requirements, for UCP 3.0, 8GB of RAM
and 3GB of storage. For production systems, 16GB of RAM is recommended for manager nodes. 
More detailed requirements are in the Docker EE UCP documentation
 at <a class="xref" href="https://docs.docker.com/ee/ucp/admin/install/system-requirements/" target="_blank">https://docs.docker.com/ee/ucp/admin/install/system-requirements/</a>. </li>

<li class="li">UCP controller nodes should be provisioned with more than the minimal requirements if other workloads run on them. </li>

<li class="li">Ideally, worker node size will vary based on your workloads so it is impossible to
define a universal standard size. </li>

<li class="li">Other considerations like target density (average number of containers per node),
whether one standard node type or several are preferred, and other operational
considerations might also influence sizing. </li>

</ol>


<p class="p">If possible, node size should be determined by experimentation and testing actual
workloads; and they should be refined iteratively. A good starting point is to select a
standard or default machine type in your environment and use this size only. If your
standard machine type provides more resources than the UCP controller nodes need, it makes
sense to have a smaller node size for these. Whatever the starting choice, it is
important to monitor resource usage and cost to improve the model.</p>



<p class="p">For this solution, the following tables describe
sizing configurations, assuming 3 Linux workers and 3 Windows workers. The vCPU allocations are described in <a class="xref" href="#sizing__vcpu-table-conref">Table 1</a> while the memory
allocation is described in <a class="xref" href="#sizing__memory-alloc-table-conref">Table 2</a>.</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="sizing__vcpu-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 1. </span>vCPU</span></caption><colgroup><col /><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e3520">vCPUs</th>
<th class="entry nocellnorowborder" align="center" valign="top" id="d29e3523">node01</th>
<th class="entry nocellnorowborder" align="center" valign="top" id="d29e3526">node02</th>
<th class="entry nocellnorowborder" align="center" valign="top" id="d29e3529">node03</th>
</tr>
</thead><tbody class="tbody">

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">ucp1</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">4</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">ucp2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">4</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">ucp3</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">4</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">dtr1</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">dtr2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">

</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">

</td>
</tr>


<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">dtr3</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">

</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">

</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">2</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">worker1</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">4</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 "> </td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">worker2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">

</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">4</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">

</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">worker3</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">

</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">
<p class="p"></p>

</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">4</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">win-worker1</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">4</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 "> </td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">win-worker2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">4</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 "> </td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">win-worker3</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">4</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">lb1</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">2
</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">

</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">

</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">lb2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">
</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">2
</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">

</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">nfs
</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">

</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">

</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">2
</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">logger
</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">

</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">2
</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">
 
</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3520 ">
Total vCPU per node
</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3523 ">
16
</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3526 ">
18
</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3529 ">
16
</td>
</tr>
</tbody></table>
</div>



<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> 
<p class="p">In the case of one ESX host failure, 2 nodes are enough to accommodate the amount
of vCPU required.</p>

</div><hr />


<p class="p"></p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="sizing__memory-alloc-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 2. </span>Memory allocation</span></caption><colgroup><col /><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e3812">RAM (GB)</th>
<th class="entry nocellnorowborder" align="center" valign="top" id="d29e3815">node01</th>
<th class="entry nocellnorowborder" align="center" valign="top" id="d29e3818">node02</th>
<th class="entry nocellnorowborder" align="center" valign="top" id="d29e3821">node03</th>
</tr>
</thead><tbody class="tbody">		

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">ucp1</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 ">16</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">ucp2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>							
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 ">16</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">ucp3</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 ">16</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">dtr1</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 ">16</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">dtr2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 ">16</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">dtr3</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 ">16</td>
</tr>



<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">lb1</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 ">4</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">lb2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 ">4</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>



<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">nfs</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 ">4</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">logger</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 ">4</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>


<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">worker1</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 ">64</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">worker2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 ">64</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">worker3</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 ">64</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">win-worker1</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 ">64</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">win-worker2</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 ">64</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "> </td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 ">win-worker3</td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "> </td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 ">64</td>
</tr>


<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 "><strong class="ph b">Total RAM required (per node)</strong></td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "><strong class="ph b">164</strong></td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "><strong class="ph b">168</strong></td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "><strong class="ph b">164</strong></td>
</tr>



<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e3812 "><strong class="ph b">Available RAM</strong></td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3815 "><strong class="ph b">384</strong></td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3818 "><strong class="ph b">384</strong></td>
<td class="entry nocellnorowborder" align="center" valign="top" headers="d29e3821 "><strong class="ph b">384</strong></td>
</tr>
</tbody></table>
</div>



<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> 
<p class="p">In the case of one ESX host failure, the two surviving hosts can accommodate the
amount of RAM required for all VMs.</p>

</div><hr />




 
</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title16" id="disaster-recovery">
<h2 class="title topictitle2" id="ariaid-title16">Disaster Recovery</h2>

<div class="body">
    
<p class="p">Recovery Time Objective (RTO) refers to the time that it takes to recover your data and
            applications while Recovery Point Objective (RPO) refers to the point in time you can
            recover to, in the event of a disaster. In essence, RPO tells you how often you will
            need to make new backups. </p>

    
<p class="p">In order to protect your installation from disasters, you need to take regular backups and 
transfer the backups to a safe location. This solution provides
a range of convenience scripts and Ansible playbooks to help automate the backup of UCP, DTR, your swarm and your Docker volumes.
See the section <a class="xref" href="#backup-restore">Backup and restore</a> for best practices, procedures and utilities for
implementing disaster recovery.
</p>




</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title17" id="security">
<h2 class="title topictitle2" id="ariaid-title17">Security</h2>

<div class="body">
<p class="p">The Docker Reference architecture for Securing Docker EE and Security Best Practices is
available at <a class="xref" href="https://success.docker.com/article/Docker_Reference_Architecture-_Securing_Docker_EE_and_Security_Best_Practices" target="_blank">https://success.docker.com/article/Docker_Reference_Architecture-_Securing_Docker_EE_and_Security_Best_Practices</a>.</p>


<p class="p">In addition to having all logs centralized in a single place and the image scanning
feature enabled for the DTR nodes, there are other guidelines that should be followed in
order to keep your Docker environment as secure as possible. The HPE Reference
Configuration paper for securing Docker on HPE Hardware places a special emphasis on
securing Docker in DevOps environments and covers best practices in terms of Docker
security. The document can be found at <a class="xref" href="http://h20195.www2.hpe.com/V2/GetDocument.aspx?docname=a00020437enw" target="_blank">http://h20195.www2.hpe.com/V2/GetDocument.aspx?docname=a00020437enw</a>.</p>

    
<p class="p">In addition, the Sysdig product also provides a strong level of container security and monitoring (see section  <a class="xref" href="#sysdig">Monitoring with Sysdig</a>).</p>
    
</div>



</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title18" id="solution-components">
<h1 class="title topictitle1" id="ariaid-title18">Solution components</h1>

<div class="body">
<p class="p">This section describes the various components that were utilized in this Reference Configuration.</p>

</div>

<div class="topic nested1" aria-labelledby="ariaid-title19" id="hardware-svt">
<h2 class="title topictitle2" id="ariaid-title19">Hardware</h2>

<div class="body">
</div>

<div class="topic nested2" aria-labelledby="ariaid-title20" id="simplivity">
<h3 class="title topictitle3" id="ariaid-title20">About <span class="ph"><span class="keyword">HPE SimpliVity</span></span></h3>

<div class="body">
<p class="p"><span class="ph"><span class="keyword">HPE SimpliVity</span></span> is an enterprise-grade hyper-converged platform uniting best-in-class data services 
  with the world's best-selling server.</p>


<p class="p">Rapid proliferation of applications and the increasing cost of maintaining legacy infrastructure
      causes significant IT challenges for many organizations. With <span class="ph"><span class="keyword">HPE SimpliVity</span></span>, you can streamline and enable IT operations at a
      fraction of the cost of traditional and public cloud solutions, by combining your IT
      infrastructure and advanced data services into a single, integrated solution. <span class="ph"><span class="keyword">HPE SimpliVity</span></span> is a powerful, simple, and efficient hyperconverged
      platform that joins best-in-class data services with the world’s best-selling server and
      offers the industry’s most complete guarantee. </p>

  
<p class="p">More information about <span class="ph"><span class="keyword">HPE SimpliVity</span></span> can be found at: <a class="xref" href="https://www.hpe.com/us/en/integrated-systems/simplivity.html" target="_blank">https://www.hpe.com/us/en/integrated-systems/simplivity.html</a></p>


</div>

</div>
</div>
<div class="topic nested1" aria-labelledby="ariaid-title21" id="software">
<h2 class="title topictitle2" id="ariaid-title21">Software</h2>

<div class="body">
<p class="p">The software components used in this Reference Configuration are listed in 
  <a class="xref" href="#software__software-3rd-party-table-content">Table 3</a>  and 
  <a class="xref" href="#software__software-hpe-table-content">Table 4</a>. 
</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="software__software-3rd-party-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 3. </span>Third-party software</span></caption><colgroup><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e4251">Component</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e4254">Version</th>
</tr>
</thead><tbody class="tbody">

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4251 ">Ansible</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4254 ">2.7</td>  
</tr> 

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4251 ">Docker EE</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4254 ">2.0 with Docker EE Engine 17.06 <p class="p">(tested with UCP 3.0.6 and DTR 2.5.6)</p>
</td>  
</tr> 
  
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4251 ">Red Hat® Enterprise Linux</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4254 ">7.5, 7.6</td>  
</tr>   

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4251 ">Microsoft® Windows</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4254 ">Server 2016</td>  
</tr> 

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4251 ">VMware®</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4254 ">ESXi 6.5.0 and vCenter 6.5.0</td>  
</tr> 


</tbody></table>
</div>




<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="software__software-hpe-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 4. </span>HPE Software</span></caption><colgroup><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e4326">Component</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e4329">Version</th>
</tr>
</thead><tbody class="tbody">

  
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4326 "><span class="ph"><span class="keyword">HPE SimpliVity</span></span> OmniStack</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4329 ">3.7.6</td>
</tr>
  
  

  

  
</tbody></table>
</div>


</div>

<div class="topic nested2" aria-labelledby="about-ansible__about-ansible-title" id="about-ansible">
<h3 class="title topictitle3" id="about-ansible__about-ansible-title">About Ansible</h3>

<div class="body">
<p class="p">Ansible is an open-source automation engine that automates software provisioning,
configuration management and application deployment.</p>


<p class="p">As with most configuration management software, Ansible has two types of servers: the
controlling machine and the nodes. A single controlling machine orchestrates the nodes
by deploying modules to the Linux nodes over SSH. The modules are temporarily stored on the
nodes and communicate with the controlling machine through a JSON protocol over the
standard output. When Ansible is not managing nodes, it does not consume resources
because no daemons or programs are executing for Ansible in the background. Ansible uses
one or more inventory files to manage the configuration of the multiple nodes in the
system.</p>

    
<p class="p">When deploying Windows nodes in a hybrid deployment, the Ansible playbooks make use 
    of the Python <code class="ph codeph">pywinrm</code> module which carries out actions via the Windows remote manager. </p>
    



<p class="p">More information about Ansible can be found at <a class="xref" href="http://docs.ansible.com/" target="_blank">http://docs.ansible.com</a>.</p>

</div>

</div>
<div class="topic nested2" aria-labelledby="ariaid-title23" id="docker-ee">
<h3 class="title topictitle3" id="ariaid-title23">About Docker Enterprise Edition</h3>

<div class="body">
<p class="p">Docker Enterprise Edition (EE) is the leading enterprise-ready container 
platform for IT that manages and secures diverse applications across disparate
infrastructure, both on-premises and in the cloud. Docker EE provides integrated
container management and security from development to production. Enterprise-ready
capabilities like multi-architecture orchestration and secure software supply chain give
IT teams the ability to manage and secure containers without breaking the developer
experience.</p>


<p class="p">Docker EE provides:</p>


<ul class="ul">
<li class="li">Integrated management of all application resources from a single web admin UI.</li>

<li class="li">Frictionless deployment of applications and Compose files to production in a few clicks.</li>

<li class="li">Multi-tenant system with granular role-based access control (RBAC) and LDAP/AD
integration.</li>

<li class="li">Self-healing application deployment with the ability to apply rolling
application updates.</li>

<li class="li">End-to-end security model with secrets management, image signing and image
security scanning.</li>

</ul>


<p class="p">More information about Docker Enterprise Edition can be found at <a class="xref" href="https://www.docker.com/enterprise-edition" target="_blank">https://www.docker.com/enterprise-edition</a>.</p>

</div>

</div>
</div>
<div class="topic nested1" aria-labelledby="ariaid-title24" id="application-software">
<h2 class="title topictitle2" id="ariaid-title24">Application software</h2>

<div class="body">
<p class="p">A number of different logging and monitoring solutions are supported by this solution:</p>


<ul class="ul">
<li class="li">Splunk</li>

<li class="li">Sysdig</li>

<li class="li">Prometheus and Grafana</li>

</ul>

    
<p class="p">The application software components used in this Reference Configuration are listed in 
  <a class="xref" href="#application-software__application-software-table-content">Table 5</a>.
</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="application-software__application-software-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 5. </span>Application software</span></caption><colgroup><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e4475">Component</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e4478">Version</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4475 ">Splunk</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4478 ">7.1.2</td>  
</tr> 

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4475 ">Sysdig</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4478 ">latest</td>  
</tr> 

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4475 ">Prometheus</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4478 ">v2.3.2</td>  
</tr> 
  
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4475 ">Grafana</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4478 ">5.2.3</td>  
</tr>   


</tbody></table>
</div>

    
    
</div>

<div class="topic nested2" aria-labelledby="ariaid-title25" id="architecture-splunk-sysdig">
<h3 class="title topictitle3" id="ariaid-title25">Monitoring with Splunk and Sysdig</h3>

<div class="body">
<p class="p">The solution can be configured to  use either Splunk or Sysdig or to enable both simultaneously. 
While there is some overlap in the functionality provided by these tools, they are ultimately  complimentary 
in what they offer. Splunk  aggregates logging and tracing for a wide variety of sources and provides a clean, high-level dashboard 
for all your enterprise systems. Sysdig, on the other hand, has been engineered from the ground up to focus on containerized
environments and includes both monitoring and security features, with built-in understanding of the different workloads running
on your cloud.
</p>

    
<p class="p">More information on configuring Splunk and running the relevant playbooks can be found in the section <a class="xref" href="#deploying-splunk">Deploying Splunk</a>.
</p>

  
<p class="p">For more information on configuring Sysdig and running the relevant playbooks, see the section <a class="xref" href="#deploying-sysdig">Deploying Sysdig monitoring</a>.
</p>
  

</div>

</div>
<div class="topic nested2" aria-labelledby="ariaid-title26" id="architecture-promgraf">
<h3 class="title topictitle3" id="ariaid-title26">Monitoring with Prometheus and Grafana</h3>

<div class="body">
<p class="p">The solution can be configured to enable the use of Prometheus and Grafana for monitoring.
In this setup, there is no need for native installs and all the required monitoring software runs in containers,
deployed as either services or stacks. See the section <a class="xref" href="#prometheus-grafana">Deploying Prometheus and Grafana on Docker RHEL nodes</a> for more information on configuring 
and deploying the software.
</p>



</div>

</div>
</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title27" id="provisioning">
<h1 class="title topictitle1" id="ariaid-title27">Preparing the environment</h1>

<div class="body">
<div class="section" id="provisioning__provisioning-section">
<p class="p">This section describes in detail how to prepare the environment that was outlined
in the architecture section. The following high level steps are required:</p>

    
<ul class="ul">
<li class="li">Verify prerequisites</li>
 
<li class="li">Enable vSphere High Availability (HA)</li>

<li class="li">Install vSphere Docker Volume Service driver on all ESXi hosts</li>

<li class="li">Create the Ansible node</li>
   
<li class="li">Create the Red Hat Linux Template and configure the <code class="ph codeph">yum</code> repositories</li>

<li class="li">Create the Windows Template (optional)</li>

<li class="li">Finalize the template</li>

</ul>



    
    
</div>

</div>

<div class="topic nested1" aria-labelledby="ariaid-title28" id="verify-prereqs">
<h2 class="title topictitle2" id="ariaid-title28">Verify prerequisites</h2>

<div class="body">
<p class="p">Before you start deployment, you must assemble the information required to assign values for each and every variable
used by the playbooks. The variables are fully documented in the section <a class="xref" href="#edit-group-var">Configuring the core components</a>.
A brief overview of the information required is presented in 
<a class="xref" href="#verify-prereqs__verify-prereqs-table-conref">Table 6</a>.</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="verify-prereqs__verify-prereqs-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 6. </span>Summary of information required</span></caption><colgroup><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e4658">Component </th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e4661">Details </th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4658 ">Virtual Infrastructure </td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4661 ">The FQDN of your vCenter server and the name of the Datacenter. You
will also need administrator credentials in order to create templates
and spin up virtual machines. </td>
</tr>
    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4658 "><span class="ph"><span class="keyword">HPE SimpliVity</span></span> Cluster </td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4661 ">The name of the <span class="ph"><span class="keyword">HPE SimpliVity</span></span> cluster and the names of the members of this cluster as 
    they appear in vCenter. You will also need to know the name of the <span class="ph"><span class="keyword">HPE SimpliVity</span></span> datastore where 
    you want to land the various virtual machines. You may have to create this datastore if you 
    just installed your <span class="ph"><span class="keyword">HPE SimpliVity</span></span> cluster. In addition, you will need the IP addresses of the 
    OmniStack virtual machines. Finally you will need credentials with admin capabilities for using the 
    OmniStack API. These credentials are typically the same as your vCenter administrator credentials.
</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4658 ">L3 Network requirements </td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4661 ">You will need one IP address for each and every VM configured in the
Ansible inventory (see the section <a class="xref" href="#edit-group-var">Configuring the core components</a>). The recommended
minimal deployment (Linux-only) configures 13 virtual machines so you
would need to allocate 13 IP addresses to use this example inventory.
You will also need to allocate at least 2 additional addresses for the virtual IPs used by the UCP and DTR loadbalancers.
If you have a hybrid environment with Windows workers, you will need to increase the allocation based on the number of workers.
Note that <strong class="ph b">the Ansible playbooks do not support DHCP</strong> so you need static
IP addresses. All the IPs should be in the same subnet. You will also
have to specify the size of the subnet (for example /22 or /24) and the
L3 gateway for this subnet. </td>
</tr>
    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4658 ">DNS </td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4661 ">You will need to know the IP addresses of your DNS server. In
addition, all the VMs you configure in the inventory must have their
names registered in DNS prior to deployment. In addition, you will need to know the domain name to
use for configuring the virtual machines (such as <code class="ph codeph">example.com</code>)
</td>
</tr>
    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4658 ">NTP Services </td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4661 ">You need time services configured in your environment. The deployed solution
uses certificates that are time-sensitive. You will need to specify the IP addresses of your time servers
(NTP). </td>
</tr>
    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4658 ">RHEL Subscription </td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4661 ">A RHEL subscription is required to pull extra packages that are not
on the DVD. </td>
</tr>
    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4658 ">Docker Prerequisites </td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4661 ">You will need a URL for the official Docker EE software download and
a license file. Refer to the Docker documentation to learn more about
this URL and the licensing requirements at: <a class="xref" href="https://docs.docker.com/engine/installation/linux/docker-ee/rhel/" target="_blank">https://docs.docker.com/engine/installation/linux/docker-ee/rhel/</a>
in the section entitled “Docker EE repository URL” </td>
</tr>
    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4658 ">Proxy </td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e4661 ">The playbooks pull the Docker packages from the Internet. If your
environment accesses the Internet through a proxy, you will need the
details of the proxy including the fully qualified domain name and the
port number. </td>
</tr>
    
</tbody></table>
</div>




</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title29" id="vsphere-ha">
<h2 class="title topictitle2" id="ariaid-title29">Enable vSphere High Availability (HA)</h2>

<div class="body">
<p class="p">You must enable vSphere High Availability (HA) to support virtual machine failover during
a HA event such as a host failure. Sufficient CPU and memory resources must be reserved
across the cluster so that all VMs on the affected host(s) can fail over to remaining
available hosts in the cluster. You configure an Admission Control Policy (ACP) to
specify the percentage CPU and memory to reserve on all the hosts in the cluster to
support HA functionality.</p>



<p class="p">More information on enabling vSphere HA and configuring Admission Control Policy is available in 
    the <span class="ph"><span class="keyword">HPE SimpliVity</span></span> documentation. Log in to the HPE Support Center at 
    <a class="xref" href="https://www.hpe.com/us/en/support.html" target="_blank">https://www.hpe.com/us/en/support.html</a> 
    and search for “<span class="ph"><span class="keyword">HPE SimpliVity</span></span> 380”. The administration guide is listed when you select the User document type.    
</p>


<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> 
<p class="p">You should not use the default Admission Control Policy. Instead, you should
calculate the memory and CPU requirements that are specific to your environment.</p>

</div><hr />

<p class="p"></p>

    
</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title30" id="install-vdvs">
<h2 class="title topictitle2" id="ariaid-title30">Install vSphere Docker Volume Service driver on all ESXi hosts</h2>

<div class="body">
    
<p class="p">vSphere Docker Volume Service technology enables stateful containers to access the
storage volumes. Setting this up is a one-off manual step. In order to be able to use Docker
volumes using the vSphere driver, you must first install the latest release of the
vSphere Docker Volume Service (vDVS) driver, which is available as a vSphere
Installation Bundle (VIB). To perform this operation, log in to each of the ESXi hosts and then
download and install the latest release of vDVS driver.</p>


<pre class="pre codeblock"><code># esxcli software vib install -v /tmp/vmware-esx-vmdkops-&lt;version&gt;.vib --no-sig-check</code></pre>

<p class="p">More information on how to download and install the driver can be found at <a class="xref" href="http://vmware.github.io/vsphere-storage-for-docker/documentation/install.html" target="_blank">http://vmware.github.io/vsphere-storage-for-docker/documentation/install.html</a>.
The version of the driver tested in this configuration is 0.21.2.
</p>


<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> 
<p class="p">You cannot mount the same persistent volume created through vSphere Docker Volume
Service (vDVS) on containers running on two different hosts at the same time.</p>

</div><hr />


</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title31" id="create-ansible-node">
<h2 class="title topictitle2" id="ariaid-title31">Create the Ansible node</h2>

<div class="body">

<p class="p">The Ansible node will act as the driver to automate the provisioning of the
environment and it is essential that it is properly installed.</p>



<ol class="ol">
<li class="li">
<p class="p">Create a Virtual Machine and install your preferred OS (in this example, and for the
sake of simplicity, RHEL 7 will be used). The rest of the instructions assume that,
if you use a different OS, you understand the possible differences in syntax for the
provided commands. If you use RHEL 7, select <code class="ph codeph">Infrastructure Server</code> as the base
environment and the <code class="ph codeph">Guests Agents</code> add-on during the installation.</p>

</li>
   
<li class="li">
<p class="p">Log in to the <code class="ph codeph">root</code> account and create an SSH key pair. Do not protect the key with
a passphrase (unless you want to use <code class="ph codeph">ssh-agent</code>). 
</p>


<pre class="pre codeblock"><code># ssh-keygen</code></pre> 
</li>
 

<li class="li"><p class="p">Configure the following yum repositories, <code class="ph codeph">rhel-7-server-rpms</code> and
<code class="ph codeph">rhel-7-server-extras-rpms</code> as explained in <a class="xref" href="#config-yum">Configure the yum repositories</a>.

The "extras" repo can be enabled as follows:
</p>


<pre class="pre codeblock"><code># subscription-manager repos --enable=rhel-7-server-extras-rpms</code></pre>
</li>



<li class="li"><p class="p">Configure the EPEL repository. For more information, see: <a class="xref" href="http://fedoraproject.org/wiki/EPEL" target="_blank">http://fedoraproject.org/wiki/EPEL</a>. Note that <code class="ph codeph">yum-config-manager</code> comes
with the Infrastructure Server base environment. If you did not select this
environment, you will have to install the <code class="ph codeph">yum-utils</code> package.
</p>


<pre class="pre codeblock"><code># rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm 
# yum-config-manager --enable rhel-7-server-extras-rpms</code></pre>    
</li>
 

<li class="li"><p class="p">Install Ansible 2.7 or higher. </p>


<pre class="pre codeblock"><code># yum install ansible</code></pre>    
</li>
 


<li class="li"><p class="p">Install the following packages which are a mandatory requirement for the playbooks
to function as expected. (Update <code class="ph codeph">pip</code> if requested).
</p>


<pre class="pre codeblock"><code># yum install python-pyvmomi python-netaddr python2-jmespath python-pip gcc python-devel openssl-devel git 
# pip install --upgrade pip 
# pip install cryptography 
# pip install pysphere 
# pip install --ignore-installed "pywinrm&gt;=0.2.2"</code></pre>
</li>


</ol>


</div>

<div class="topic nested2" aria-labelledby="ariaid-title32" id="config-yum">
<h3 class="title topictitle3" id="ariaid-title32">Configure the yum repositories</h3>

<div class="body">
    
<p class="p">The Red Hat packages required during the deployment of the solution come from two
repositories: <code class="ph codeph">rhel-7-server-rpms </code>and <code class="ph codeph">rhel-7-server-extras-rpms</code>. The first repository is on the Red Hat DVD
but the second is not. There are two options, with both options requiring a Red Hat
Network account. Logon to your VM template using SSH with the credentials you
configured for the root account and then implement one of the two options below: </p>


<p class="p"><strong class="ph b">Option 1:</strong> Use Red Hat subscription manager to register your system. This is the
easiest way and will automatically give you access to the official Red Hat repositories.
Use the <code class="ph codeph">subscription-manager register</code> command as follows. </p>



<pre class="pre codeblock"><code># subscription-manager register --auto-attach</code></pre>


<p class="p">If you are behind a proxy, you must configure this before running the above command
to register.
</p>


<pre class="pre codeblock"><code># subscription-manager config --server.proxy_hostname=&lt;proxy IP&gt; --server.proxy_port=&lt;proxy port&gt;</code></pre>

<p class="p">
<span class="ph">Verify that you don't have the issue described here: <a class="xref" href="https://access.redhat.com/solutions/3317671" target="_blank">https://access.redhat.com/solutions/3317671</a> by entering the following
command.</span>
</p>


<pre class="pre codeblock"><code># yum repolist</code></pre>

<p class="p">If you have the issue, fix it with the following command</p>


<pre class="pre codeblock"><code># subscription-manager repos --disable=rhel-7-server-rt-beta-rpms</code></pre>


<p class="p">The playbooks will later automatically enable the <code class="ph codeph">extras</code> repository on
the VMs that need it.</p>


<p class="p"><strong class="ph b">Option 2:</strong> Use an internal repository. Instead of pulling the packages from
Red Hat, you can create copies of the required repositories on a dedicated node. You
can then configure the package manager to pull the packages from the dedicated node.
Your <code class="ph codeph">/etc/yum.repos.d/redhat.repo</code> could look as follows.
</p>


<pre class="pre codeblock"><code>
[RHEL7-Server]
name=Red Hat Enterprise Linux $releasever - $basearch
baseurl=http://yourserver.example.com/rhel-7-server-rpms/
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release

[RHEL7-Server-extras]
name=Red Hat Enterprise Linux Extra pkg $releasever - $basearch
baseurl=http://yourserver.example.com/rhel-7-server-extras-rpms/
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
</code></pre>


<p class="p">To see how you can create a local mirror of the Red Hat
repositories and how to share them, check the Red Hat documentation at <a class="xref" href="https://access.redhat.com/solutions/23016" target="_blank">https://access.redhat.com/solutions/23016</a> and at 
<a class="xref" href="https://access.redhat.com/solutions/7227" target="_blank">https://access.redhat.com/solutions/7227</a>.

</p>




</div>

</div>
</div>
<div class="topic nested1" aria-labelledby="ariaid-title33" id="create-rhel-template">
<h2 class="title topictitle2" id="ariaid-title33">Create the Red Hat Linux template</h2>

<div class="body">
    
<p class="p">To create the Red Hat Linux VM template that you will use as
the base for all your nodes, you  first create
a Virtual Machine with the OS installed and then convert the Virtual Machine to a VM
Template. The VM Template is created as lean as possible, with any additional
software installs and/or system configuration performed subsequently using Ansible.
</p>

    
<p class="p">As the creation of the template is
a one-off task, this procedure has not been automated. The steps required to manually create a VM template
 are outlined below.</p>



<p class="p">Log in to vCenter and create a new Virtual Machine with the following
characteristics:
</p>

    
<ul class="ul">
<li class="li">Guest OS Family: Linux, Guest OS Version: Red Hat Enterprise Linux (64-bit) </li>

<li class="li">Hard Disk size: 50GB, (Thin provisioning) </li>

<li class="li">A single network controller connected to the network or VLAN of your choice.
All VMs will connect to this same network. </li>

<li class="li">Optionally you can remove the floppy drive </li>

</ul>



<p class="p">Install Red Hat Enterprise 7:</p>

    
<ol class="ol">
    
<li class="li">Select a language which is supported by Docker </li>

    
<li class="li">For the software selection, choose <strong class="ph b">Infrastructure Server</strong> as the base
environment and add the <strong class="ph b">Guest Agents</strong> from the lists of add-ons
available for this environment. The Infrastructure Server environment is
selected here versus the Minimal Install because customization of Linux
guest operating systems requires that Perl is installed in the Linux guest
operating system. </li>

    
<li class="li">Configure the network settings so that you can later access the VM using
SSH. Specify an IP address for the network interface, a default gateway,
DNS settings and possibly any HTTP/HTTPS proxies that apply in your environment. </li>

    
<li class="li">Specify a password for the root account and optionally created an admin user. </li>

    
<li class="li">Wait for the installation to finish and for the VM to reboot. </li>

    
</ol>

    


</div>

<div class="topic nested2" aria-labelledby="ariaid-title34" id="finalize-template">
<h3 class="title topictitle3" id="ariaid-title34">Finalize the template</h3>

<div class="body">
    
<p class="p">Log in to the <code class="ph codeph">root</code> account on the Ansible box and
copy the SSH public key to the VM Template. This will allow your Ansible node to SSH
to all the Virtual Machines created from the VM Template without the need for a password.</p>


<pre class="pre codeblock"><code>ssh-copy-id root@&lt;IP of your VM_Template&gt;</code></pre>


<p class="p">Perform the following steps on the VM Template to finalize its
creation:</p>


<ol class="ol">
<li class="li">Clean up the template by running the following commands from the <strong class="ph b">Virtual
Machine Console:</strong>
<pre class="pre codeblock"><code>
# rm /etc/ssh/ssh_host_*
# nmcli con del ens192
# logrotate -f /etc/logrotate.conf
# rm /var/log/*-201?*
# history -c		
</code></pre>

</li>


<li class="li">Shutdown the VM
<pre class="pre codeblock"><code># shutdown -h now</code></pre>				
</li>


<li class="li">Turn the VM into a template by right-clicking on your VM and selecting <code class="ph codeph">Template -&gt; Convert to Template</code>.	
This will create a new template visible under VM Templates in Folders, ready for future use.						
</li>


</ol>



<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> In both the Ansible node and the VM Template, you might need to configure
the network so one node can reach the other. Instructions for this step have been
omitted since it is a basic step and could vary depending on the user’s environment.</div><hr />


</div>

</div>
</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title35" id="edit-group-var">
<h1 class="title topictitle1" id="ariaid-title35">Configuring the core components</h1>

<div class="body">
<p class="p">Once you have prepared your environment, you need to download the solution software and 
edit the configuration variables to match your setup.
</p>

</div>



<div class="topic nested1" aria-labelledby="ariaid-title36" id="ansible-config">
<h2 class="title topictitle2" id="ariaid-title36">Ansible configuration</h2>

<div class="body">
<ol class="ol">    
<li class="li"><p class="p">On the Ansible node, retrieve the latest version of the playbooks using Git.</p>



<pre class="pre codeblock"><code># git clone <span class="ph"><span class="keyword">https://github.com/HewlettPackard/Docker-SimpliVity</span></span>.git</code></pre>
</li>

    
<li class="li"><p class="p">Change to the directory that you just cloned:</p>


<pre class="pre codeblock"><code># cd ~/<span class="ph"><span class="keyword">Docker-SimpliVity</span></span></code></pre>
</li>

    
 
</ol>


<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> All subsequent file names are relative to the <code class="ph codeph"><span class="ph"><span class="keyword">Docker-SimpliVity</span></span></code> directory. For example
<code class="ph codeph">vm_hosts</code> is located in the top-level <code class="ph codeph">~/<span class="ph"><span class="keyword">Docker-SimpliVity</span></span></code> while
<code class="ph codeph">group_vars/vars</code> corresponds to
<code class="ph codeph">~/<span class="ph"><span class="keyword">Docker-SimpliVity</span></span>/groups_vars/vars</code>.</div><hr />



<p class="p">You now need to prepare the configuration to match your own environment, prior to
deploying Docker EE and the rest of the nodes. To do so, you will need to
modify a number of files including:</p>

<ul class="ul">
<li class="li"><code class="ph codeph">site.yml</code>, the main entry point for the playbooks.</li>
   
<li class="li"><code class="ph codeph">vm_hosts</code>, the inventory file.</li>

</ul>


<p class="p">You also need to create and populate a number of files: </p>

<ul class="ul">   
<li class="li"><code class="ph codeph">group_vars/vars</code>, the group variables file.</li>

<li class="li"><code class="ph codeph">group_vars/vault</code>, containing sensitive information that needs to be protected.</li>

<li class="li"><code class="ph codeph">group_vars/backups</code>, containing backup-related variables. </li>

</ul>


<p class="p">For the latter group, a set of sample files has been provided to help you get started:</p>


<ul class="ul">   
<li class="li"><code class="ph codeph">group_vars/vars.sample</code>, a sample group variables file.</li>

<li class="li"><code class="ph codeph">group_vars/vault.sample</code>, a sample vault file.</li>

<li class="li"><code class="ph codeph">group_vars/backups.sample</code>, a sample backup configuration file. </li>

</ul>


<p class="p">The file <code class="ph codeph">group_vars/win_worker.yml</code> supports  advanced configuration of Windows remote
management and in general should not require modification.</p>


<p class="p">You should work from the <code class="ph codeph">root</code> account for the configuration steps and also later on when you run
the playbooks.</p>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title37" id="edit-inventory">
<h2 class="title topictitle2" id="ariaid-title37">Editing the inventory</h2>

<div class="body">
<p class="p">The inventory is the file named <code class="ph codeph">vm_hosts</code> in the
<code class="ph codeph">~/<span class="ph"><span class="keyword">Docker-SimpliVity</span></span></code> directory. You need to edit this file to
describe the configuration you want to deploy.</p>


<p class="p">The nodes inside the inventory are organized in groups. The groups are defined by
brackets and the group names are static so they must not be changed. Other fields
(hostnames, specifications, IP addresses…) are edited to match your setup.
The groups are as follows:</p>

<ul class="ul">
<li class="li"><code class="ph codeph">[ucp_main]</code>: A group containing one single node which will be the
main UCP node and swarm leader. Do not add more than one node under this group.</li>

<li class="li"><code class="ph codeph">[ucp]</code>: A group containing all the UCP nodes, including the main UCP
node. Typically you should have either 3 or 5 nodes under this group.</li>

<li class="li"><code class="ph codeph">[dtr_main]</code>: A group containing one single node which will be the
first DTR node to be installed. Do not add more than one node under this group.</li>

<li class="li"><code class="ph codeph">[dtr]</code>: A group containing all the DTR nodes, including the main DTR
node. Typically you should have either 3 or 5 nodes under this group.</li>

    
<li class="li"><code class="ph codeph">[worker]</code>: A group containing all the Linux worker nodes.</li>

<li class="li"><code class="ph codeph">[win_worker]</code>: A group containing all the Windows worker nodes.</li>

    

<li class="li"><code class="ph codeph">[nfs]</code>: A group containing one single node which will be the NFS
node. Do not add more than one node under this group.</li>

<li class="li"><code class="ph codeph">[logger]</code>: A group containing one single node which will be the
logger node. Do not add more than one node under this group.</li>

<li class="li"><code class="ph codeph">[local]</code>: A group containing the local Ansible host. It contains an
entry that should not be modified.</li>

</ul>


<p class="p">If you are deploying the new <code class="ph codeph">active-active</code> load balancers, using floating IPs managed by <code class="ph codeph">keepalived</code>:</p>

    
<ul class="ul">
<li class="li"><code class="ph codeph">[loadbalancer]</code>: A group containing the UCP, DTR and any worker load balancers you are deploying.</li>
    
</ul>

    
    
<p class="p">If you are using the legacy, standalone load balancers:</p>

<ul class="ul">
<li class="li"><code class="ph codeph">[ucp_lb]</code>: A group containing one single node which will be the load
balancer for the UCP nodes. Do not add more than one node under this group.</li>

<li class="li"><code class="ph codeph">[dtr_lb]</code>: A group containing one single node which will be the load
balancer for the DTR nodes. Do not add more than one node under this group.</li>

<li class="li"><code class="ph codeph">[worker_lb]</code>: A group containing one single node which will be the
load balancer for the worker nodes. Do not add more than one node under this
group.</li>
    
<li class="li"><code class="ph codeph">[lbs]</code>: A group containing all the legacy standalone load balancers. This group will
have 3 nodes, also defined individually in the three groups above.</li>
    
</ul>
    
    

<p class="p">There are also a few special groups:</p>

<ul class="ul">
<li class="li">[docker:children]: A group of groups including all the nodes where Docker will be
installed.</li>

<li class="li">[vms:children]: A group of groups including all the Virtual Machines involved, with the exception of the local host.</li>

</ul>


<p class="p">Finally, you will find some variables defined for each group:</p>

<ul class="ul">
<li class="li">[vms:vars]: A set of variables defined for all VMs. Currently only the size of the
boot disk is defined here.</li>

<li class="li">[ucp:vars]: A set of variables defined for all nodes in the [<code class="ph codeph">ucp</code>]
group.</li>

<li class="li">[dtr:vars]: A set of variables defined for all nodes in the [<code class="ph codeph">dtr</code>]
group.</li>

<li class="li">[worker:vars]: A set of variables defined for all nodes in the
[<code class="ph codeph">worker</code>] group.</li>

<li class="li">[win_worker:vars]: A set of variables defined for all nodes in the
[<code class="ph codeph">win_worker</code>] group.</li>

<li class="li">[loadbalancer:vars]: A set of variables defined for all nodes in the [<code class="ph codeph">loadbalancer</code>]
group.</li>
    
<li class="li">[lbs:vars]: A set of variables defined for all nodes in the [<code class="ph codeph">lbs</code>]
group.</li>

<li class="li">[nfs:vars]: A set of variables defined for all nodes in the [<code class="ph codeph">nfs</code>]
group.</li>

<li class="li">[logger:vars]: A set of variables defined for all nodes in the
[<code class="ph codeph">logger</code>] group.</li>

</ul>


<p class="p">If you wish to configure your nodes with different specifications to the ones
defined by the group, it is possible to declare the same variables at the node level,
overriding the group value. For instance, you could have one of your workers with higher
specifications by setting:</p>


<pre class="pre codeblock"><code>[worker] 
worker01 ip_addr='10.0.0.10/16' esxi_host='esxi1.domain.local' 
worker02 ip_addr='10.0.0.11/16' esxi_host='esxi1.domain.local' 
worker03 ip_addr='10.0.0.12/16' esxi_host='esxi1.domain.local' cpus='16' ram'32768' 

[worker:vars] 
cpus='4' ram='16384' disk2_size='200'</code></pre>

<p class="p">In the example above, the <code class="ph codeph">worker03</code> node would have 4 times more CPU and
double the RAM compared to the rest of the worker nodes.</p>

<p class="p">The different variables you can use are described in 
<a class="xref" href="#edit-inventory__variables-local-table-content">Table 7</a> below. They are all
mandatory unless otherwise specified.</p>




<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="edit-inventory__variables-local-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 7. </span>Variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e5526">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e5529">Scope</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e5532">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5526 ">ip_addr</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5529 ">Node</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5532 ">IP address in CIDR format to be given to a node</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5526 ">esxi_host</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5529 ">Node</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5532 ">ESXi host where the node will be deployed. If the cluster is configured with DRS, this option will be overridden</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5526 ">cpus</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5529 ">Node/Group</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5532 ">Number of CPUs to assign to a VM or a group of VMs</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5526 ">ram</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5529 ">Node/Group</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5532 ">Amount of RAM in MB to assign to a VM or a group of VMs</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5526 ">disk2_usage</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5529 ">Node/Group</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5532 ">Size of the second disk in GB to attach to a VM or a group of VMs. This variable is only mandatory on Docker nodes (UCP, DTR, worker) and NFS node. It is not required for the logger node or the load balancers.</td>
</tr>
  
  
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5526 ">node_policy</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5529 ">Node/Group</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5532 ">HPE SimpliVity backup policy to assign to a VM or a group of VMs. The name has to match 
    one of the backup policies defined in the <code class="ph codeph">group_vars/vars</code> file described in the section
    <a class="xref" href="#svt-backup-config">HPE SimpliVity backup configuration</a>.</td>
</tr>    
		    
</tbody></table>
</div>



</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title38" id="vmware-config">
<h2 class="title topictitle2" id="ariaid-title38">VMware configuration</h2>

<div class="body">
<p class="p">All VMware-related variables are mandatory and are described in <a class="xref" href="#vmware-config__vmware-variables-table-conref">Table 8</a>.</p>




<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="vmware-config__vmware-variables-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 8. </span>VMware variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e5658">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e5661">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e5664">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">vcenter_hostname</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">IP or hostname of the vCenter appliance</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">vcenter_username</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">Username to log in to the vCenter appliance. It might include a domain, for
example, '<code class="ph codeph">administrator@vsphere.local</code>'.</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">vcenter_password</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 "><strong class="ph b">group_vars/vault</strong></td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">The password for the <code class="ph codeph">vcenter_username</code> user above.</td>
</tr>
    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">vcenter_validate_certs</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">‘no’</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">datacenter</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">Name of the datacenter where the environment will be provisioned</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">vm_username</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">Username to log into the VMs. It needs to match the one from the VM Template, so
unless you have created a user, you must use 'root'.</td>
</tr>
    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">vm_password</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 "><strong class="ph b">group_vars/vault</strong></td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">The password for the <code class="ph codeph">vm_username</code> user above.</td>
</tr>
    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">vm_template</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">Name of the RHEL VM Template to be use. Note that this is the name from a vCenter
perspective, not the hostname.</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">folder_name</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">vCenter folder to deploy the VMs. If you do not wish to deploy in a particular
folder, the value should be <code class="ph codeph">/</code>. Note: If you want to deploy in a
specific folder, you need to create this folder in the inventory of the selected
datacenter before starting the deployment.</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">datastores</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">List of datastores to be used, in list format, i.e.
['<code class="ph codeph">Datastore1</code>','<code class="ph codeph">Datastore2</code>'...]. This or these
datastore(s) must exist before you run the playbooks. Note that all the datastores should be mounted on all the ESXi hosts.


<p class="p">Please note that from a <span class="ph"><span class="keyword">HPE SimpliVity</span></span> perspective, it is a best practice to use only one Datastore. 
    Using more than one will not provide any advantages in terms of reliability and will add additional complexity. </p>


</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">disk2</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">UNIX® name of the second disk for the Docker VMs. Typically
<code class="ph codeph">/dev/sdb</code></td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">disk2_part</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">UNIX name of the partition of the second disk for the Docker VMs. Typically
<code class="ph codeph">/dev/sdb1</code></td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">vsphere_plugin_version</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 ">Version of the vSphere plugin for Docker. The default is 0.21.2 which is the latest
version at the time of writing this document. The version of the plugin should match
the version of the vSphere Installation Bundle (VIB) that you installed on the ESXi
servers.</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5658 ">vm_portgroup</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5661 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5664 "><p class="p">Used by the playbook <code class="ph codeph">create_vms.yml</code>, this variable 
specifies the portgroup connected to the network that connects all the VMs. There is
currently only one network. </p>

<p class="p">It is recommended that the template, which is used as the base
for all deployed VMs, specifies a network adapter but it is not required. If 
a network adapter is specified, you should not attach this adapter to a standard switch
if the portgroup designated by <code class="ph codeph">vm_portgroup</code> is connected to a distributed vSwitch.
In addition, you should make sure that the adapter specifies <code class="ph codeph">Connect At Power On</code>.</p>

</td>
</tr>
</tbody></table>
</div>



</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title39" id="svt-variables-config">
<h2 class="title topictitle2" id="ariaid-title39"><span class="ph"><span class="keyword">HPE SimpliVity</span></span> configuration</h2>

<div class="body">
<p class="p">Variables related to your  <span class="ph"><span class="keyword">HPE SimpliVity</span></span> deployment are mandatory and are described in <a class="xref" href="#svt-variables-config__svt-variables-table-conref">Table 9</a>.</p>




<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="svt-variables-config__svt-variables-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 9. </span><span class="ph"><span class="keyword">HPE SimpliVity</span></span> variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e5935">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e5938">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e5941">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5935 ">simplivity_username</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5938 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5941 ">Username to log in to the <span class="ph"><span class="keyword">HPE SimpliVity</span></span> Omnistack appliances. It might include a domain, for example, <code class="ph codeph">administrator@vsphere.local</code>. 
The corresponding password is stored in the variable named <code class="ph codeph">simplivity_password</code>.</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5935 ">simplivity_password</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5938 "><strong class="ph b">group_vars/vault</strong></td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5941 ">The password for the <code class="ph codeph">simplivity_username</code> user.</td>  
</tr>
 
  
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5935 ">omnistack_ovc</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5938 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e5941 ">List of Omnistack hosts to be used, in list format, i.e. <code class="ph codeph">[‘omni1.local’,’onmi2.local’...]</code>. 
  If your OmniStack virtual machines do not have their names registered in DNS, you can use their IP addresses.</td>
</tr>
  

  
  
</tbody></table>
</div>





<div class="section"><h3 class="title sectiontitle">VM placement and number of <span class="ph"><span class="keyword">HPE SimpliVity</span></span> servers in the cluster</h3>
  

<p class="p">The placement of the various VMs deployed by the playbooks depends on whether DRS is enabled or not:</p>


<ol class="ol">
<li class="li">If DRS is not enabled, the placement of the VMs is specified in the ansible inventory file vm_hosts</li>

<li class="li">If DRS is enabled, the placement of the VMs is outside the control of the playbooks</li>

</ol>

  
The playbooks have only been tested with three nodes in the ESX cluster, but the following sections provide guidance on how to use more than three nodes.  
</div>

  
<div class="section"><h3 class="title sectiontitle">Using more than three nodes when DRS is not enabled</h3>
  

<p class="p">The default <code class="ph codeph">vm_hosts</code> file in the solution GitHub repository corresponds to a deployment 
  on a 3-node <span class="ph"><span class="keyword">HPE SimpliVity</span></span> cluster. For each Ansible host in the inventory, you use the <code class="ph codeph">esxi_hosts</code> 
  variable to specify on which ESX hosts the VM should be placed. 
  The following code extract shows 3 UCP VMs distributed across the three members of the cluster. 
  This is the recommended placement as you do not want to one node to host two UCP VMs since a failure of 
  that node would result in the cluster losing quorum.</p>
  
  
<pre class="pre codeblock"><code>
[ucp]
hpe-ucp01 ip_addr='10.10.174.112/22' esxi_host='simply01.am2.cloudra.local'
hpe-ucp02 ip_addr='10.10.174.113/22' esxi_host='simply02.am2.cloudra.local'
hpe-ucp03 ip_addr='10.10.174.114/22' esxi_host='simply03.am2.cloudra.local'
</code></pre>  

<p class="p">In the above example, the first UCP VM will be placed on the ESX host named <code class="ph codeph">simply01.am2.cloudra.local</code>. 
  Note that the value for <code class="ph codeph">esxi_host</code> is the name of the ESX host in the vCenter inventory.</p>


<p class="p">The default <code class="ph codeph">vm_hosts</code> inventory configures three Docker worker nodes and distributes them across the three ESX hosts:</p>


<pre class="pre codeblock"><code>
[worker]
hpe-worker01 ip_addr='10.10.174.122/22' esxi_host='simply01.am2.cloudra.local'
hpe-worker02 ip_addr='10.10.174.123/22' esxi_host='simply02.am2.cloudra.local'
hpe-worker03 ip_addr='10.10.174.124/22' esxi_host='simply03.am2.cloudra.local'
</code></pre>

<p class="p">If you have more than three ESX hosts in your cluster, you can add an additional worker node as follows:</p>

  
<pre class="pre codeblock"><code>
[worker]
hpe-worker01 ip_addr='10.10.174.122/22' esxi_host='simply01.am2.cloudra.local'
hpe-worker02 ip_addr='10.10.174.123/22' esxi_host='simply02.am2.cloudra.local'
hpe-worker03 ip_addr='10.10.174.124/22' esxi_host='simply03.am2.cloudra.local'
hpe-worker04 ip_addr='10.10.174.150/22' esxi_host='simply04.am2.cloudra.local'
</code></pre>  

<p class="p">You can also distribute the infrastructure VMs across fours nodes rather than across the default nodes. 
  For example, the default placement for the NFS server VM is as follows:</p>


<pre class="pre codeblock"><code>
[nfs]
hpe-nfs ip_addr='10.10.174.121/22'    esxi_host='simply03.am2.cloudra.local'
</code></pre>

<p class="p">Instead, you can change the placement NFS server VM, leveraging a fourth ESX node:</p>

  
<pre class="pre codeblock"><code>
[nfs]
hpe-nfs ip_addr='10.10.174.121/22'    esxi_host='simply04.am2.cloudra.local'
</code></pre>  

<p class="p">When you specify the placement of the VM, you should ensure that you follow these placement guidelines:</p>

  
<ul class="ul">
<li class="li">Do not place two UCP VMs on the same node. If the node fails, the UCP cluster will lose quorum and the service will go down.</li>

<li class="li">Do not place two DTR replicas (VMs) on the same node. Once again, the cluster will lose quorum if that node fails.</li>
  
</ul>
  

<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> 
The OmniStack software maintains two replicas on two different hosts for each VM. As a result, when a VM is scheduled 
on an ESX server that does not have local access to one of the replicas, the VM will report 
the warning “SimpliVity VM Data Access Not Optimized”. You can safely ignore this warning.  
</div><hr />

  
</div>
  
  
<div class="section"><h3 class="title sectiontitle">Using more than three nodes when DRS is enabled</h3>


<p class="p">When DRS is enabled, it controls the placement of the VMs and as a result, the placement you have specified 
  in the <code class="ph codeph">vm_hosts</code> inventory is ignored. Instead, you use DRS rules to make sure that 
  the UCP and DTR VMs are distributed across three nodes for the reasons explained earlier.</p>

  
<hr /><div class="note warning"><span class="warningtitle"><b>Warning:</b></span> 
If you do not specify DRS rules to determine the placement, DRS will automatically move the VMs that 
report the “SimpliVity VM Data Access Not Optimized” warning to a node with a replica of the VM 
which may break the earlier placement guideline.  
</div><hr />
  

</div>



</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title40" id="svt-backup-config">
<h2 class="title topictitle2" id="ariaid-title40">HPE SimpliVity backup configuration</h2>

<div class="body">

<p class="p">Variables related to configuring  <span class="ph"><span class="keyword">HPE SimpliVity</span></span> backups 
  are described in <a class="xref" href="#svt-backup-config__svt-backup-variables-table-content">Table 10</a>.</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="svt-backup-config__svt-backup-variables-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 10. </span><span class="ph"><span class="keyword">HPE SimpliVity</span></span> backup variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6160">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6163">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6166">Description</th>
</tr>
</thead><tbody class="tbody">
  
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6160 ">backup_policies</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6163 ">group_vars/vault</td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6166 ">List of dictionaries containing the different backup policies to be used along with the scheduling information.
  Any number of backup policies can be created and they need to match the node_policy variables defined in the inventory. 
  Times are indicated in minutes. All month calculations use a 30-day month.
  All year calculations use a 365-day year. The format is as follows:

<pre class="pre codeblock"><code>
backup_policies:
 - name: daily'   
   days: 'All'   
   start_time: '11:30'   
   frequency: '1440'   
   retention: '10080' 
 - name: 'hourly'   
   days: 'All'   
   start_time: '00:00'   
   frequency: '60'   
   retention: '2880'
</code></pre>
</td>  
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6160 ">dummy_vm_prefix</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6163 ">group_vars/vault</td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6166 ">In order to be able to backup the Docker volumes, a number of "dummy" VMs need to spin up. 
  This variable will set a recognizable prefix for them.</td>  
</tr>
  
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6160 ">docker_volumes_policy</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6163 ">group_vars/vault</td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6166 ">Backup policy to use for the Docker volumes.</td>  
</tr>    
 
</tbody></table>
</div>



</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title41" id="network-config">
<h2 class="title topictitle2" id="ariaid-title41">Networking configuration</h2>

<div class="body">
<p class="p">All network-related variables are mandatory and are described in <a class="xref" href="#network-config__network-config-table-conref">Table 11</a>.</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="network-config__network-config-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 11. </span>Network variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6251">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6254">File</th>    
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6257">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6251 ">nic_name</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6254 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6257 ">Name of the device, for RHEL this is typically
<code class="ph codeph">ens192</code> and it is recommended to leave it as
is.</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6251 ">gateway</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6254 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6257 ">IP address of the gateway to be used</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6251 ">dns</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6254 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6257 ">List of DNS servers to be used, in list format, i.e.
['<code class="ph codeph">10.10.173.1</code>','<code class="ph codeph">10.10.173.2</code>'...]</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6251 ">domain_name</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6254 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6257 ">Domain name for your Virtual Machines</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6251 ">ntp_server</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6254 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6257 ">List of NTP servers to be used, in list format, i.e.
['<code class="ph codeph">1.2.3.4</code>','<code class="ph codeph">0.us.pool.net.org</code>'...]</td>
</tr>
</tbody></table>
</div>



</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title42" id="environment-config">
<h2 class="title topictitle2" id="ariaid-title42">Environment configuration</h2>

<div class="body">
<p class="p">All Environment-related variables are described in
<a class="xref" href="#environment-config__env-variables-table">Table 12</a> below.</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="environment-config__env-variables-table" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 12. </span>Environment variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="" valign="top" id="d29e6378">Variable</th>
<th class="entry nocellnorowborder" align="" valign="top" id="d29e6381">File</th>    
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6384">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="" valign="top" headers="d29e6378 ">env</td>
<td class="entry nocellnorowborder" align="" valign="top" headers="d29e6381 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6384 ">Dictionary containing all environment variables. It contains
three entries described below. Please leave  the proxy related
settings empty if not required:<ul class="ul">
<li class="li"><code class="ph codeph">http_proxy</code>: HTTP proxy URL, such as <code class="ph codeph">'http://15.184.4.2:8080'</code>. This variable defines
the HTTP proxy URL if your environment is behind a
proxy.</li>

<li class="li"><code class="ph codeph">https_proxy</code>: HTTPS proxy URL, such as <code class="ph codeph">'http://15.184.4.2:8080'</code>. This variable defines
the HTTPS proxy URL if your environment is behind a
proxy.</li>

<li class="li"><code class="ph codeph">no_proxy</code>: List of hostnames or IPs that don't require proxy,
such as <code class="ph codeph">'localhost,127.0.0.1,.cloudra.local,10.10.174.'</code></li>

</ul>
</td>
</tr>
</tbody></table>
</div>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title43" id="docker-config">
<h2 class="title topictitle2" id="ariaid-title43">Docker configuration</h2>

<div class="body">
<p class="p">All Docker-related variables are mandatory and are described in <a class="xref" href="#docker-config__docker-config-table-conref">Table 13</a>.</p>




<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="docker-config__docker-config-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 13. </span>Docker variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6468">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6471">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6474">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6468 ">docker_ee_url</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6471 "><strong class="ph b">group_vars/vault</strong></td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6474 ">Note: This is a private link to your Docker EE subscription.  The value for
<code class="ph codeph">docker_ee_url</code> is the URL documented at the
following address: <a class="xref" href="https://docs.docker.com/engine/installation/linux/docker-ee/rhel/" target="_blank">https://docs.docker.com/engine/installation/linux/docker-ee/rhel/</a>.</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6468 ">docker_ee_reponame</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6471 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6474 ">For Docker EE 2.0, this variable must be set to the value <code class="ph codeph">stable-17.06</code></td>


</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6468 ">rhel_version</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6471 ">group_vars/vars</td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6474 ">For the Docker installation, this sets the version of your RHEL OS, such as <code class="ph codeph">7.5</code>. The
playbooks were tested with RHEL 7.5.</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6468 ">dtr_version</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6471 ">group_vars/vars</td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6474 ">Version of the Docker DTR you wish to install. You can use a
numeric version or <code class="ph codeph">latest</code> for the most recent one.
The playbooks were tested with 2.5.6.</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6468 ">ucp_version</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6471 ">group_vars/vars</td>      
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6474 ">Version of the Docker UCP you wish to install. You can use a
numeric version or <code class="ph codeph">latest</code> for the most recent one.
The playbooks were tested with UCP 3.0.6.
<p class="p">To address <a class="xref" href="https://nvd.nist.gov/vuln/detail/CVE-2018-1002105" target="_blank">CVE-2018-1002105</a>, 
    a critical security issue in the Kubernetes API Server, 
    Docker is using a custom build of Kubernetes 1.8.15 for UCP 3.0.7.
It is recommended that you use <code class="ph codeph">3.0.7</code> to avoid this issue. </p>

</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6468 ">images_folder</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6471 ">group_vars/vars</td>     
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6474 ">Directory in the NFS server that will be mounted in the DTR nodes
and that will host your Docker images.</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6468 ">license_file</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6471 ">group_vars/vars</td>     
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6474 ">Full path to your Docker EE license file on your Ansible host. The license file is available from the Docker
Store</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6468 ">ucp_username</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6471 ">group_vars/vars</td>     
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6474 ">Username of the administrator user for UCP and DTR, typically
<code class="ph codeph">admin</code>.</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6468 ">ucp_password</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6471 "><strong class="ph b">group_vars/vault</strong></td>     
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6474 ">The password for the <code class="ph codeph">ucp_username</code> account.</td> 
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6468 ">docker_storage_driver</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6471 ">group_vars/vars</td>     
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6474 ">Storage driver for Docker nodes. Accepted values are <code class="ph codeph">overlay2</code> (the default) 
    and <code class="ph codeph">devicemapper</code>.</td> 
</tr>

</tbody></table>
</div>


<p class="p">To see how to use customer-supplied certificates with UCP and DTR, see <a class="xref" href="#customer-certs">Appendix B</a>.</p>


</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title44" id="orchestrator-config">
<h2 class="title topictitle2" id="ariaid-title44">Orchestrator configuration</h2>

<div class="body">

<div class="section">
<p class="p">The variable <code class="ph codeph">orchestrator</code> in the <code class="ph codeph">[worker]</code> group is used to 
  specify if a worker node should be assigned to the Kubernetes orchestrator (<code class="ph codeph">orchestrator: 'kubernetes'</code>) 
  or to the Swarm orchestrator (<code class="ph codeph">orchestrator: 'swarm'</code>). In general, you should only change the orchestrator 
  for worker nodes.

</p>

  
<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> Docker supports a third type, <code class="ph codeph">mixed</code>, that enables workloads to be scheduled 
  by both Kubernetes and Swarm on the same node. Mixing orchestrator types on the same node is not recommended 
  for production deployments because of the likelihood of resource contention and so, these playbooks do not support
the <code class="ph codeph">mixed</code> type.</div><hr />
  
  
  
<p class="p">The following example shows how to set Kubernetes as the default orchestrator for worker nodes,
        and how to over-ride the default to use Swarm on one specific node instead. </p>

  
<pre class="pre codeblock"><code>
## WORKER
[worker]
hpe-worker01 ip_addr='10.60.59.21/16' esxi_host='esxi-hpe-1.cloudra.local'
hpe-worker02 ip_addr='10.60.59.22/16' esxi_host='esxi-hpe-2.cloudra.local'
hpe-worker03 ip_addr='10.60.59.23/16' esxi_host='esxi-hpe-3.cloudra.local' <strong class="ph b">orchestrator=swarm</strong>
 
[worker:vars]
cpus='4'
ram='65536'
disk2_size='500'
<strong class="ph b">orchestrator=kubernetes</strong>
</code></pre>  
  

<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> 

The playbooks do not change Docker's default orchestrator type which is <code class="ph codeph">swarm</code>. Instead, the inventory is used 
to configure worker nodes for Kubernetes workloads or swarm workloads as explained above. If you want to change the default orchestrator type, use the method explained in the Docker documentation at
<a class="xref" href="https://docs.docker.com/ee/ucp/admin/configure/set-orchestrator-type/#set-the-default-orchestrator-type-for-new-nodes" target="_blank">https://docs.docker.com/ee/ucp/admin/configure/set-orchestrator-type/#set-the-default-orchestrator-type-for-new-nodes</a>.


It is possible to manually change the orchestrator type for a node. When you do this, existing workloads are evicted
and they are not migrated automatically to the new orchestrator. If you want the workloads to be scheduled by the new orchestrator, 
you must migrate them manually. More information is available in the Docker documentation at 
<a class="xref" href="https://docs.docker.com/ee/ucp/admin/configure/set-orchestrator-type/#what-happens-when-you-change-a-nodes-orchestrator" target="_blank">https://docs.docker.com/ee/ucp/admin/configure/set-orchestrator-type/#what-happens-when-you-change-a-nodes-orchestrator</a>.
   
</div><hr />




</div>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title45" id="k8s-config">
<h2 class="title topictitle2" id="ariaid-title45">Kubernetes configuration</h2>

<div class="body">
  
<div class="section">
  
<p class="p">The current playbooks support the deployment of UCP 3.0.* which deploys Kubernetes version 1.8.*. 
  This version of the playbooks will not work with a version of UCP lower than version 3. If you 
  wish to deploy using UCP 2.*, you will need to download the previous release of the playbooks, which
  is available on the GitHub site.
</p>
  
  
<p class="p">The preceding section <a class="xref" href="#orchestrator-config">Orchestrator configuration</a> explains how to assign a worker node to the Kubernetes 
orchestrator. This sections covers specific Kubernetes configuration, including how to set the pod CIDR and
how to configure Kubernetes Persistent Volumes.</p>
  
  
</div>
  
    
    
<div class="section"><h3 class="title sectiontitle">Pod CIDR</h3>
 
  
<p class="p">The variable <code class="ph codeph">k8s_pod_cidr</code> is specified in <code class="ph codeph">group_vars/vars</code> and configures a custom range of IP 
  addresses to be used by pods. The specific range you use should be dedicated to the cluster. 
</p>

  
<p class="p">The default value is <code class="ph codeph">192.168.0.0/16</code>. To set an alternative value, use the variable as shown in the example:</p>

  
<pre class="pre codeblock"><code>    k8s_pod_cidr: 192.168.128.0/17</code></pre>
    
</div>
 
  
  
  </div>

<div class="topic nested2" aria-labelledby="ariaid-title46" id="k8s-persistent-volume-config">
<h3 class="title topictitle3" id="ariaid-title46">Kubernetes Persistent Volume configuration</h3>

<div class="body">
    

<p class="p">Variables related to the configuration of Kubernetes Persistent Volumes are shown in 
<a class="xref" href="#k8s-persistent-volume-config__k8s-persistent-volume-config-table-conref">Table 14</a>.</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="k8s-persistent-volume-config__k8s-persistent-volume-config-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 14. </span>Kubernetes Persistent Volume variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6804">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6807">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e6810">Description</th>
</tr>
</thead><tbody class="tbody">

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6804 ">nfs_provisioner_role</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6807 ">group_vars/vars</td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6810 ">Name of the role to create, for example, <code class="ph codeph">nfs-provisioner-runner</code>.</td>
</tr>   
  
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6804 ">nfs_provisioner_name</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6807 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6810 ">Name of the provisioner, for example, 	<code class="ph codeph">hpe.com/nfs</code></td>
</tr>
 
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6804 ">nfs_provisioner_storage_class_name</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6807 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6810 ">Name of the storage class to create, for example, 	<code class="ph codeph">nfs</code></td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6804 ">nfs_provisioner_server_ip</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6807 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6810 ">IP address (or FQDN) of your external NFS server, for example, 	<code class="ph codeph">hpe-nfs.cloudra.local</code></td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6804 ">nfs_provisioner_server_share</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6807 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e6810 ">Name of the NFS share where all the persistent volume data will be stored, for example, 	<code class="ph codeph">/k8s</code></td>
</tr>


</tbody></table>
</div>
  


<div class="section"><h4 class="title sectiontitle">Related playbooks</h4>
   
    
<ul class="ul">
<li class="li"><code class="ph codeph">playbooks/k8s-nfs-provisioner.yml</code>	is used to enable a dynamic NFS provisioner which can be used 
    to automatically create and allocate Kubernetes persistent volumes. The backend storage is provided by an NFS backend. 
    This playbook is run from the Ansible box after downloading a UCP client bundle for the <code class="ph codeph">admin</code> account 
    and sourcing the downloaded <code class="ph codeph">env.sh</code> file. For more information on using this playbook, see the section <a class="xref" href="#k8s-nfs-provisioner">Deploying the NFS provisioner for Kubernetes</a>.</li>
    
    
</ul>
    
    
</div>


</div>

</div>
</div>
<div class="topic nested1" aria-labelledby="ariaid-title47" id="edit-vault">
<h2 class="title topictitle2" id="ariaid-title47">Protecting sensitive information</h2>

<div class="body">
<p class="p">A vault file is used to protect any sensitive variables that should not appear in clear text in your
<code class="ph codeph">group_vars/vars</code> file. The vault file will be encrypted and will require a password to be entered before
it can be read or updated.</p>

    
<p class="p">A sample vault file is provided named <code class="ph codeph">group_vars/vault.sample</code> that you can
use as a model for your own vault file. To create a vault, you create a new file called
<code class="ph codeph">group_vars/vault</code> and add entries similar to:</p>


<pre class="pre codeblock"><code>---
docker_ee_url: 'your_url_here'
vcenter_password: 'xxxx'
vm_password: 'xxxx'

simplivity_password: 'xxx'
ucp_password: 'zzzz'
win_password: 'yourpass'
sysdig_access_key: 'enter_sysdig_access_key'
rhn_orgid: "YourOrgId"
rhn_key: "YourActivationKey"
#password for the splunk universal forwarder. Must meet password complexity requirement (see splunk doc)
splunk_uf_password: 'YourPa$$word12'
</code></pre>

<p class="p"><code class="ph codeph">rhn_orgid</code> and <code class="ph codeph">rhn_key</code> are the credentials needed to
subscribe the virtual machines with Red Hat Customer Portal. For more information regarding
activation keys, see the following URL: <a class="xref" href="https://access.redhat.com/articles/1378093" target="_blank">https://access.redhat.com/articles/1378093</a></p>

    
<p class="p">To encrypt the vault you need to run the following command:</p>


<pre class="pre codeblock"><code># ansible-vault encrypt group_vars/vault</code></pre>

<p class="p">You will be prompted for a password that will decrypt the vault when required. You can
update the values in your vault by running:</p>


<pre class="pre codeblock"><code># ansible-vault edit group_vars/vault</code></pre>

<p class="p">In order for Ansible to be able to read the vault, you need to specify a file where the password
is stored, for instance, in a file called <code class="ph codeph">.vault_pass</code>. Once the file is
created, take the following precautions to avoid illegitimate access to this file:</p>

<ol class="ol">
<li class="li"> Change the permissions so only <code class="ph codeph">root</code> can read it using <code class="ph codeph">#
chmod 600 .vault_pass</code>
</li>

<li class="li"> Add the file to your <code class="ph codeph">.gitignore</code> file if you are using a Git repository to manage your playbooks. </li>

</ol>

<p class="p"></p>

    
<p class="p">When you use a vault, you must specify the password file every time on the command line, for example,</p>
   
    
<pre class="pre codeblock"><code>
# ansible-playbook -i vm_hosts site.yml <strong class="ph b">--vault-password-file .vault_pass</strong>
</code></pre>    
    
</div>





</div>
<div class="topic nested1" aria-labelledby="ariaid-title48" id="inventory-group-variables">
<h2 class="title topictitle2" id="ariaid-title48">Inventory group variables</h2>

<div class="body">
  
<p class="p">Additional configuration files for each group in the inventory are available, including <code class="ph codeph">group_vars/vms.yml</code>,
<code class="ph codeph">group_vars/ucp.yml</code>, <code class="ph codeph">group_vars/dtr.yml</code>, <code class="ph codeph">group_vars/worker.yml</code> 
  and <code class="ph codeph">group_vars/nfs.yml</code>.
  
</p>

  
<p class="p">These group files facilitate more sophisticated settings, such as additional drives and additional network interfaces. For example, here
  is the <code class="ph codeph">group_vars/nfs.yml</code> file.
</p>
   

<pre class="pre codeblock"><code>networks:
  - name:  '{{ vm_portgroup }}'
    ip:  "{{ ip_addr | ipaddr('address') }}"
    netmask: "{{ ip_addr | ipaddr('netmask') }}"
    gateway: "{{ gateway }}"
 
disks_specs:
  - size_gb:  '{{ disk1_size }}'
    type: thin
    datastore: "{{ datastores | random }}"
  - size_gb: '{{ disk2_size }}'
    type: thin
    datastore: "{{ datastores | random }}"
  - size_gb: 10
    type: thin
    datastore: "{{ datastores | random }}"</code></pre>


<p class="p">In this example, the size of the first two drives is specified using the values of the variables <code class="ph codeph">disk1_size</code> and <code class="ph codeph">disk2_size</code>  
  that are declared in the <code class="ph codeph">group_vars/vars</code> file. This maintains compatibility with <code class="ph codeph">vm_hosts</code> inventories 
  from earlier releases of the playbooks. However, it is possible to provide explicit values, depending on your requirements, for the
individual UCP, DTR, worker or NFS VMs. For example, you may want to increase the size of the second disk for the NFS VM as this is used
to store the DTR images, so the default value of 500GB may not be sufficient to meet your needs. </p>

 
<p class="p">In this release, support has been added for configuring a third drive that can be used to hold Kubernetes persistent volume data.
The default size (10GB) is set low as the use of the NFS VM for storing persistent volume data is only considered suitable 
for demo purposes and should not be used in a production environment.
</p>

  
<p class="p">In the following example, the <code class="ph codeph">group_vars/nfs.yml</code> has been modified to 
configure the NFS VM  with a 50GB boot disk, a 500GB drive for DTR images and an 800GB drive for Kubernetes persistent volumes data. 
</p>
  
  
<pre class="pre codeblock"><code>
networks:
  - name:  '{{ vm_portgroup }}'
    ip:  "{{ ip_addr | ipaddr('address') }}"
    netmask: "{{ ip_addr | ipaddr('netmask') }}"
    gateway: "{{ gateway }}"
 
disks_specs:
  - size_gb:  <strong class="ph b">50</strong>
    type: thin
    datastore: "{{ datastores | random }}"
  - size_gb: <strong class="ph b">500</strong>
    type: thin
    datastore: "{{ datastores | random }}"
  - size_gb: <strong class="ph b">800</strong>
    type: thin
    datastore: "{{ datastores | random }}"</code></pre>

<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> The number of drives and the purpose of each drive is determined by the role of the VM and the specific playbooks
  that uses the information. The first disk is always used as the boot disk, irrespective of VM role, while the purpose of the 
  second or third disk is specific to the role.</div><hr />

  
</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title49" id="playbooks-overview">
<h1 class="title topictitle1" id="ariaid-title49">Overview of the playbooks</h1>

<div class="body">

<p class="p">The Ansible playbooks are available to download at 
<span class="ph"><a class="xref" href="https://github.com/HewlettPackard/Docker-SimpliVity" target="_blank"><span class="keyword">https://github.com/HewlettPackard/Docker-SimpliVity</span></a></span>. 
Once you have cloned the repository, change directory to <code class="ph codeph">/root/<span class="ph"><span class="keyword">Docker-SimpliVity</span></span></code>.</p>



<p class="p">You can use the playbook <code class="ph codeph">site.yml</code> as the day 0 playbook to deploy the solution.
It is simply a wrapper around a number of required and optional playbooks that allow you to configure 
the deployment to your needs. 
</p>

    
<p class="p">To start a deployment, use the following command:</p>


<pre class="pre codeblock"><code># ansible-playbook -i vm_hosts site.yml --vault-password-file .vault_pass</code></pre>

    
<p class="p">The playbooks should run for approximately 35-40 minutes for the default deployment with 3 UCP, 3 DTR and 3 Linux worker nodes
    (depending on your server specifications and the size of your environment).</p>

   
</div>



<div class="topic nested1" aria-labelledby="ariaid-title50" id="core-infrastructure">
<h2 class="title topictitle2" id="ariaid-title50">Core components</h2>

<div class="body">
    
<p class="p">The playbooks for deploying the core components  are described in the following sections:</p>


<ul class="ul">
<li class="li"><a class="xref" href="#create-rhel-vms">Provisioning RHEL VMs</a></li>

<li class="li"><a class="xref" href="#unique_2072037671">Provisioning load balancers for UCP and DTR</a></li>

<li class="li"><a class="xref" href="#install-docker-ucp-dtr">Installing Docker UCP and DTR on RHEL VMs</a></li>

<li class="li"><a class="xref" href="#add-rhel-workers">Deploying RHEL workers</a></li>

</ul>


</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title51" id="optional-playbooks">
<h2 class="title topictitle2" id="ariaid-title51">Optional components</h2>

<div class="body">

<p class="p">The playbooks for deploying optional components are described in the following sections:</p>
   
<ul class="ul">
<li class="li"><a class="xref" href="#add-windows-workers">Playbooks for adding Windows workers</a></li>

<li class="li"><a class="xref" href="#install-sysdig">Playbooks for installing Sysdig on RHEL</a></li>

<li class="li"><a class="xref" href="#install-splunk">Playbooks for installing Splunk</a></li>
   
<li class="li"><a class="xref" href="#prometheus-grafana">Deploying Prometheus and Grafana on Docker RHEL nodes</a></li>

</ul>


</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title52" id="backup-restore-playbooks">
<h2 class="title topictitle2" id="ariaid-title52">Backup and restore playbooks</h2>

    
<div class="body">
    
<div class="section">
<p class="p">Best practices and procedures are described in the section <a class="xref" href="#backup-restore">Backup and restore</a>. The following playbooks are used to perform backups:</p>

    
<ul class="ul">
<li class="li"><code class="ph codeph">playbooks/backup_swarm.yml</code> is used to back up the swarm data</li>

<li class="li"><code class="ph codeph">playbooks/backup_ucp.yml</code> is used to back up UCP</li>

<li class="li"><code class="ph codeph">playbooks/backup_dtr_meta.yml</code> is used to back up DTR metadata</li>

<li class="li"><code class="ph codeph">playbooks/backup_dtr_images.yml</code> is used to back up DTR images</li>

</ul>



</div>

    
<div class="section">
<p class="p">The following playbooks are used to restore the system:</p>

    
<ul class="ul">
<li class="li"><code class="ph codeph">playbooks/restore_dtr_images.yml</code> is used to restore DTR images</li>

<li class="li"><code class="ph codeph">playbooks/restore_dtr_metadata.yml</code> is used to restore DTR metadata</li>

<li class="li"><code class="ph codeph">playbooks/restore_ucp.yml</code> is used to restore UCP</li>

</ul>

    
    
</div>
 
    
<div class="section">
<p class="p">The playbooks for configuring  <span class="ph"><span class="keyword">HPE SimpliVity</span></span> backup functionality 
    are run as part of <code class="ph codeph">site.yml</code> by default:</p>

<ul class="ul">
<li class="li"><code class="ph codeph">playbooks/config_dummy_vms_for_docker_volumes_backup.yml</code> </li>
      
<li class="li"><code class="ph codeph">playbooks/config_simplivity_backups.yml</code>    
</li>

</ul>

    
    
</div>




</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title53" id="convenience-playbooks">
<h2 class="title topictitle2" id="ariaid-title53">Convenience playbooks</h2>

    
<div class="body">
 
<div class="section">

<ul class="ul">
<li class="li"><code class="ph codeph">playbooks/clean_all.yml</code> powers off and deletes all VMs in your inventory.</li>
    
<li class="li"><code class="ph codeph">playbooks/distribute_keys.yml</code> distributes public keys between all nodes, to allow each node to 
password-less log in to every other node. As this is not essential and can be regarded as a security risk 
(a worker node probably should not be able to log in to a UCP node, for instance), this playbook is commented out in
 <code class="ph codeph">site.yml</code> by default.</li>
    
</ul>

    
</div>
 
</div>
        
</div>
<div class="topic nested1" aria-labelledby="ariaid-title54" id="convenience-scripts">
<h2 class="title topictitle2" id="ariaid-title54">Convenience scripts</h2>

    
<div class="body">
 
<div class="section">

<ul class="ul">
<li class="li"><code class="ph codeph">backup.sh</code> can be used to take a backup of the swarm, UCP, DTR metadata and the DTR images in one go.</li>

<li class="li"><code class="ph codeph">restore_dtr.sh</code> can be used to restore DTR metadata and DTR images.</li>

<li class="li"><code class="ph codeph">scale_worker.sh</code> can be used to scale the worker nodes.</li>

</ul>


 
</div>

</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title55" id="running-playbooks">
<h1 class="title topictitle1" id="ariaid-title55">Deploying the core components</h1>

<div class="body">
<div class="section" id="running-playbooks__running-playbooks-section">
<p class="p">At this point, the system is ready to be deployed. Make sure you are logged on as 
    <code class="ph codeph">root</code> on your ansible box and that your current directory is <code class="ph codeph">/root/<span class="ph"><span class="keyword">Docker-SimpliVity</span></span></code></p>



    

<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> As well as configuring your <code class="ph codeph">vars</code> and <code class="ph codeph">vault</code> files, you must also provide a <code class="ph codeph">backups</code> configuration 
    file in the <code class="ph codeph">group_vars</code> folder when running <code class="ph codeph">site.yml</code>. 
    An example file is provided in the repository named <code class="ph codeph">backups.sample</code>. Rename it to <code class="ph codeph">backups</code> before 
    running the playbooks. Details on how to configure this file are available in the section 
    <a class="xref" href="#backup-restore">Backup and restore</a>. </div><hr />
 
    
</div>

</div>

<div class="topic nested1" aria-labelledby="ariaid-title56" id="create-rhel-vms">
<h2 class="title topictitle2" id="ariaid-title56">Provisioning RHEL VMs</h2>

<div class="body">
  
<p class="p">The following playbooks are used to provision RHEL VMs:</p>
  
  
<ul class="ul">

<li class="li"><code class="ph codeph">playbooks/create_vms.yml</code> will create all the necessary virtual machines for 
the environment from the VM Template defined in the <code class="ph codeph">vm_template</code> variable. All Linux VMs are now created in one go, regardless of the number of drives they have. 
This playbook also has the potential to configure additional network adapters.
</li>


<li class="li"><code class="ph codeph">playbooks/config_networking.yml</code> will configure the network settings in all the virtual machines.</li>


<li class="li"><code class="ph codeph" id="create-rhel-vms__resize_syspart">playbooks/resize_syspart.yml</code> resizes the logical volume that holds the <code class="ph codeph">/</code> partition 
of the Linux VMs to use all the space available on the drive.</li>


<li class="li"><code class="ph codeph">playbooks/config_subscription.yml</code> registers and subscribes all virtual machines to the Red Hat Customer Portal. 
This is only needed if you pull packages from Red Hat. This playbook is commented out by default but you should uncomment 
it to make sure each VM registers with the Red Hat portal. It is commented out so that you can test the deployment first 
without having to unregister all the VMs from the Red Hat Customer Portal between each test. If you are using an 
internal repository, as described in the section "Create a VM template", you can keep this playbook commented out.</li>



<li class="li"><code class="ph codeph">playbooks/config_ntp.yml</code> configures the <strong class="ph b">chrony</strong> client package in all virtual machines in 
order to have a synchronized clock across the environment. It will use the list of servers specified in the 
<code class="ph codeph">ntp_servers</code> variable in the file <code class="ph codeph">group_vars/vars</code>.</li>



</ul>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title57" id="unique_2072037671">
<h2 class="title topictitle2" id="ariaid-title57">Provisioning load balancers for UCP and DTR</h2>

<div class="body">
    
<div class="section">
    
<p class="p">The playbook <code class="ph codeph">playbooks/loadbalancer.yml</code> is used to deploy load balancers in an <code class="ph codeph">active-active</code>
configuration to provide highly-available access to UCP and DTR.</p>


<p class="p">At least two nodes are specified in the <code class="ph codeph">[loadbalancer]</code> group in the inventory, 
    along with group variables defining CPU and RAM requirements. These nodes  run
<code class="ph codeph">keepalived</code> and <code class="ph codeph">HAproxy</code>.</p>


<pre class="pre codeblock"><code>
[loadbalancer]
hpe-lb1 ip_addr='10.10.174.248/22' esxi_host='simply04.am2.cloudra.local' ucp=true
hpe-lb2 ip_addr='10.10.174.249/22' esxi_host='simply05.am2.cloudra.local' dtr=true

[loadbalancer:vars]
cpus='2'
ram='4096'
node_policy='hpe-bronze'
</code></pre>
    
<p class="p">The virtual IP for UCP will be handled by <code class="ph codeph">hpe-lb1</code> by default, which will split the traffic across
the three UCP VMs <code class="ph codeph">hpe-ucp01</code>, <code class="ph codeph">hpe-ucp02</code> and <code class="ph codeph">hpe-ucp03</code>.
In the case of a failure of <code class="ph codeph">hpe-lb1</code>, the virtual IP for UCP will automatically move
to the second load balancer node <code class="ph codeph">hpe-lb2</code> which will again distribute the traffic to the UCP VMs.</p>

    
<p class="p">Similarly, the virtual IP for DTR will be handled by default by the load balancer <code class="ph codeph">hpe-lb2</code>,
splitting the traffic across
the three DTR VMs <code class="ph codeph">hpe-dtr01</code>, <code class="ph codeph">hpe-dtr02</code> and <code class="ph codeph">hpe-dtr03</code>.    
In the case of a failure of <code class="ph codeph">hpe-lb2</code>, the virtual IP for DTR will automatically move
to the first load balancer node <code class="ph codeph">hpe-lb1</code> which will again distribute the traffic to the DTR VMs.</p>
    


<p class="p">To configure the virtual IPs for UCP and DTR, you need to add a <code class="ph codeph">loadbalancers</code> dictionary to your
<code class="ph codeph">group_vars/vars</code> file as shown in the excerpt below:</p>

    
<pre class="pre codeblock"><code>
loadbalancers:
  ucp:
    public_interface: 'ens192'
    public_vip: '10.60.59.251'
    public_fqdn: hpe-ucpvip.cloudra.local
    virtual_router_id: 54
  dtr:
    public_interface: 'ens192'
    public_vip: '10.60.59.252'
    public_fqdn: hpe-dtrvip.cloudra.local
    virtual_router_id: 55

</code></pre>
 
<hr /><div class="note warning"><span class="warningtitle"><b>Warning:</b></span> If you re-run <code class="ph codeph">playbooks/loadbalancer.yml</code> after a configuration change, you may need to 
subsequently run <code class="ph codeph">playbooks/reconfigure_dtr.yml</code> as the latter playbook configures the virtual IP address for accessing
the UCP Single-Sign-On (SSO) page. If there is no virtual IP or FQDN defined for UCP in the variables file, the playbook will choose
the address of the first UCP node in the <code class="ph codeph">[ucp]</code> group. This scenario introduces a single point of failure and should be avoided.
</div><hr />



<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> By default, the playbook supports ports 
<code class="ph codeph">433</code> and <code class="ph codeph">6443</code> for UCP and port <code class="ph codeph">433</code> for DTR. 
If you deploy Prometheus and Grafana on Docker Swarm, the Grafana port <code class="ph codeph">3000</code>
will be handled as well.</div><hr />


<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> The playbook <code class="ph codeph">playbooks/loadbalancer.yml</code> can be used to create one or more load balancers 
for applications running on your worker nodes. However, it is impossible for the playbooks to know what ports to support, so
manual configuration of HAproxy and <code class="ph codeph">keepalived</code> may be required. By default, the playbooks support ports <code class="ph codeph">80</code> and 
<code class="ph codeph">443</code> for worker nodes.</div><hr />


</div>
    
    
    


<div class="section"><h3 class="title sectiontitle">Legacy stand-alone load balancers</h3>
   
    
<p class="p">The playbook <code class="ph codeph">playbooks/install_haproxy.yml</code> is used to deploy three separate load balancers, for the UCP,
DTR and worker nodes. It is recommended that you use the HAproxy/<code class="ph codeph">keepalived</code> solution documented above instead of this option.
</p>
    
    
</div>



<div class="section"><h3 class="title sectiontitle">Deploying without load balancers</h3>
 
<p class="p">If you do not want to deploy load balancers when running <code class="ph codeph">site.yml</code>, you should comment out any declarations in the inventory and variables files. This
includes any legacy stand-alone load balancers.</p>
    
</div>
    
    
    
<div class="section"><h3 class="title sectiontitle">Deploying with your own load balancers</h3>
 
<p class="p">If you are using external load balancers for UCP and DTR, you can configure UCP and DTR to use these external load balancers 
by specifying FQDNs in the <code class="ph codeph">loadbalancers</code> dictionary in <code class="ph codeph">group_vars/vars</code>:</p>
 
    
<pre class="pre codeblock"><code>
loadbalancers:
  ucp:
    public_fqdn: external-ucpvip.am2.cloudra.local
  dtr:
    public_fqdn: external-dtrvip.am2.cloudra.local
</code></pre>    
    
    
    
</div>
    

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title58" id="install-docker-ucp-dtr">
<h2 class="title topictitle2" id="ariaid-title58">Installing Docker UCP and DTR on RHEL VMs</h2>

<div class="body">
  
<p class="p">The following playbooks are used to install Docker UCP and DTR on RHEL VMs.</p>
    
  
<ul class="ul">

<li class="li"><code class="ph codeph">playbooks/config_storage_driver.yml</code> prepares drives for local Docker volumes and container images. 
    It also configures Docker with either the <code class="ph codeph">overlay2</code> storage driver (the default) or the 
    <code class="ph codeph">devicemapper</code> storage driver, depending on the value of the <code class="ph codeph">docker_storage_driver</code> variable
    in <code class="ph codeph">group_vars/vars</code>. This playbook was previously called <code class="ph codeph">playbooks/config_docker_lvs.yml</code>
    in earlier releases of the solution.
</li>
  
  
  
<li class="li"><code class="ph codeph">playbooks/install_docker.yml</code> installs Docker along with all of its dependencies.</li>



<li class="li"><code class="ph codeph">playbooks/install_rsyslog.yml</code> installs and configures <strong class="ph b">rsyslog</strong> in the logger node 
and in all Docker nodes. The logger node will be configured to receive all <code class="ph codeph">syslogs</code> on port 514 and 
the Docker nodes will be configured to send all logs (including container logs) to the logger node.</li>





<li class="li"><code class="ph codeph">playbooks/docker_post_config.yml</code> performs a variety of tasks to complete 
the installation of the Docker environment, including configuration of the HTTP/HTTPS proxies, if any, 
and installation of the VMware vSphere Storage for Docker volume plugin.</li>



<li class="li"><code class="ph codeph">playbooks/install_nfs_server.yml</code> installs and configures an NFS server on the NFS node.

<p class="p">This playbook has been updated to configure a third drive which is used to hold the data of the persistent volumes 
created with the NFS provisioner. This default size for this drive is purposefully kept small because 
using the NFS VM to store persistent volumes is not recommended for production use. However, this can be useful for demo purposes.</p>

</li>



<li class="li"><code class="ph codeph">playbooks/install_nfs_clients.yml</code> installs the required packages on the Docker nodes 
to be able to mount an NFS share.</li>



<li class="li"><code class="ph codeph">playbooks/create_main_ucp.yml</code> installs and
configures the first Docker UCP instance on the target node
defined by the group <code class="ph codeph">ucp_main</code> in the
<code class="ph codeph">vm_hosts</code> inventory.</li>



<li class="li"><code class="ph codeph">playbooks/scale_ucp.yml</code> installs and configures
additional instances of UCP on the target nodes defined by
the group <code class="ph codeph">ucp</code> in the
<code class="ph codeph">vm_hosts</code> inventory, except for the node defined in the group
<code class="ph codeph">ucp_main</code>.</li>


	

<li class="li"><code class="ph codeph">playbooks/create_main_dtr.yml</code> installs and
configures the first Docker DTR instance on the target node
defined by the group <code class="ph codeph">dtr_main</code> in the <code class="ph codeph">vm_hosts</code> inventory.</li>

  

<li class="li"><code class="ph codeph">playbooks/config_scheduler.yml</code> configures the scheduler to prevent regular users 
(i.e. non-admin users) scheduling containers on the Docker nodes running instances of UCP and DTR.</li>

	

<li class="li"><code class="ph codeph">playbooks/scale_dtr.yml</code> installs and configures additional instances (or replicas) 
of DTR on the target nodes defined by the group <code class="ph codeph">dtr</code> in the <code class="ph codeph">vm_hosts</code> inventory, 
with the exception of the node defined in the group <code class="ph codeph">dtr_main</code>.</li>


	

<li class="li"><code class="ph codeph">playbooks/reconfigure_dtr.yml</code>	is used to reconfigure DTR with the FQDN of the UCP Load Balancer and also 
enables image scanning.
</li>

  
  
  


</ul>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title59" id="add-rhel-workers">
<h2 class="title topictitle2" id="ariaid-title59">Deploying RHEL workers</h2>

<div class="body">
  
<p class="p">By default, <code class="ph codeph">site.yml</code> will automatically deploy any RHEL (and / or Windows) worker nodes that are declared in the inventory. 
</p>

  
<p class="p">
If you subsequently want additional RHEL worker nodes, add them to the inventory as appropriate and 
then run the playbooks for <a class="xref" href="#create-rhel-vms">Provisioning RHEL VMs</a>, followed by the specific playbooks for RHEL worker nodes outlined below: 
</p>
  
  
<ul class="ul">
	
<li class="li"><code class="ph codeph">playbooks/scale_workers.yml</code> installs and configures additional Linux workers 
on the target nodes defined by the group <code class="ph codeph">worker</code> in the <code class="ph codeph">vm_hosts</code> inventory.</li>

  
</ul>

    
<p class="p">A utility script <code class="ph codeph">scale_worker.sh</code> is provided to assist you in adding worker nodes after the initial deployment.</p>
    
    
</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title60" id="svt-backup-playbooks">
<h2 class="title topictitle2" id="ariaid-title60"><span class="ph"><span class="keyword">HPE SimpliVity</span></span> backup playbooks</h2>

<div class="body">
  
<p class="p">Two playbooks are provided to support the backup of Docker volumes using <span class="ph"><span class="keyword">HPE SimpliVity</span></span> functionality. The playbooks are run by default
when using <code class="ph codeph">site.yml</code> to deploy the solution.</p>

  



<div class="section"><h3 class="title sectiontitle">Configure dummy VMs to backup Docker volumes</h3>

<p class="p">The playbook <code class="ph codeph">playbooks/config_dummy_vms_for_docker_volumes_backup.yml</code> ensures that you can back up Docker volumes 
  that have been created using the vSphere plugin (vDVS) in <span class="ph"><span class="keyword">HPE SimpliVity</span></span>. There is not a straight forward way to do this, 
  so you need to use a workaround. Since all Docker volumes are going to be stored in the <code class="ph codeph">dockvols</code> folder 
  in the datastore(s), you need to create a ‘dummy’ VM per datastore. The <code class="ph codeph">vmx</code>, <code class="ph codeph">vmsd</code> 
  and <code class="ph codeph">vmkd</code> files from this VMs will have to be inside the dockvols folder, 
  so that, when these VMs are backed up, the volumes are backed up as well. Obviously these VMs don’t need 
  to consume any resources and you can keep them powered off.</p>

   
</div>

  
<div class="section"><h3 class="title sectiontitle">Configure <span class="ph"><span class="keyword">HPE SimpliVity</span></span> backups</h3>
  
<p class="p">The playbook <code class="ph codeph">playbooks/config_simplivity_backups.yml</code> configures the  backup policies, defined in the 
  group variables file, on your <span class="ph"><span class="keyword">HPE SimpliVity</span></span> installation.
  It will configure all Docker nodes plus the ‘dummy’ VMs created before, 
  so that existing Docker volumes are also taken into account. The playbook will mainly use the <span class="ph"><span class="keyword">HPE SimpliVity</span></span> REST API 
  to perform these tasks. A reference to the REST API can be found at: 
  <a class="xref" href="https://api.simplivity.com/rest-api_getting-started_overview/rest-api_getting-started_overview_rest-api-overview.html" target="_blank">https://api.simplivity.com/rest-api_getting-started_overview/rest-api_getting-started_overview_rest-api-overview.html</a> </p>
  
</div>


</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title61" id="deploying-windows-workers">
<h1 class="title topictitle1" id="ariaid-title61">Deploying Windows workers</h1>

<div class="body">

<p class="p">The <code class="ph codeph">site.yml</code> playbook will automatically deploy any Windows workers declared in the inventory.
The playbooks should run for approximately 70-80 minutes with 3 Windows workers added to the default deployment 
(depending on your server specifications and the size of your environment). The increase in 
running time is primarily due to the need to update Windows after creating the VMs.  
</p>


<p class="p">This section described the functionality and configuration of the Windows-specific playbooks. It also details how 
  to create the initial Windows template and how to manage deploying Windows worker nodes behind a proxy. </p>

  
</div>

<div class="topic nested1" aria-labelledby="ariaid-title62" id="create-template-win">
<h2 class="title topictitle2" id="ariaid-title62">Create the Windows Template</h2>

<div class="body">
    
<p class="p">To create the Windows VM Template that you will use as the
base for all your Windows worker nodes, you will first
create a Virtual Machine with the OS installed and then convert the Virtual Machine
to a VM Template. The VM Template is created as lean as possible, with any additional
software installs and/or system configuration performed subsequently using Ansible.
</p>

    
<p class="p">As the creation of the template is
a one-off task, this procedure has not been automated. The steps to create a VM template
manually are outlined below.</p>



<p class="p">Log in to vCenter and create a new Virtual Machine with the following
characteristics:</p>


<ul class="ul">
<li class="li">Guest OS Family: Windows, Guest OS Version: Microsoft Windows Server
2016 (64-bit).
</li>


<li class="li">Hard Disk size: 100GB (Thin provisioning), 1 vCPU and 4 GB of RAM. Both vCPU and memory can be altered later after you deploy from this template. 
</li>

    
<li class="li">A single network controller connected to the network or VLAN of your
choice. All VMs will connect to this same network.
</li>

    
<li class="li">Change the network type to VMXNET3, and attach the Windows Server
2016 ISO image from a datastore ensuring you connect the CD/DVD drive on
boot.
</li>

    
<li class="li">Click on the <code class="ph codeph">VM Options</code> tab, and in the <code class="ph codeph">Boot Options</code>
section, select <code class="ph codeph">Force BIOS setup(*)</code> to ensure that the machine enters the
BIOS setup screen on next boot of this VM. This will allow you to adjust the
boot order, placing the virtual CD-ROM in front of your hard drive.</li>

    
<li class="li">Optionally you can remove the floppy drive.						
</li>

</ul>




<p class="p">Install Windows Server 2016:</p>

    
<ul class="ul">
<li class="li">Power on the selected VM and then <code class="ph codeph">Open Console</code>.</li>

    
<li class="li">Once connected to the console, you will be placed in the BIOS setup screen.
Select the <code class="ph codeph">Boot</code> tab, click on CD-ROM Drive and 
 move up the CD-ROM drive above the hard drive. This allows your Windows
Server 2016 ISO image to be loaded first on boot. Save your changes and exit.</li>

    
<li class="li">Enter your choices for Language, Time/Currency Format, Keyboard and then select
<code class="ph codeph">Install Now</code>.</li>

    
<li class="li">Select the OS you want to install, and then select <code class="ph codeph">Custom: Install Windows
Only</code>.</li>

    
<li class="li">Select drive 0, the 100 GB drive you specified earlier, as the location for
installing windows.</li>

<li class="li">Add a password for the Administrator user.</li>

<li class="li">Install VMware Tools and
reboot.</li>

</ul>


<p class="p">Once the VM has re-booted:</p>

    
<ul class="ul">    
<li class="li">Add a temporary network IP address and configure any proxy settings required to download Windows Updates.</li>

<li class="li">Use the <code class="ph codeph">sconfig</code> utility from (MS-DOS) command line to install Windows Updates and enable Remote Desktop.</li>

<li class="li">Perform any other customizations you require at this point.</li>

</ul>
    
    
<p class="p">Prior to converting the VM to Template, run Sysprep: <code class="ph codeph">C:\Windows\System32\Sysprep\Sysprep.exe</code>.</p>

    
<ul class="ul">
<li class="li">Ensure ‘System Out-of-Box Experience (OOBE)’ is selected.</li>

<li class="li">Select the ‘Generalize’ option.</li>

<li class="li">Select ‘Shutdown’ from the Shutdown Options.</li>

</ul>


<p class="p">Shutdown the VM, and untick  <code class="ph codeph">Connect CD/DVD</code> so that the Windows Server 2016 ISO is no
longer  mounted. Boot the Windows VM one final time and enter regional settings applicable to your location
and keyboard mapping, enter an Administrator user password then Shutdown VM.</p>


<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> The <code class="ph codeph">vmware_guest</code> module used by the playbooks will generate a new SID.</div><hr />

    
<p class="p">Turn the VM into a template by right-clicking on your VM and selecting <code class="ph codeph">Template -&gt; Convert to Template</code>.	
This will create a new template visible under VM Templates in Folders, ready for future use.
</p>



</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title63" id="add-windows-workers">
<h2 class="title topictitle2" id="ariaid-title63">Playbooks for adding Windows workers</h2>

<div class="body">
<ul class="ul">  
     
<li class="li"><code class="ph codeph">playbooks/create_vms.yml</code> will create all the necessary Windows 2016 VMs 
for the environment based on the Windows VM Template defined in the <code class="ph codeph">win_vm_template</code> variable. 
Windows workers nodes are 
    defined in the group <code class="ph codeph">win_worker</code> in the <code class="ph codeph">vm_hosts</code> inventory.
</li>
   
    

<li class="li"><code class="ph codeph">playbooks/install_docker.yml</code> installs Docker along with all its dependencies 
on your Windows VMs</li>



<li class="li"><code class="ph codeph">playbooks/scale_workers.yml</code> installs and configures additional Windows workers on the target nodes 
    defined by the group <code class="ph codeph">win_worker</code> in the <code class="ph codeph">vm_hosts</code> inventory.</li>

 
<li class="li"><code class="ph codeph">playbooks/splunk_uf_win.yml</code> installs and configures the Splunk Universal Forwarder on each 
    Windows machine in the inventory.</li>

 
</ul>
 
</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title64" id="windows-config">
<h2 class="title topictitle2" id="ariaid-title64">Windows configuration</h2>

<div class="body">
<div class="section">

<p class="p">Window-related variables are shown in 
   <a class="xref" href="#windows-config__windows-config-table-conref">Table 15</a>.</p>

    

<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="windows-config__windows-config-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 15. </span>Windows variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8184">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8187">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8190">Description</th>
</tr>
</thead><tbody class="tbody">




<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8184 ">win_vm_template</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8187 ">group_vars/vars</td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8190 ">Name of the Windows 2016 VM Template to use. Note that this is the name from a
vCenter perspective, not the hostname.</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8184 ">win_username</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8187 ">group_vars/vars</td>     
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8190 ">Windows user name. The default is <code class="ph codeph">Administrator</code>
</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8184 ">win_password</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8187 "><strong class="ph b">group_vars/vault</strong></td>     
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8190 ">The password for the Windows account.
</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8184 ">windows_vdvs_ps</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8187 ">group_vars/vars</td>     
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8190 ">Variable used to download the PowerShell script that is used to install vDVS for Windows. For example,
<code class="ph codeph">https://raw.githubusercontent.com/vmware/vsphere-storage-for-docker/master/install-vdvs.ps1</code>
</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8184 ">windows_vdvs_path</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8187 ">group_vars/vars</td>     
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8190 ">Variable used to download vSphere Docker Volume Service software. This variable is combined 
with <code class="ph codeph">windows_vdvs_version</code> (below) to generate a URL of the form <code class="ph codeph">&lt;windows_vdvs_path&gt;_&lt;windows_vdvs_version&gt;.zip</code> to download the software. 
For example, to download version 0.21, set <code class="ph codeph">windows_vdvs_path</code> equal to <code class="ph codeph">https://vmware.bintray.com/vDVS/vsphere-storage-for-docker_windows</code> 
and <code class="ph codeph">windows_vdvs_version</code> equal to <code class="ph codeph">0.21</code>
</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8184 ">windows_vdvs_version</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8187 ">group_vars/vars</td> 
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8190 ">Combined with <code class="ph codeph">windows_vdvs_path</code>, this variable is used to generate the URL for downloading the software.</td>
</tr>  

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8184 ">windows_vdvs_directory</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8187 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8190 ">Variable used to determine where vDVS software will be unzipped and installed from. The default is <code class="ph codeph">C:\Users\Administrator\Downloads</code></td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8184 ">windows_winrm_script</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8187 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8190 ">Variable used to determine where the <code class="ph codeph">winrm</code> Powershell script will be downloaded from. See the section 
    <a class="xref" href="#windows-proxy">Deploying Windows workers behind a proxy</a> for more information.

</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8184 ">docker_ee_version_windows</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8187 "><strong class="ph b">group_vars/vars</strong></td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8190 ">It is important that the version of the Docker engine running on your Windows worker nodes is the same as that running on RHEL in the
rest of your cluster. You should use this variable to explicitly match up the versions. For Docker 2.0, the recommended value is 
<code class="ph codeph">'17.06'</code>. If you do not explicitly set this value, you may end up with an incompatible newer version running on
your Windows workers.</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8184 ">windows_update</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8187 ">group_vars/vars</td>    
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8190 ">Variable used to determine if Windows updates are automatically downloaded when installing Docker on Windows worker nodes 
    (in the <code class="ph codeph">playbooks/install_docker.yml</code>). 
Defaults to <code class="ph codeph">true</code>. See the section 
    <a class="xref" href="#windows-proxy">Deploying Windows workers behind a proxy</a> for more information.
</td>
</tr>
    
    
    
</tbody></table>
</div>
   
</div>



<div class="section"><h3 class="title sectiontitle">group_vars/win_worker.yml</h3>


<p class="p">There is a separate file in the <code class="ph codeph">group_vars</code> directory named <code class="ph codeph">win_worker.yml</code> for advanced, Windows-specific configuration. These variables are
used in the following playbooks:  
</p>


<ul class="ul" id="windows-config__windows-playbooks">
<li class="li">playbooks/create_windows_vms.yml</li>

<li class="li">playbooks/install_docker_window.yml</li>

<li class="li">playbooks/scale_workers_win.yml</li>

</ul>


<p class="p">In general, it should not be necessary to modify this file, but the variables are documented in 
    <a class="xref" href="#windows-config__windows-advanced-config-table-content">Table 16</a> for the sake of completeness.</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="windows-config__windows-advanced-config-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 16. </span>Advanced windows variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8424">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8427">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8430">Description</th>
</tr>
</thead><tbody class="tbody"> 

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8424 ">ansible_user</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8427 "><strong class="ph b">group_vars/win_worker.yml</strong></td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8430 ">Defaults to the Windows user account <code class="ph codeph">win_username</code> as specified in <code class="ph codeph">group_vars/vars</code> </td>
</tr>   

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8424 ">ansible_password</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8427 "><strong class="ph b">group_vars/win_worker.yml</strong></td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8430 ">Defaults to the Windows user password <code class="ph codeph">win_password</code> as specified in <code class="ph codeph">group_vars/vault</code></td>
</tr>   

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8424 ">ansible_port</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8427 "><strong class="ph b">group_vars/win_worker.yml</strong></td> 
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8430 ">5986</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8424 ">ansible_connection</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8427 "><strong class="ph b">group_vars/win_worker.yml</strong></td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8430 ">winrm</td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8424 ">ansible_winrm_server_cert_validation</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8427 "><strong class="ph b">group_vars/win_worker.yml</strong></td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8430 ">Defaults to <code class="ph codeph">ignore</code></td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8424 ">ansible_winrm_operation_timeout_sec</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8427 "><strong class="ph b">group_vars/win_worker.yml</strong></td>   
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8430 ">Defaults to <code class="ph codeph">250</code></td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8424 ">ansible_winrm_read_timeout_sec</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8427 "><strong class="ph b">group_vars/win_worker.yml</strong></td>   
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8430 ">Defaults to <code class="ph codeph">300</code></td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8424 ">windows_timezone</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8427 "><strong class="ph b">group_vars/win_worker.yml</strong></td>  
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8430 ">Defaults to <code class="ph codeph">15</code></td>
</tr>




</tbody></table>
</div>
    
</div>
 


</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title65" id="windows-proxy">
<h2 class="title topictitle2" id="ariaid-title65">Deploying Windows workers behind a proxy</h2>

<div class="body">


<div class="section"><h3 class="title sectiontitle">Configuring the <code class="ph codeph">winrm</code> remoting script</h3>
  



<p class="p">The playbooks for deploying Windows workers rely on a Powershell script for remote access from the Ansible machine. The script
<code class="ph codeph">ConfigureRemotingForAnsible.ps1</code> is available online on GitHub and you can configure the playbooks to use
  the script directly as follows:</p>

  
<pre class="pre codeblock"><code>windows_winrm_script: 'https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1'</code></pre>

<p class="p">
However, if the deployment is running behind a proxy, you may need to make this script available locally.
</p>
  

<ol class="ol">
<li class="li">Download the script:    
<pre class="pre codeblock"><code>wget https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1
</code></pre>      
</li>

  
<li class="li">Deploy a local HTTP server, enabling port 80, for example:
<pre class="pre codeblock"><code>yum install httpd
systemctl enable httpd
systemctl start httpd
firewall-cmd --permanent --add-port 80/tcp --zone=pubpic
firewall-cmd --reload
</code></pre>    
</li>


<li class="li">Copy the downloaded script to the web server:
<pre class="pre codeblock"><code>cp ConfigureRemotingForAnsible.ps1 /var/www/html
</code></pre>
</li>


<li class="li">Configure the variable to point at the local web server, for example,  
<pre class="pre codeblock"><code>windows_winrm_script: 'http://10.10.174.230/ConfigureRemotingForAnsible.ps1'</code></pre>  
</li>


</ol>



</div>


</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title66" id="lifecycle-windows">
<h2 class="title topictitle2" id="ariaid-title66">Windows operating system and Docker EE</h2>

<div class="body">
<div class="section">



<p class="p">Docker Enterprise Edition for Windows Server (Docker EE) enables native Docker containers on Windows Server. 
This solution has been tested with Windows worker nodes running Windows Server 2016 
and with Docker EE 17.06. More recent versions of Windows Server may work but have not been tested.</p>


    
<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> 
<p class="p">Docker Universal Control Plane is not currently supported on Windows Server 1709 due to image incompatibility issues. 
For more information, see the Docker documentation <a class="xref" href="https://docs.docker.com/install/windows/docker-ee/" target="_blank">Install Docker Enterprise Edition for Windows Server</a></p>
        

<p class="p">This solution recommends that you only run Windows Server 2016 on your Windows worker nodes 
and that you install any required updates to your Windows nodes in a timely manner.</p>

</div><hr />
    

  
<p class="p">For information on how to update Docker EE on Windows Server 2016, see the Docker 
documentation <a class="xref" href="https://docs.docker.com/install/windows/docker-ee/#update-docker-ee" target="_blank">Update Docker EE</a></p>




</div>

</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title67" id="deploying-sysdig">
<h1 class="title topictitle1" id="ariaid-title67">Deploying Sysdig monitoring</h1>

<div class="body">

<p class="p"> By default, the playbooks for deploying Sysdig are commented out in
<code class="ph codeph">site.yml</code> and must be explicitly enabled in that file if you want it included in the initial
  deployment. Alternatively, you can run the specific playbooks detailed in this section in a stand-alone manner, 
  subsequent to the initial deployment. </p>


<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> <p class="p">By default, you must have outgoing port 6666 open in your firewall, to allow data to flow
to <code class="ph codeph">collector.sysdigcloud.com</code>. You can configure the agent to use a different port by setting the 
<code class="ph codeph">collector_port</code> parameter in the configuration file <code class="ph codeph">/opt/draios/etc/dragent.yaml</code>. For more
information, see the relevant Sysdig documentation at 
<a class="xref" href="https://support.sysdig.com/hc/en-us/articles/204205969" target="_blank">https://support.sysdig.com/hc/en-us/articles/204205969</a>.</p>

   
<p class="p">If you are using a proxy, it must be configured to be "fully-transparent". Non-transparent proxies will not allow the agent to connect.</p>
   
</div><hr />

  
</div>

<div class="topic nested1" aria-labelledby="ariaid-title68" id="sysdig">
<h2 class="title topictitle2" id="ariaid-title68">Monitoring with Sysdig</h2>

<div class="body">
<p class="p">Sysdig's approach to Docker monitoring uses transparent instrumentation to see inside
containers from the outside, with no need for agents in each container. Metrics from Docker
containers, and from your applications running inside them, are aggregated in real-time across
each service to provide meaningful monitoring dashboards and alerts for your application.
</p>

<p class="p"><a class="xref" href="#sysdig__sysdig-arch">Figure 3</a> provides an overview of the Sysdig architecture.
</p>

  
<div class="fig fignone" id="sysdig__sysdig-arch">
  
  <img class="image" src="media/sysdig-architecture.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 3. </b></span>Sysdig architecture</span></div><p> </p></div>
  
  
<p class="p"><strong class="ph b">Sysdig Monitor</strong> allows you to analyze response times, application performance metrics,
container and server utilization metrics, and network metrics. You can build dashboards across
applications, micro-services, container and networks, and explore metadata from Docker,
Kubernetes, Mesos and AWS.  For more information, see the 
<a class="xref" href="https://www.youtube.com/watch?v=NR9XLZw0ndo" target="_blank">Sysdig Container Monitoring</a> video overview 
and the <a class="xref" href="https://sysdig.teachable.com/p/sysdig-101" target="_blank">Sysdig Monitor 101</a> training course. </p>


<p class="p"><strong class="ph b">Sysdig Secure</strong> provides security at the orchestrator as well as the container level.
You create service-aware policies that allow you to take actions (like killing a container) or
send alerts (to Slack, Splunk, etc) whenever a policy violation occurs. All commands are
audited to help you identify anomalous actions, along with taking snapshots of all activities
pre-and-post a policy violation. For more information, see the 
<a class="xref" href="https://www.youtube.com/watch?v=e_kdjHjK7mY" target="_blank">Sysdig Container Monitoring</a> video overview 
and the <a class="xref" href="https://sysdig.teachable.com/p/sysdig-secure-101" target="_blank">Sysdig Secure 101</a> training course. </p>


<p class="p">The implementation in this solution uses the Software as a Service (SaaS) version of Sysdig.
The playbooks deploy Sysdig Agent software on each UCP, DTR and Linux worker node, as well as the NFS, logger and load balancer VMs
and captured data is relayed back to your Sysdig SaaS Cloud portal. The deployment provides access to a 
90 day try-and-buy, fully featured version of the Sysdig software.</p>


<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> The Sysdig functionality is not turned on by default in this solution - see
the section on Configuration for more information on how to enable Sysdig. 
 For more information on how to access the 90 day try-and-buy version, see the 
 GitHub repository at <span class="ph"><a class="xref" href="https://github.com/HewlettPackard/Docker-SimpliVity" target="_blank"><span class="keyword">https://github.com/HewlettPackard/Docker-SimpliVity</span></a></span>.
</div><hr />

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title69" id="install-sysdig">
<h2 class="title topictitle2" id="ariaid-title69">Playbooks for installing Sysdig on RHEL</h2>

<div class="body">
<ul class="ul">
<li class="li"><code class="ph codeph">playbooks/install_sysdig.yml</code> is used to configure Sysdig. It opens the required port in the firewall, 
and installs the latest version of the Sysdig agent image on the nodes. By default, this playbook is commented out in <code class="ph codeph">site.yml</code>, so if you want to use the solution to 
automatically configure Sysdig, you must uncomment this line. Alternatively, you can run this playbook standalone, post-deployment.
</li>

<li class="li"><code class="ph codeph">playbooks/k8s-install-sysdig.yml</code> is used to configure Sysdig for Kubernetes.</li>
        
</ul>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title70" id="monitoring-config-sysdig">
<h2 class="title topictitle2" id="ariaid-title70">Sysdig configuration</h2>

<div class="body">


<div class="section" id="monitoring-config-sysdig__sysdig-config-docker-swarm"><h3 class="title sectiontitle">Sysdig configuration for Docker swarm</h3>
   
   

<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="monitoring-config-sysdig__sysdig-config-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 17. </span>Sysdig variables for Docker swarm</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8832">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8835">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8838">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8832 ">sysdig_access_key</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8835 "><strong class="ph b">group_vars/vault</strong></td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8838 ">After the activation of your account on the Sysdig portal, you will be 
provided with your access key which will be used by the playbooks to install the agent on each UCP, DTR and Linux worker node, 
as well as the NFS, logger and load balancer VMs.
</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8832 ">sysdig_agent</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8835 ">group_vars/vars</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8838 ">Specifies the URL to the Sysdig Linux native install agent, for example, <code class="ph codeph">https://s3.amazonaws.com/download.draios.com/stable/install-agent</code></td>
</tr>    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8832 ">sysdig_tags</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8835 ">group_vars/vars</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8838 ">Tagging your hosts is highly recommended. Tags allow you to sort the nodes of your infrastructure 
   into custom groups in Sysdig Monitor. Specify location, role, and owner in the format: <code class="ph codeph">'location:City,role:Enterprise CaaS,owner:Customer Name'</code></td>
</tr>    
</tbody></table>
</div>



</div>




<div class="section" id="monitoring-config-sysdig__sysdig-config-k8s"><h3 class="title sectiontitle">Sysdig configuration for Kubernetes</h3>
   
   

<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="monitoring-config-sysdig__sysdig-config-k8s-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 18. </span>Sysdig variables for Kubernetes</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8912">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8915">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e8918">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8912 ">sysdig_access_key</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8915 "><strong class="ph b">group_vars/vault</strong></td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8918 ">After the activation of your account on the Sysdig portal, you will be 
provided with your access key which will be used by the playbooks to install the agent on each UCP, DTR and Linux Kubernetes worker node.
</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8912 ">sysdig_restricted_control_role</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8915 ">group_vars/vars</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8918 ">The Sysdig service account requires the 'Restricted Control' grant. So this value will typically be set as follows:
<pre class="pre codeblock"><code><strong class="ph b">sysdig_restricted_control_role: 'Restricted Control'</strong></code></pre>
</td>  
</tr>    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8912 ">k8s_cluster</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8915 ">group_vars/vars</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e8918 ">This should match the cluster name displayed when you source the environment setup script, for example. 
<pre class="pre codeblock"><code># source env.sh
Cluster "<strong class="ph b">ucp_hpe-ucp.cloudra.local</strong>:6443_admin" set.
User "ucp_hpe-ucp.cloudra.local:6443_admin" set.
</code></pre>
For more information, see the section on installing the UCP client bundle in <a class="xref" href="#deploying-sysdig-k8s">Deploying Sysdig monitoring on Kubernetes</a>,   
</td>
</tr>    
</tbody></table>
</div>



</div>






   
   
   

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title71" id="sysdig-trial">
<h2 class="title topictitle2" id="ariaid-title71">Registering for Sysdig trial</h2>

<div class="body">

<p class="p">Hewlett Packard Enterprise has teamed up with Sysdig to offer a fully featured 90-day trial version of 
    Sysdig Monitor and Secure as part of the <span class="ph"><span class="keyword">HPE Express Containers with Docker Enterprise Edition on HPE SimpliVity</span></span>. 
    For more details on how to sign up, see the GitHub repository at <span class="ph"><a class="xref" href="https://github.com/HewlettPackard/Docker-SimpliVity" target="_blank"><span class="keyword">https://github.com/HewlettPackard/Docker-SimpliVity</span></a></span>.</p>


<p class="p">Browse to the registration page at <a class="xref" href="https://sysdig.com/partner-hpe/" target="_blank">https://sysdig.com/partner-hpe/</a>
    as shown in <a class="xref" href="#sysdig-trial__sysdig-hpe">Figure 4</a>. 
</p>

 
<div class="fig fignone" id="sysdig-trial__sysdig-hpe">

<br /><img class="image" src="media/sysdig-hpe.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 4. </b></span>Sysdig and HPE</span></div><p> </p></div>


<p class="p">Scroll to the registration form at the bottom of the page, as shown in <a class="xref" href="#sysdig-trial__sysdig-registration">Figure 5</a>. </p>

  
<div class="fig fignone" id="sysdig-trial__sysdig-registration">

<br /><img class="image" src="media/sysdig-registration.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 5. </b></span>Sysdig trial registration</span></div><p> </p></div>
  

<p class="p">After completing and submitting the form, you will receive an email containing an activation link, 
as shown in <a class="xref" href="#sysdig-trial__sysdig-email">Figure 6</a>. </p>



<div class="fig fignone" id="sysdig-trial__sysdig-email">

<br /><img class="image" src="media/sysdig-email.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 6. </b></span>Sysdig email</span></div><p> </p></div>
  
 
<p class="p">Clicking on the activation link will bring you to the Sysdig Monitor welcome page,
as shown in <a class="xref" href="#sysdig-trial__sysdig-monitor-welcome">Figure 7</a>. </p>


<div class="fig fignone" id="sysdig-trial__sysdig-monitor-welcome">

<br /><img class="image" src="media/sysdig-monitor-welcome.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 7. </b></span>Sysdig Monitor welcome page</span></div><p> </p></div>
  


<p class="p">Click  the <code class="ph codeph">Next</code> button to set up you environment,
as shown in <a class="xref" href="#sysdig-trial__sysdig-setup-env">Figure 8</a>.</p>


<div class="fig fignone" id="sysdig-trial__sysdig-setup-env">

<br /><img class="image" src="media/sysdig-setup-env.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 8. </b></span>Sysdig Monitor set up environment</span></div><p> </p></div>
 
  
  
<div class="section"><h3 class="title sectiontitle">Sysdig Monitoring for Kubernetes</h3>

<p class="p">If you are deploying Sysdig monitoring on Kubernetes, select the <code class="ph codeph">Kubernetes | GKE | OpenShift</code> option. 
You will be presented with an access code, as shown in 
<a class="xref" href="#sysdig-trial__sysdig-k8s">Figure 9</a>.
</p>

  
<div class="fig fignone" id="sysdig-trial__sysdig-k8s">

<br /><img class="image" src="media/sysdig-k8s.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 9. </b></span>Sysdig Monitor access code for Kubernetes</span></div><p> </p></div>
 

<p class="p">Use the <code class="ph codeph">sysdig_access_key</code> field in your <code class="ph codeph">group_vars/vault</code>,
as described in the section <a class="xref" href="#monitoring-config-sysdig__sysdig-config-k8s">Sysdig configuration for Kubernetes</a>. 
Once you deploy your environment and your Kubernetes nodes connect to the Sysdig SaaS platform, 
Sysdig will automatically display information regarding your setup,
as shown in <a class="xref" href="#sysdig-trial__sysdig-k8s-spotlight">Figure 10</a>.
</p>


<div class="fig fignone" id="sysdig-trial__sysdig-k8s-spotlight">

<br /><img class="image" src="media/sysdig-k8s-spotlight.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 10. </b></span>Sysdig Monitor Spotlight for Kubernetes</span></div><p> </p></div>
 

<p class="p">Select <code class="ph codeph">View Dashboard</code> for an entry point to accessing all your monitoring data.
Alternatively, you can browse to 
<a class="xref" href="https://app.sysdigcloud.com" target="_blank">https://app.sysdigcloud.com</a> at any time to access your dashboards.  
</p>

  
</div>

  
<div class="section"><h3 class="title sectiontitle">Sysdig Monitor for Docker swarm</h3>

<p class="p">If you are deploying Sysdig monitoring on Docker swarm, select the <code class="ph codeph">Non-Orchestrated: <strong class="ph b">Native Linux</strong></code> option.
You will be presented with a screen containing details for the URL to download the Sysdig agent, along with your access code
embedded in the command, as shown in <a class="xref" href="#sysdig-trial__sysdig-docker">Figure 11</a>.
</p>

  
<div class="fig fignone" id="sysdig-trial__sysdig-docker">

<br /><img class="image" src="media/sysdig-docker.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 11. </b></span>Sysdig Monitor download location and access code for Docker</span></div><p> </p></div>
 
  
<p class="p">The download URL is used in the <code class="ph codeph">sysdig_agent</code> field in <code class="ph codeph">group_vars/vars</code> while the access code is 
stored in the <code class="ph codeph">sysdig_access_key</code> field in your <code class="ph codeph">group_vars/vault</code>,
as described in the section <a class="xref" href="#monitoring-config-sysdig__sysdig-config-docker-swarm">Sysdig configuration for Docker swarm</a>. 
</p>
  
 
<p class="p">Once you deploy your environment and your Docker swarm nodes connect to the Sysdig SaaS platform, 
Sysdig will automatically display information regarding your setup. Alternatively, you can browse to 
<a class="xref" href="https://app.sysdigcloud.com" target="_blank">https://app.sysdigcloud.com</a> at any time to access your dashboards.</p>



</div>

  
</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title72" id="deploying-sysdig-k8s">
<h2 class="title topictitle2" id="ariaid-title72">Deploying Sysdig monitoring on Kubernetes</h2>

<div class="body">

<div class="section"><h3 class="title sectiontitle">Prerequisites</h3>

  
<ul class="ul">
<li class="li">Install the <code class="ph codeph">kubectl</code> binary on your Ansible box -  
  see <a class="xref" href="https://docs.docker.com/ee/ucp/user-access/kubectl/" target="_blank">https://docs.docker.com/ee/ucp/user-access/kubectl/</a>.

</li>
  
<li class="li">Install the UCP Client bundle for the <code class="ph codeph">admin</code> user - 
  see <a class="xref" href="https://docs.docker.com/ee/ucp/user-access/cli/" target="_blank">https://docs.docker.com/ee/ucp/user-access/cli/</a></li>
  
<li class="li">Confirm that you can connect to the cluster by running a test command, for example, <code class="ph codeph">kubectl get nodes</code></li>

<li class="li">Ensure that you have configured the required variables, as described in the section 
  <a class="xref" href="#monitoring-config-sysdig__sysdig-config-k8s">Sysdig configuration for Kubernetes</a>.
  
For example, you add the relevant variables in the <code class="ph codeph">group_vars/vars</code> file.

<pre class="pre codeblock"><code>sysdig_restricted_control_role: 'Restricted Control'
k8s_cluster: ucp_gab-ucp.cloudra.local
</code></pre>
  
You should add the access key  to the encrypted <code class="ph codeph">group_vars/vault</code> using the command <code class="ph codeph">ansible-vault edit group_vars/vault</code>.
  
<pre class="pre codeblock"><code>sysdig_access_key: '10****97-9160-****-9061-84bfd0f****0'
</code></pre>
  
</li>
  
</ul>
  
</div>

  
<div class="section"><h3 class="title sectiontitle">Running the playbook</h3>
  
  
<pre class="pre codeblock"><code># cd <span class="ph"><span class="keyword">Docker-SimpliVity</span></span>
# ansible-playbook -i vm_hosts playbooks/k8s-install-sysdig.yml --vault-password-file .vault_pass
</code></pre> 
 
</div>




<div class="section">
<p class="p">Using the Sysdig software as a solution (SaaS) website <a class="xref" href="https://app.sysdigcloud.com" target="_blank">https://app.sysdigcloud.com</a>, you are able to view, analyze and
inspect various different dashboards. Initially, you will just see the monitoring information for the 
infrastructure itself. Deploy a sample application, as detailed in the section <a class="xref" href="#k8s-example-guestbook">Kubernetes guestbook example with Redis</a>,
  and use the Sysdig solution to analyze the different facets of the deployed application. 

</p>
  
  
</div>


  
<p class="p"></p>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title73" id="deploying-sysdig-swarm">
<h2 class="title topictitle2" id="ariaid-title73">Deploying Sysdig monitoring on Docker Swarm</h2>

<div class="body">
  
<p class="p">The playbook <code class="ph codeph">playbooks/install_sysdig.yml</code> is used to automate the
configuration of the SaaS setup. By default, this playbook is commented out in
<code class="ph codeph">site.yml</code> and must be explicitly enabled. An access key variable must be set
in the <code class="ph codeph">group_vars/vault</code> file as detailed in <a class="xref" href="#monitoring-config-sysdig__sysdig-config-table-content">Table 17</a>.</p>
  
  
<pre class="pre codeblock"><code># cd <span class="ph"><span class="keyword">Docker-SimpliVity</span></span>
# ansible-playbook -i vm_hosts playbooks/install_sysdig.yml --vault-password-file .vault_pass
</code></pre> 

<p class="p">Using the Sysdig software as a solution (SaaS) website <a class="xref" href="https://app.sysdigcloud.com" target="_blank">https://app.sysdigcloud.com</a>, you are able to view, analyze and
inspect various different dashboards.</p>



</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title74" id="deploying-splunk">
<h1 class="title topictitle1" id="ariaid-title74">Deploying Splunk</h1>

<div class="body">
<p class="p">This section provides an overview of Splunk, outlines how to configure and run the relevant playbooks and 
    shows how to access the UI to see the resultant Docker and Kubernetes dashboards.</p>
    
</div>

<div class="topic nested1" aria-labelledby="ariaid-title75" id="splunk">
<h2 class="title topictitle2" id="ariaid-title75">Monitoring with Splunk</h2>

<div class="body">

    
<p class="p">Splunk Enterprise allows you to collect and index any data from any source, and to monitor systems and infrastructure in real
time to preempt issues before they happen. It allows you to analyze your data to understand trends, patterns of activity and
behavior, giving you valuable intelligence across your entire organization.</p>
    
    
<p class="p">This solution allows you to integrate your Containers-as-a-Service (CaaS) deployment with an existing Splunk Enterprise installation or to deploy a stand-alone Splunk Enterprise demo environment as
a Docker stack in your cloud. In both instances, Universal Forwarders are used to collect data from your applications running on your Linux and Windows worker nodes 
in your cloud, as well as log data from the Docker platform itself and from the infrastructure VMs  and servers. 
<a class="xref" href="#splunk__splunk-arch">Figure 12</a> shows the Splunk architecture.
</p>

    
<div class="fig fignone" id="splunk__splunk-arch">

<img class="image" src="media/splunk-architecture.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 12. </b></span>Splunk architecture</span></div><p> </p></div>



<p class="p">All the Universal Forwarders run natively on the operating system to allow
greater flexibility in terms of configuration options. Each forwarder sends the data it collects to one or more indexers in the central
Splunk. </p>

    
<p class="p"><strong class="ph b">Linux worker nodes:</strong> The Universal Forwarders on the Linux worker nodes collect log and metrics data. The log data includes:</p>
    

<ul class="ul">
<li class="li"><code class="ph codeph">/var/log/messages</code> from the Docker host (including the daemon engine logs)</li>

<li class="li"><code class="ph codeph">/var/log/secure</code> from the Docker hosts</li>

<li class="li">container logs via a Splunk technical add-on</li>

</ul>


<p class="p">The metrics data is collected via a technical add-on and includes:</p>


<ul class="ul">
<li class="li"><code class="ph codeph">docker stats</code></li>

<li class="li"><code class="ph codeph">docker top</code></li>

<li class="li"><code class="ph codeph">docker events</code></li>

<li class="li"><code class="ph codeph">docker service stats</code></li>

</ul>

    
<p class="p"><strong class="ph b">Windows worker nodes:</strong> The Universal Forwarders running on the  Windows worker nodes collect the following data:</p>

<ul class="ul">
<li class="li">Windows logs</li>

<li class="li">CPU stats</li>

<li class="li">Memory stats</li>

<li class="li">Network Interface stats</li>

<li class="li">and more</li>

</ul>


<p class="p">For more information on configuring standalone Splunk for Linux and Windows worker nodes, see the section on 
    <a class="xref" href="#splunk-pre-deploy">Splunk prerequisites</a>.
 
</p>
   
 

<p class="p"><strong class="ph b">UCP and ESXi:</strong> UCP operational logs and ESXi logs are forwarded to the logger VM  via TCP ports 1514 and 514 respectively. 
Port 1514 is assigned a special <code class="ph codeph">sourcetype</code> of <code class="ph codeph">ucp</code> which is then used by the 
Splunk Docker APP to interpret UCP logs. The Universal Forwarder runs the rsyslog daemon which will record the 
log messages coming from the ESX machines into the <code class="ph codeph">/var/log/messages</code> file on the VM. </p>

 
 
    
<div class="p"><strong class="ph b">Non-Docker VMs:</strong> Other VMs, for example, NFS, use a Splunk <code class="ph codeph">monitor</code> to collect and 
forward data from the following files:
    
<ul class="ul">
<li class="li">/var/log/messages</li>

<li class="li">/var/log/secure (Red Hat)</li>

</ul>


<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> You can configure the list of files monitored by the Universal Forwarder.</div><hr />

</div>

    
<p class="p">Other syslog senders can be configured to send their data to the logger VM or directly to central Splunk.</p>
       

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title76" id="install-splunk">
<h2 class="title topictitle2" id="ariaid-title76">Playbooks for installing Splunk</h2>

<div class="body">
  
<p class="p">The following playbooks are used to deploy Splunk.</p>
  
  
<ul class="ul">
<li class="li"><code class="ph codeph">playbooks/splunk_demo.yml</code> installs a demo of Splunk Enterprise in the cluster (if the <code class="ph codeph">splunk_demo</code> deployment option is selected. A value of <code class="ph codeph">splunk</code>
is used to configure an external production Splunk deployment.)</li>
    

<li class="li"><code class="ph codeph">playbooks/splunk_uf.yml</code> installs and configures the Splunk Universal Forwarder on each Linux and Windows machine in the inventory</li>



</ul>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title77" id="monitoring-config-splunk">
<h2 class="title topictitle2" id="ariaid-title77">Splunk configuration</h2>

<div class="body">
<p class="p">This solution supports two types of Splunk deployment. Firstly, there is a built-in deployment 
useful for demos and for getting up to speed with Splunk. Alternatively, the solution can be configured
to interact with a standalone, production Splunk deployment that you set up independently.  In this case, you must
explicitly configure the universal forwarders with external "forward servers" (Splunk indexers), whereas this
happens automatically with the built-in option.

</p>


<p class="p">In the standalone deployment, you can enable SSL authentication between the universal forwarders and
the indexers, by setting the <code class="ph codeph">splunk_ssl</code> variable to <code class="ph codeph">yes</code>
in the file <code class="ph codeph">group_vars/vars</code>. The built-in deployment does not support SSL and so, in this instance,
the value of the <code class="ph codeph">splunk_ssl</code> variable is ignored. For more information on enabling SSL, see 
<a class="xref" href="#splunk-ssl">Appendix C</a>.</p>



  
</div>

<div class="topic nested2" aria-labelledby="ariaid-title78" id="splunk-pre-deploy">
<h3 class="title topictitle3" id="ariaid-title78">Splunk prerequisites</h3>

<div class="body">
<div class="section">
  
<p class="p">You should select the Splunk deployment type that you require  by setting the variable 
<code class="ph codeph">monitoring_stack</code> in the <code class="ph codeph">group_vars/vars</code> file to either <strong class="ph b">splunk</strong>,
 for a standalone Splunk deployment, or <strong class="ph b">splunk_demo</strong> for the built-in version.
If you omit this variable, or if it has an invalid value, no Splunk deployment will be configured.
</p>

  
<p class="p">For both types of deployment, you need to download the Splunk Universal forwarder images/packages from 
<a class="xref" href="https://www.splunk.com/en_us/download/universal-forwarder.html" target="_blank">https://www.splunk.com/en_us/download/universal-forwarder.html</a>. 
 Packages are available for 64-bit Linux  and 64-bit Windows 8.1/Windows 10.  
 Download the RPM package for Linux 64-bit (2.6+ kernel Linux distributions) to <code class="ph codeph">./files/splunk/linux</code>.
If you are deploying Windows nodes, download the MSI package for Windows 64 bit to <code class="ph codeph">./files/splunk/windows</code>.
For a dual Linux/Windows deployment, the images and packages must have same name and version, along with the appropriate extensions, for example:
</p>
  

<ul class="ul">
<li class="li">files/splunk/windows/splunkforwarder-7.1.2.msi</li>

<li class="li">files/splunk/linux/splunkforwarder-7.1.2.rpm</li>

</ul>


<p class="p">You need to set the variable <code class="ph codeph">splunk_architecture_universal_forwarder_package</code> to
the name you selected for the package(s), not including the file extension. Depending on the Splunk deployment you have chosen,
edit the file <code class="ph codeph">templates/splunk/<strong class="ph b">splunk</strong>/vars.yml</code> or the file <code class="ph codeph">templates/splunk/<strong class="ph b">splunk_demo</strong>/vars.yml</code>
and set the variable, for example:   
</p>

  
<pre class="pre codeblock"><code>splunk_architecture_universal_forwarder_package: 'splunkforwarder-7.1.2'
</code></pre>  

<p class="p">As of Splunk version 7.1, the Splunk universal forwarder must be deployed with a password. 
  This password is specified using the variable <code class="ph codeph">splunk_uf_password</code> which is configured in <code class="ph codeph">group_vars/vault</code>. </p>



<p class="p">If you are using a standalone Splunk deployment, you must specify the list of indexers using the variable
<code class="ph codeph">splunk_architecture_forward_servers</code> in <code class="ph codeph">group_vars/vars</code>, for example: </p>

  
<pre class="pre codeblock"><code>splunk_architecture_forward_servers:
- splunk-indexer1.cloudra.local:9997
- splunk-indexer2.cloudra.local:9997</code></pre>  
 
<p class="p">By default, the indexers are configured in a single load balancing group. This can be
changed by editing the file <code class="ph codeph">outputs.conf.j2</code> in the folder <code class="ph codeph">templates/splunk/splunk/</code>.
For more information on  forwarding using Universal Forwarder, see the Splunk documentation at
<a class="xref" href="http://docs.splunk.com/Documentation/Forwarder/7.1.2/Forwarder/Configureforwardingwithoutputs.conf" target="_blank">http://docs.splunk.com/Documentation/Forwarder/7.1.2/Forwarder/Configureforwardingwithoutputs.conf</a>.</p>



<p class="p">On your standalone Splunk installation, you need to install the following add-ons and apps.</p>


<p class="p">To monitor <strong class="ph b">Linux worker nodes</strong>, the <strong class="ph b">Docker app</strong> should be installed on central Splunk. 
More information on this Docker app can be found at 
<a class="xref" href="https://github.com/splunk/docker-itmonitoring" target="_blank">https://github.com/splunk/docker-itmonitoring</a>  and at
<a class="xref" href="https://hub.docker.com/r/splunk/universalforwarder/" target="_blank">https://hub.docker.com/r/splunk/universalforwarder/</a>.

</p>

    
    
<p class="p">To monitor the <strong class="ph b">Windows worker nodes</strong>, install the <strong class="ph b">Splunk App for Windows Infrastructure</strong> on   
  central Splunk along with its dependencies:</p>

  
<ul class="ul" id="splunk-pre-deploy__splunk-mvi2-versions">
<li class="li">Splunk App for Windows Infrastructure version 1.4.4. The Splunk App for Windows Infrastructure is not compatible with the Splunk Add-on for Windows 5.0 at this time. See <a class="xref" href="https://splunkbase.splunk.com/app/1680/" target="_blank">https://splunkbase.splunk.com/app/1680/</a></li>

<li class="li">Splunk Add-on for Microsoft Windows version 4.8.4 - see <a class="xref" href="https://splunkbase.splunk.com/app/742/" target="_blank">https://splunkbase.splunk.com/app/742/</a></li>

<li class="li">Splunk Add-On for Microsoft Active Directory version 1.0.0 - see <a class="xref" href="https://splunkbase.splunk.com/app/3207/" target="_blank">https://splunkbase.splunk.com/app/3207/</a></li>

<li class="li">Splunk Add-on for Microsoft Windows DNS version 1.0.1 (if this is not installed on central Splunk, you will see yellow icons on some dashboards with 
the message <code class="ph codeph">eventtype wineventlog-dns does not exist or is disabled</code>) -  see <a class="xref" href="https://splunkbase.splunk.com/app/3208/" target="_blank">https://splunkbase.splunk.com/app/3208/</a></li>

<li class="li">Splunk Supporting Add-on for Active Directory version 2.1.7 (if this is not installed on central Splunk, you will see yellow icons on some dashboards with 
the message <code class="ph codeph">eventtype wineventlog-ds does not exist or is disabled</code>) - see <a class="xref" href="https://splunkbase.splunk.com/app/1151/" target="_blank">https://splunkbase.splunk.com/app/1151/</a></li>

</ul>



<p class="p">If you want to use your own certificates in your standalone Splunk deployment
  to secure the communications between the indexers and the universal forwarders,
  see the subsequent section <a class="xref" href="#splunk-ssl">Enabling SSL</a>.
</p>

  


  
  
<p class="p">You can specify advanced Splunk configuration in the following files:  </p>

<ul class="ul">
<li class="li">files/splunk/linux/SPLUNK_HOME</li>

<li class="li">files/splunk/linux/DOCKER_TAS</li>

<li class="li">files/splunk/windows/SPLUNK_HOME</li>

</ul>

<p class="p">These files will be copied as-is to the systems running the universal forwarder.</p>



</div>

</div>

</div>
<div class="topic nested2" aria-labelledby="ariaid-title79" id="splunk-ucp-syslog">
<h3 class="title topictitle3" id="ariaid-title79">Configuring syslog in UCP</h3>

<div class="body">
<p class="p">In order to see some data in the UCP operational dashboard, you need to have UCP send its
logs to the VM configured in the [logger] group. For example, for the following <code class="ph codeph">vm_host</code> file:</p>


<pre class="pre codeblock"><code>[logger]
hpe-logger ip_addr='10.60.59.24/16' esxi_host='esxi-hpe-2.cloudra.local'</code></pre>

<p class="p">This will configure UCP to send its logs to <code class="ph codeph">hpe-logger.cloudra.local:1514</code>. You
need to select the TCP protocol as shown in <a class="xref" href="#splunk-ucp-syslog__ucp-config-syslog">Figure 13</a>.</p>


<div class="fig fignone" id="splunk-ucp-syslog__ucp-config-syslog">

<img class="image" src="media/ucp-config-syslog.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 13. </b></span>Configure Remote Syslog Server in UCP</span></div><p> </p></div>



</div>

</div>
<div class="topic nested2" aria-labelledby="ariaid-title80" id="splunk-esx-syslog">
<h3 class="title topictitle3" id="ariaid-title80">Configuring syslog in ESX</h3>

<div class="body">
<p class="p">This configuration must be done manually for each ESX server. The syslog server should be the
server configured in the [logger] group in your <code class="ph codeph">vm_hosts</code> inventory. The protocol should be <code class="ph codeph">tcp</code> and
the port <code class="ph codeph">514</code> as shown in <a class="xref" href="#splunk-esx-syslog__esx-config-syslog">Figure 14</a>.</p>


<div class="fig fignone" id="splunk-esx-syslog__esx-config-syslog">

<img class="image" src="media/esx-config-syslog.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 14. </b></span>Configure Syslog on ESXi Hosts</span></div><p> </p></div>


<p class="p">For more information, see the VMware documentation at
<a class="xref" href="https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.security.doc/GUID-9F67DB52-F469-451F-B6C8-DAE8D95976E7.html" target="_blank">https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.security.doc/GUID-9F67DB52-F469-451F-B6C8-DAE8D95976E7.html</a>.</p>


</div>

</div>
<div class="topic nested2" aria-labelledby="ariaid-title81" id="splunk-limitations">
<h3 class="title topictitle3" id="ariaid-title81">Limitations</h3>

<div class="body">
<ul class="ul">
<li class="li">The Dockerized Splunk App has a number of open issues
<ul class="ul">
<li class="li"><a class="xref" href="https://github.com/splunk/docker-itmonitoring/issues/19" target="_blank">https://github.com/splunk/docker-itmonitoring/issues/19</a></li>

<li class="li"><a class="xref" href="https://github.com/splunk/docker-itmonitoring/issues/20" target="_blank">https://github.com/splunk/docker-itmonitoring/issues/20</a></li>

</ul>

</li>

<li class="li">The Docker events tab is not working</li>

</ul>

</div>

</div>
</div>
<div class="topic nested1" aria-labelledby="ariaid-title82" id="splunk-ui">
<h2 class="title topictitle2" id="ariaid-title82">Accessing Splunk UI</h2>

<div class="body">
  
<p class="p">After the installation is complete, the Splunk UI can be reached at <code class="ph codeph">http://&lt;fqdn&gt;:8000</code>,
where <code class="ph codeph">&lt;fqdn&gt;</code> is the FQDN of one of your Linux Docker nodes. Mesh routing does not currently work on
Windows so you must use a Linux node to access the UI. For example: </p>

  
<pre class="pre codeblock"><code>http://hpe-ucp01.am2.cloudra.local:8000/</code></pre>  
  
<p class="p">The default username and password for Splunk is <code class="ph codeph">admin</code> / <code class="ph codeph">changeme</code>.</p>
  
  
<p class="p">Use the <code class="ph codeph">Docker App</code> to view the Docker overview
as shown in <a class="xref" href="#splunk-ui__splunk-ui-docker">Figure 15</a> and the Docker stats as shown in 
<a class="xref" href="#splunk-ui__splunk-ui-docker-stats">Figure 16</a>. 
</p>

  
<div class="fig fignone" id="splunk-ui__splunk-ui-docker">

<br /><img class="image" src="media/splunk-ui-docker.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 15. </b></span>Docker overview</span></div><p> </p></div>
  

<div class="fig fignone" id="splunk-ui__splunk-ui-docker-stats">

<br /><img class="image" src="media/splunk-ui-docker-stats.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 16. </b></span>Docker stats</span></div><p> </p></div>
  
  
  
<p class="p">Use the <code class="ph codeph">k8s App</code> to see the Kubernetes overview as shown in 
<a class="xref" href="#splunk-ui__splunk-ui-docker">Figure 15</a> and then access the details for deployments, daemon sets,
replica sets, services, etc.</p>
  

<div class="fig fignone" id="splunk-ui__splunk-k8s-overview">

<br /><img class="image" src="media/splunk-k8s-overview.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 17. </b></span>Kubernetes overview</span></div><p> </p></div>
  


</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title83" id="redeploying-splunk-demo">
<h2 class="title topictitle2" id="ariaid-title83">Redeploying Splunk demo</h2>

<div class="body">
<p class="p">The Splunk demo deployment, whilst fully featured, is severely restricted in the amount of data it can process. Once this limit has been
reached, often after running for just one or two days, it is necessary to re-deploy the application if you want to continue 
experimenting with the demo.</p>

  
<p class="p">Before you redeploy, it is necessary to remove the corresponing Docker stack and delete the associated volumes.</p>
  
  
<pre class="pre codeblock"><code># ssh hpe-ucp02

# docker stack rm splunk_demo
Removing service splunk_demo_splunkenterprise
Removing network splunk_demo_default

# docker volume ls | grep splunk
vsphere:latest      splunk_demo_vsplunk-opt-splunk-etc@Docker_GAB
vsphere:latest      splunk_demo_vsplunk-opt-splunk-var@Docker_GAB

# docker volume rm splunk_demo_vsplunk-opt-splunk-etc@Docker_GAB
splunk_demo_vsplunk-opt-splunk-etc@Docker_GAB

# docker volume rm splunk_demo_vsplunk-opt-splunk-var@Docker_GAB
splunk_demo_vsplunk-opt-splunk-var@Docker_GAB

</code></pre>  
 
<p class="p">Then re-run the playbook on your ansible node.</p>

  
<pre class="pre codeblock"><code>ansible-playbook -i vm_hosts playbooks/splunk_demo.yml --vault-password-file .vault_pass</code></pre>
 

</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title84" id="prometheus-grafana">
<h1 class="title topictitle1" id="ariaid-title84">Deploying Prometheus and Grafana on Docker RHEL nodes </h1>

<div class="body">
  

</div>

<div class="topic nested1" aria-labelledby="ariaid-title85" id="promgraf-overview">

<h2 class="title topictitle2" id="ariaid-title85">Monitoring with Prometheus and Grafana</h2>

<div class="body">
<p class="p">The solution can be configured to enable the use of Prometheus and Grafana for monitoring.
In this setup, there is no need for native installs and all the required monitoring software runs in containers,
deployed as either services or stacks. 
The load among the three hosts will be shared as 
per <a class="xref" href="#promgraf-overview__simplivity-ops-architecture-promgraf">Figure 18</a>.</p>


<div class="fig fignone" id="promgraf-overview__simplivity-ops-architecture-promgraf">
 
<img class="image" src="media/simplivity-ops-architecture-promgraf.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 18. </b></span>Solution architecture: Linux workers with Prometheus and Grafana</span></div><p> </p></div>



<p class="p">The Prometheus
and Grafana services are declared in a Docker stack as replicated services with one
replica each, so if they fail, Docker EE will ensure that they are restarted on one
of the UCP VMs. cAdvisor and node-exporter are declared in the same stack as global
services, so Docker EE will ensure that there is always one copy of each running on
every machine in the cluster.</p>

    
<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> 
Prometheus and Grafana functionality is not turned on by default in this solution - 
see the section on Configuration for more information on how to enable these tools.
Additionally, this functionality will not work for the Windows worker nodes in your environment at present.
</div><hr />
    
    
</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title86" id="prometheus-grafana-playbooks">
<h2 class="title topictitle2" id="ariaid-title86">Playbooks for installing Prometheus and Grafana</h2>

<div class="body">
<p class="p">The following playbooks are used to deploy Prometheus and Grafana on Docker RHEL nodes.</p>
  
<ul class="ul">

<li class="li"><code class="ph codeph">playbooks/install_logspout.yml</code> installs and configures <strong class="ph b">Logspout</strong> on all Docker nodes. 
Logspout is responsible for sending logs produced by containers running on the Docker nodes to the central logger VM.
By default, this playbook is commented out in <code class="ph codeph">site.yml</code>.
</li>



<li class="li"><code class="ph codeph">playbooks/config_monitoring.yml</code> configures a monitoring system for the Docker environment 
based on Grafana, Prometheus, cAdvisor and node-exporter Docker containers. By default, this playbook is commented out in <code class="ph codeph">site.yml</code>, so if you want to use the solution to automatically
deploy a Prometheus/Grafana monitoring system, you must explicitly uncomment both this and the <code class="ph codeph">playbooks/install_logspout.yml</code>
playbook in <code class="ph codeph">site.yml</code> or run these playbooks post-deployment.
</li>
  
  
</ul>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title87" id="monitoring-config-promgraf">
<h2 class="title topictitle2" id="ariaid-title87">Prometheus and Grafana configuration</h2>

<div class="body">
<p class="p">All Monitoring-related variables for Prometheus and Grafana are described in <a class="xref" href="#monitoring-config-promgraf__monitoring-config-table-conref">Table 19</a>. The variables determine
the versions of various software tools that are used and it is
recommended that the values given below are used.</p>
 



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="monitoring-config-promgraf__monitoring-config-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 19. </span>Monitoring variables</span></caption><colgroup><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e10123">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e10126">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10123 ">cadvisor_version</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10126 ">
<code class="ph codeph">v0.28.3</code>
</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10123 ">node_exporter_version</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10126 ">
<code class="ph codeph">v1.15.0</code>
</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10123 ">prometheus_version</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10126 ">
<code class="ph codeph">v2.3.2</code>
</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10123 ">grafana_version</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10126 ">
<code class="ph codeph">5.2.3</code>
</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10123 ">prom_persistent_vol_name</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10126 ">The name of the volume which will be used to store the monitoring
data. The volume is created using the vSphere Docker Volume
plugin.</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10123 ">prom_persistent_vol_size</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10126 ">The size of the volume which will hold the monitoring data. The
exact syntax is dictated by the vSphere Docker Volume plugin. The
default value is 10GB.</td>
</tr>
</tbody></table>
</div>



</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title88" id="using-promgraf-swarm">
<h2 class="title topictitle2" id="ariaid-title88">Accessing Grafana UI</h2>

<div class="body">
 
<p class="p">The Grafana UI is available at the UCP VIP, using <code class="ph codeph">HTTP</code> on port <code class="ph codeph">3000</code>, for example, </p>

<pre class="pre codeblock"><code>http://hpe-ucpvip.am2.cloudra.local:3000</code></pre>

<p class="p">The default username and password for Grafana is <code class="ph codeph">admin</code>/<code class="ph codeph">admin</code>.
The first time you login, you will be asked to reset the default <code class="ph codeph">admin</code> password.</p>
	
  
<p class="p">Select the <code class="ph codeph">Docker Swarm Monitor</code> dashboard that has already been loaded by the playbooks, 
as shown in <a class="xref" href="#using-promgraf-swarm__grafana-dashboard">Figure 19</a> and 
<a class="xref" href="#using-promgraf-swarm__grafana-dashboard2">Figure 20</a>. 
</p>

  
<div class="fig fignone" id="using-promgraf-swarm__grafana-dashboard">

<br /><img class="image" src="media/grafana-dashboard.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 19. </b></span>Docker Swarm Monitor</span></div><p> </p></div>
  


<div class="fig fignone" id="using-promgraf-swarm__grafana-dashboard2">

<br /><img class="image" src="media/grafana-dashboard2.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 20. </b></span>Docker Swarm Monitor</span></div><p> </p></div>
  
  
    
</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title89" id="post-deployment">
<h1 class="title topictitle1" id="ariaid-title89">
<span class="ph">Post deployment</span>
</h1>

<div class="body">
<p class="p">
The playbooks are intended to be used to deploy a new environment. You should only use them for Day 0 deployment purposes.
</p>

<p class="p">
The Ansible log is stored in the folder <code class="ph codeph">/root/<span class="ph"><span class="keyword">Docker-SimpliVity</span></span></code>. 
    If the deployment fails, you may find useful hints in this log.
</p>


<p class="p">To see how to check if your certs have been deployed correctly, see Appendix D.</p>

    

</div>




<div class="topic nested1" aria-labelledby="ariaid-title90" id="install-kubectl">
<h2 class="title topictitle2" id="ariaid-title90">Installing kubectl</h2>

<div class="body">

<p class="p">You can find the version number for the current stable version of <code class="ph codeph">kubectl</code> at 
  <a class="xref" href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a>.
  At the time of writing, the stable version is <code class="ph codeph">1.13</code>.</p>

  
  
<p class="p">However, this version of <code class="ph codeph">kubectl</code> is not compatible with UCP 3.0.*. Instead, you need to install
an earlier version, for example, <code class="ph codeph">v1.10.4</code>. To download this specific version, follow the instructions below.</p>
  
  
<pre class="pre codeblock"><code># version=v1.10.4
# wget -O kubectl https://storage.googleapis.com/kubernetes-release/release/${version}/bin/linux/amd64/kubectl
# chmod +x ./kubectl
# sudo mv ./kubectl /usr/local/bin/kubectl

# kubectl version
Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.4", GitCommit:"5ca598b4ba5abb89bb773071ce452e33fb66339d", GitTreeState:"clean", BuildDate:"2018-06-06T08:13:03Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"8+", GitVersion:"v1.8.11-docker-8d637ae", GitCommit:"8d637aedf46b9c21dde723e29c645b9f27106fa5", GitTreeState:"clean", BuildDate:"2018-04-26T16:51:21Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}
</code></pre>  
    
<p class="p">More details on installing <code class="ph codeph">kubectl</code> are available at 
<a class="xref" href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a>
</p>

  
</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title91" id="k8s-example-guestbook">
<h2 class="title topictitle2" id="ariaid-title91">Kubernetes guestbook example with Redis </h2>

<div class="body">

<p class="p">Many sample Kubernetes applications are available at <a class="xref" href="https://kubernetes.io/docs/tutorials/" target="_blank">https://kubernetes.io/docs/tutorials/</a>.
This section details how to deploy the stateless guestbook application with Redis as documented at
<a class="xref" href="https://kubernetes.io/docs/tutorials/stateless-application/guestbook/" target="_blank">https://kubernetes.io/docs/tutorials/stateless-application/guestbook/</a>. When deploying 
applications, you must be aware that Kubernetes version 1.8 shipped with Docker 2.0 and, as a result, you may have to make
changes in some places to the configuration files, particularly with the <code class="ph codeph">apiVersion</code> field.

</p>

  
<p class="p">The manifest file, included below, specifies a deployment controller that runs a single replica Redis master pod.</p>
  
<pre class="pre codeblock"><code># cat redis-master-deployment.yaml
apiVersion: apps/<strong class="ph b">v1beta2</strong>  # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: redis-master
  labels:
    app: redis
spec:
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: k8s.gcr.io/redis:e2e  # or just image: redis
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
</code></pre>

<p class="p">Apply the Redis master deployment from the <code class="ph codeph">redis-master-deployment.yaml</code> file:
</p>

  
<pre class="pre codeblock"><code># kubectl apply -f redis-master-deployment.yaml</code></pre>  

<p class="p">Query the list of Pods to verify that the Redis master pod is running.</p>

<pre class="pre codeblock"><code># kubectl get pods
NAME                            READY     STATUS    RESTARTS   AGE
<strong class="ph b">redis-master-5f8d48b75f-8ztt4</strong>   1/1       Running   0          33s
</code></pre>

<p class="p">Use the <code class="ph codeph">kubectl logs</code> command to view the logs from the Redis master pod:</p>

<pre class="pre codeblock"><code>[root@gab-ansible guestbook]#  kubectl logs -f  <strong class="ph b">redis-master-5f8d48b75f-8ztt4</strong>
                _._
           _.-``__ ''-._
      _.-``    `.  `_.  ''-._           Redis 2.8.19 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._
 (    '      ,       .-`  | `,    )     Running in stand alone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 1
  `-._    `-._  `-./  _.-'    _.-'
 |`-._`-._    `-.__.-'    _.-'_.-'|
 |    `-._`-._        _.-'_.-'    |           http://redis.io
  `-._    `-._`-.__.-'_.-'    _.-'
 |`-._`-._    `-.__.-'    _.-'_.-'|
 |    `-._`-._        _.-'_.-'    |
  `-._    `-._`-.__.-'_.-'    _.-'
      `-._    `-.__.-'    _.-'
          `-._        _.-'
              `-.__.-'

[1] 24 Aug 15:24:06.575 # Server started, Redis version 2.8.19
[1] 24 Aug 15:24:06.576 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
[1] 24 Aug 15:24:06.576 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
[1] 24 Aug 15:24:06.576 * The server is now ready to accept connections on port 6379
</code></pre>

<p class="p">The guestbook application needs to communicate with the Redis master to write its data. You need to apply a service to proxy the traffic to the Redis master pod. A service defines a policy to access the pods.</p>


<pre class="pre codeblock"><code># cat redis-master-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend
</code></pre>


<p class="p">Apply the Redis master service from the <code class="ph codeph">redis-master-service.yaml</code> file:</p>

  
<pre class="pre codeblock"><code>#  kubectl apply -f redis-master-service.yaml
service "redis-master" created
</code></pre>

<p class="p">Query the list of services to verify that the Redis master service is running.</p>

  
<pre class="pre codeblock"><code>#  kubectl get service
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes     ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    2d
redis-master   ClusterIP   10.96.174.129   &lt;none&gt;        6379/TCP   1m
</code></pre>
  
<p class="p">Although the Redis master is a single pod, you can make it highly available to meet traffic demands by adding replica Redis slaves.</p>



<pre class="pre codeblock"><code># cat redis-slave-deployment.yaml
apiVersion: apps/<strong class="ph b">v1beta2</strong> # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: redis-slave
  labels:
    app: redis
spec:
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google_samples/gb-redisslave:v1
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # Using `GET_HOSTS_FROM=dns` requires your cluster to
          # provide a dns service. As of Kubernetes 1.3, DNS is a built-in
          # service launched automatically. However, if the cluster you are using
          # does not have a built-in DNS service, you can instead
          # access an environment variable to find the master
          # service's host. To do so, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379
</code></pre>  

<p class="p">Create the Redis slaves from the <code class="ph codeph">redis-slave-deployment.yaml</code> file.</p>


<pre class="pre codeblock"><code># kubectl apply -f  redis-slave-deployment.yaml
deployment.apps "redis-slave" created
</code></pre>  
  
  
<p class="p">Query the list of Pods to verify that the Redis slave pods are running.</p>
  
  
<pre class="pre codeblock"><code># kubectl get pods
NAME                            READY     STATUS    RESTARTS   AGE
redis-master-5f8d48b75f-8ztt4   1/1       Running   0          39m
redis-slave-58fb7bb9b5-25z4m    1/1       Running   0          1m
redis-slave-58fb7bb9b5-q8zn9    1/1       Running   0          1m
</code></pre> 
  
<p class="p">The guestbook application needs to communicate to Redis slaves to read data. To make the Redis slaves discoverable, 
  you need to set up a service that provides transparent load balancing to the set of pods.</p>
 
  
<pre class="pre codeblock"><code># cat redis-slave-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend
</code></pre>


<p class="p">Deploy the Redis slave service from the <code class="ph codeph">redis-slave-service.yaml</code> file</p>

<pre class="pre codeblock"><code># kubectl apply -f redis-slave-service.yaml
service "redis-slave" created
</code></pre>  
  
<p class="p">Query the list of services to verify that the Redis slave service is running.</p>
 
  
  
<pre class="pre codeblock"><code># kubectl get services
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes     ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    2d
redis-master   ClusterIP   10.96.174.129   &lt;none&gt;        6379/TCP   30m
redis-slave    ClusterIP   10.96.82.175    &lt;none&gt;        6379/TCP   3m
</code></pre>  


<p class="p">The guestbook application has a web frontend  written in PHP serving the HTTP requests. 
  It is configured to connect to the <code class="ph codeph">redis-master</code> service for write requests 
  and the <code class="ph codeph">redis-slave</code> service for read requests.</p>



<pre class="pre codeblock"><code># cat frontend-deployment.yaml
apiVersion: <strong class="ph b">apps/v1beta2</strong> # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: frontend
  labels:
    app: guestbook
spec:
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v4
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # Using `GET_HOSTS_FROM=dns` requires your cluster to
          # provide a dns service. As of Kubernetes 1.3, DNS is a built-in
          # service launched automatically. However, if the cluster you are using
          # does not have a built-in DNS service, you can instead
          # access an environment variable to find the master
          # service's host. To do so, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 80
</code></pre>

<p class="p">Create the frontend deployment using the  <code class="ph codeph">frontend-deployment.yaml</code> file.</p>

  
<pre class="pre codeblock"><code># kubectl apply -f frontend-deployment.yaml
deployment.apps "frontend" created
</code></pre>  

<p class="p">Query the list of pods to verify that the three frontend replicas are running.</p>

  
<pre class="pre codeblock"><code># kubectl get pods -l app=guestbook -l tier=frontend
NAME                        READY     STATUS    RESTARTS   AGE
frontend-574b66f649-2qh6h   1/1       Running   0          7m
frontend-574b66f649-782mr   1/1       Running   0          7m
frontend-574b66f649-mnbc9   1/1       Running   0          7m
</code></pre>

<p class="p">If you want guests to be able to access your guestbook, you must configure the frontend service
  to be externally visible, so a client can request the service from outside the container cluster.</p>

  
<pre class="pre codeblock"><code># cat frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # comment or delete the following line if you want to use a LoadBalancer
  <strong class="ph b">type: NodePort</strong>
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend
</code></pre>

<p class="p">Deploy the <code class="ph codeph">frontend</code> service using the <code class="ph codeph">frontend-service.yaml</code> file</p>


<pre class="pre codeblock"><code># kubectl apply -f frontend-service.yaml
service "frontend" created
</code></pre>

<p class="p">Query the list of services to verify that the <code class="ph codeph">frontend</code> service is running.</p>


<pre class="pre codeblock"><code># kubectl get services
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP      PORT(S)        AGE
frontend       NodePort    10.96.14.76     &lt;none&gt;        80:<strong class="ph b">35440</strong>/TCP   15s
kubernetes     ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        2d
redis-master   ClusterIP   10.96.174.129   &lt;none&gt;        6379/TCP       54m
redis-slave    ClusterIP   10.96.82.175    &lt;none&gt;        6379/TCP       27m
</code></pre>


<p class="p">Access the UI, using the identified port, at <code class="ph codeph">http://gab-ucp03.cloudra.local:<strong class="ph b">35440</strong>/ </code>
    as shown in <a class="xref" href="#k8s-example-guestbook__k8s-guestbook1">Figure 21</a>. 
</p>

 
<div class="fig fignone" id="k8s-example-guestbook__k8s-guestbook1">

<br /><img class="image" src="media/k8s-guestbook1.png" /><br />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 21. </b></span>Guestbook UI</span></div><p> </p></div>
   
  

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title92" id="k8s-nfs-provisioner">
<h2 class="title topictitle2" id="ariaid-title92">Deploying the NFS provisioner for Kubernetes</h2>

<div class="body">
<div class="section"><h3 class="title sectiontitle">Prerequisites</h3>

  
<ul class="ul">
<li class="li">Configure the variables described in the section <a class="xref" href="#k8s-persistent-volume-config">Kubernetes Persistent Volume configuration</a></li>

<li class="li">Install the <code class="ph codeph">kubectl</code> binary on your Ansible box as documented at
  <a class="xref" href="https://docs.docker.com/ee/ucp/user-access/kubectl/" target="_blank">https://docs.docker.com/ee/ucp/user-access/kubectl/</a></li>
  
<li class="li">Install the UCP Client bundle for the <code class="ph codeph">admin</code> user as described at  <a class="xref" href="https://docs.docker.com/ee/ucp/user-access/cli/#download-client-certificates-by-using-the-rest-api" target="_blank">https://docs.docker.com/ee/ucp/user-access/cli/#download-client-certificates-by-using-the-rest-api</a></li>
  
<li class="li">Confirm that you can connect to the cluster by running a test command, for example, <code class="ph codeph">kubectl get nodes</code></li>
  
</ul>
  
  
</div>


<div class="section"><h3 class="title sectiontitle">Running the playbook</h3>

  
<p class="p">Once the prerequisites are satisfied, run the appropriate playbook on your Ansible node:</p>
  
  
<pre class="pre codeblock"><code># cd <span class="ph"><span class="keyword">Docker-SimpliVity</span></span>
# ansible-playbook -i vm_hosts playbooks/k8s-nfs-provisioner.yml --vault-password-file .vault_pass
</code></pre>  
  
</div>

 

  
</div>

<div class="topic nested2" aria-labelledby="ariaid-title93" id="k8s-nfs-provisioner-vm">
<h3 class="title topictitle3" id="ariaid-title93">Using NFS VM for post-deployment verification</h3>

<div class="body">
<div class="section">

 
<p class="p">In this example, it is assumed that the relevant variables are configured as follows:</p>
 
  

<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="k8s-nfs-provisioner-vm__k8s-persistent-volume-verify" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 20. </span>Configuration values</span></caption><colgroup><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e10667">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e10670">Value</th>
</tr>
</thead><tbody class="tbody">

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10667 ">nfs_provisioner_role</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10670 "><code class="ph codeph">nfs-provisioner-runner</code></td>
</tr>     

 
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10667 ">nfs_provisioner_name</td> 
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10670 "><code class="ph codeph">hpe.com/nfs</code></td>
</tr>
 
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10667 ">nfs_provisioner_storage_class_name</td> 
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10670 "><code class="ph codeph"><strong class="ph b">nfs</strong></code></td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10667 ">nfs_provisioner_server_ip</td> 
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10670 "><code class="ph codeph">hpe-nfs.cloudra.local</code></td>
</tr>

<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10667 ">nfs_provisioner_server_share</td> 
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10670 "><code class="ph codeph">/k8s</code></td>
</tr>

  
</tbody></table>
</div>
  
  
<p class="p">In this instance, the server IP is set to the NFS VM that has been deployed.</p>


<p class="p">Ensure that your environment satisfies the prerequisites as described in <a class="xref" href="#k8s-nfs-provisioner">Deploying the NFS provisioner for Kubernetes</a> and then run the playbook:</p>
  

<pre class="pre codeblock"><code># cd <span class="ph"><span class="keyword">Docker-SimpliVity</span></span>
# ansible-playbook -i vm_hosts playbooks/k8s-nfs-provisioner.yml --vault-password-file .vault_pass
</code></pre> 

<p class="p">Running the command <code class="ph codeph">kubectl get sc</code> will show the storage class named <code class="ph codeph">nfs</code>:</p>


<pre class="pre codeblock"><code>
# kubectl get sc
NAME      PROVISIONER   AGE
nfs       hpe.com/nfs   5m
</code></pre>


<p class="p">Create a temporary file <code class="ph codeph">/tmp/pvc.yml</code> for a persistent volume claim (PVC) named <code class="ph codeph">dynnfs-testpvc</code>
with a storage class of <code class="ph codeph">nfs</code>
</p>

<pre class="pre codeblock"><code># cat /tmp/pvc.yml &lt;&lt;EOF
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: <strong class="ph b">dynnfs-testpvc</strong>
  annotations:
    volume.beta.kubernetes.io/storage-class: "<strong class="ph b">nfs</strong>"
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Mi
EOF  
</code></pre>

  
<p class="p">Create the PVC resource by running <code class="ph codeph">kubectl apply</code> on this file.</p>


<pre class="pre codeblock"><code># kubectl apply -f /tmp/pvc.yml
persistentvolumeclaim "dynnfs-testpvc" created</code></pre>
  
<p class="p">Verify that the corresponding persistent volume (PV) was created at the same time.</p>
  
  
 <pre class="pre codeblock"><code># kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
pvc-e685a9d2-8a6f-11e8-9025-0242ac110010   100Mi      RWX            Delete           Bound     default/dynnfs-testpvc   nfs                      4s
</code></pre> 
  

<p class="p">Define a pod that will mount the persistent volume through using the persistent volume claim. The persistent volume
  is mounted under <code class="ph codeph">/tmp/foo</code>.</p>

  
<pre class="pre codeblock"><code>cat /tmp/pod.yml &lt;&lt;EOF
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: <strong class="ph b">dynnfs-testpod</strong>
spec:
  selector:
    matchLabels:
      app: dynnfs-testpod
  replicas: 1
  template:
    metadata:
      labels:
        app: dynnfs-testpod
    spec:
      volumes:
      - name: pod-data
        persistentVolumeClaim:
          claimName: <strong class="ph b">dynnfs-testpvc</strong>
      containers:
      - name: dynnfs-testpod
        command:
        - sh
        - -c
        - while true; do sleep 1; done
        image: radial/busyboxplus:curl
        volumeMounts:
        - mountPath: <strong class="ph b">/tmp/foo</strong>
          name: pod-data
EOF</code></pre>  
  
<p class="p">Create the pod resource by running <code class="ph codeph">kubectl apply</code> on the file.</p>
  
  
  
<pre class="pre codeblock"><code># kubectl apply -f /tmp/pod.yml
deployment.apps "dynnfs-testpod" created
</code></pre>  

<p class="p">Retrieve the pod ID  and then execute a command in the pod to create a test file on the persistent volume. The file is named <code class="ph codeph">/tmp/foo/bar.txt</code> 
 and contains the string <code class="ph codeph">hello</code>.</p>


<pre class="pre codeblock"><code># pod=$(kubectl get pod | awk '/dynnfs-testpod-/ {print $1}')
# kubectl exec -it $pod -- sh -c "<strong class="ph b">echo hello &gt;/tmp/foo/bar.txt</strong>"</code></pre>
  
  
<p class="p">In this example, where the NFS VM is being used as the storage back-end, you can examine the content of the folder containing the 
persistent volumes. Given the values specified above, where the NFS VM is named <code class="ph codeph">hpe-nfs</code> and 
  the <code class="ph codeph">nfs_provisioner_server_share</code> is <code class="ph codeph">k8s</code>, you can connect to the VM and explore the folder as follows. </p>
  

<pre class="pre codeblock"><code>
# ssh hpe-nfs ls -R /k8s
/k8s:
default-dynnfs-testpvc-pvc-e685a9d2-8a6f-11e8-9025-0242ac110010
 
/k8s/default-dynnfs-testpvc-pvc-e685a9d2-8a6f-11e8-9025-0242ac110010:
<strong class="ph b">bar.txt</strong></code></pre>
  
<p class="p">Examine the contents of the file to ensure that the string <code class="ph codeph">hello</code> has been persisted in the file <code class="ph codeph">bar.txt</code>.</p>
  


<pre class="pre codeblock"><code>
# ssh hpe-nfs cat /k8s/default-dynnfs-testpvc-pvc-e685a9d2-8a6f-11e8-9025-0242ac110010/bar.txt
<strong class="ph b">hello</strong></code></pre>
  
  
  
  
</div>

  </div>

</div>
</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title94" id="backup-restore">
<h1 class="title topictitle1" id="ariaid-title94">Backup and restore</h1>

<div class="body">
<p class="p">This reference configuration provides playbooks and scripts to help you back up and restore:</p>

<ul class="ul">
<li class="li">Docker UCP and DTR</li>

<li class="li">Docker volumes</li>

</ul>

</div>

<div class="topic nested1" aria-labelledby="ariaid-title95" id="backup-restore-ucp-dtr">
<h2 class="title topictitle2" id="ariaid-title95">Backup and restore UCP and DTR</h2>

<div class="body">


<p class="p">The playbooks provided in this solution implement the backup and restore procedures as they
are described in the Docker documentation at <a class="xref" href="https://docs.docker.com/enterprise/backup/" target="_blank">https://docs.docker.com/enterprise/backup/</a>.

The solution follows the recommendations in the Docker best practices document at 
<a class="xref" href="https://success.docker.com/article/backup-restore-best-practices" target="_blank">https://success.docker.com/article/backup-restore-best-practices</a>.
</p>


  
<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> It is important that you make copies of the backed up data and that you store the copies in a separate physical location. 
You must also recognize that the backed up data contains sensitive information such as private keys and so it is important to restrict
access to the generated files. However, the playbooks do not backup the sensitive information in your <code class="ph codeph">group_vars/vault</code> file so you should make 
sure to keep track of the credentials for the UCP Administrator. </div><hr />
  
  
<hr /><div class="note warning"><span class="warningtitle"><b>Warning:</b></span> The restore procedures do not restore swarm data. You should follow
infrastructure as code (IaC) guidelines and maintain your service, stack and network definitions using source code or configuration
management tools.

You must also ensure that you safely manage the credentials of your administration accounts as 
existing UCP Client bundles will not work when you restore UCP on a new swarm. 
</div><hr />
  

</div>




   
    
<div class="topic nested2" aria-labelledby="ariaid-title96" id="backup-ucp-dtr">
<h3 class="title topictitle3" id="ariaid-title96">Backup UCP and DTR</h3>


<div class="body">   
<p class="p">The playbooks support backing up the swarm, UCP, DTR metadata and DTR images.</p>

</div>

<div class="topic nested3" aria-labelledby="ariaid-title97" id="backup-variables">
<h4 class="title topictitle4" id="ariaid-title97">Backup configuration variables</h4>

<div class="body">


<p class="p"><a class="xref" href="#backup-variables__backup-ucp-table-content">Table 21</a>
shows the variables related to backing up UCP and DTR. All these variables are defined in
the file <strong class="ph b">group_vars/backups</strong>. All the data that is backed up is streamed over an SSH
connection to the backup server. Currently, the playbooks only support the use of the
Ansible box as the backup server.</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="backup-variables__backup-ucp-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 21. </span>Backup variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e10983">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e10986">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e10989">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10983 ">backup_server</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10986 "><strong class="ph b">group_vars/backups</strong></td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10989 ">Currently, the playbooks only support the use of the Ansible box as the backup
server. </td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10983 ">backup_dest</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10986 "><strong class="ph b">group_vars/backups</strong></td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10989 ">This variable should point to an existing folder on your Ansible box where the
<code class="ph codeph">root</code> user has write access. All the backups will be stored in this folder. For
example, <code class="ph codeph">/root/backup</code></td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10983 ">#swarm_offline_backups</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10986 "><strong class="ph b">group_vars/backup</strong></td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e10989 ">This variable is commented out by default. More information on this variable is provided below. </td>
</tr>    
</tbody></table>
</div>


</div>

</div>
<div class="topic nested3" aria-labelledby="ariaid-title98" id="backup-swarm">
<h4 class="title topictitle4" id="ariaid-title98">Backing up the Swarm</h4>

<div class="body">

<p class="p">When you backup the swarm, your services and stack definitions are backed up together with the
networks definitions. However, Docker volumes or their contents will not be backed up. (If
Docker volumes are defined in stacks, they will be re-created when you restore the stacks,
but their content will be lost). You can backup the swarm using the playbook named
<code class="ph codeph">backup_swarm.yml</code> which is located in the <code class="ph codeph">playbooks</code>
folder on your Ansible server. The playbook is invoked as follows: </p>


<pre class="pre codeblock"><code># ansible-playbook -i vm_hosts playbooks/backup_swarm.yml</code></pre>
    
<p class="p">This playbook creates two archives in the folder specified by the variable
<code class="ph codeph">backup_dest</code> in <code class="ph codeph">group_vars/backups</code>. By default, the file
is named using the following pattern:</p>


<pre class="pre codeblock"><code>&lt;backup_dest&gt;/backup_swarm_&lt;vmname&gt;_&lt;timestamp&gt;.tgz
&lt;backup_dest&gt;/backup_swarm_&lt;vmname&gt;_&lt;timestamp&gt;.vars.tgz</code></pre>

<p class="p"><code class="ph codeph">&lt;vmname&gt;</code> is the name of the host (in the inventory) that was used
to take the backup, and <code class="ph codeph">&lt;timestamp&gt;</code> is the time at which the backup was
taken. The file with the extension <code class="ph codeph">.vars.tgz</code> contains information 
regarding the system that was backed up.</p>


<p class="p">You can override the generated file name by defining the variable <strong class="ph b">backup_name</strong> on the
command line when running the playbook. In the example below: </p>

    
<pre class="pre codeblock"><code># ansible-playbook -i vm_hosts playbooks/backup_swarm.yml -e backup_name=<strong class="ph b">my_swarm_backup</strong></code></pre>
    
<p class="p">The generated files won't have <code class="ph codeph">&lt;vmname&gt;</code> or
<code class="ph codeph">&lt;timestamp&gt;</code> appended:</p>


<pre class="pre codeblock"><code>&lt;backup_dest&gt;/my_swarm_backup.tgz
&lt;backup_dest&gt;/my_swarm_backup.vars.tgz</code></pre>

<hr /><div class="note warning"><span class="warningtitle"><b>Warning:</b></span> <strong class="ph b">Online versus offline backups:</strong> By default, the playbook performs online backups.
You can take offline backups by setting the variable <code class="ph codeph">swarm_backup_offline</code> to <code class="ph codeph">"true"</code>.
The playbook will then stop the Docker daemon on the machine used to take the backup (a manager or UCP node).
Before it does so, the playbook will verify that enough managers are running in the cluster to
maintain the quorum. If this is not the case, the playbook will exit with an error. For more information, see the Docker
documentation at <a class="xref" href="https://docs.docker.com/engine/swarm/admin_guide/#recover-from-disasterv" target="_blank">https://docs.docker.com/engine/swarm/admin_guide/#recover-from-disasterv</a></div><hr />


</div>

</div>
<div class="topic nested3" aria-labelledby="ariaid-title99" id="backup-ucp">
<h4 class="title topictitle4" id="ariaid-title99">Backing up the Universal Control Plane (UCP)</h4>

<div class="body">

<p class="p">When you backup UCP, you save the data/metadata outlined in <a class="xref" href="#backup-ucp__backup-ucp-data-meta-table-content">Table 22</a>: </p>


<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="backup-ucp__backup-ucp-data-meta-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 22. </span>UCP data backed up</span></caption><colgroup><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e11163">Data</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e11166">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11163 ">Configurations</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11166 ">The UCP cluster configurations, as shown by <code class="ph codeph">docker config ls</code>,
including Docker EE license and swarm and client CAs </td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11163 ">Access control</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11166 ">Permissions for team access to swarm resources, including collections, grants, and
roles</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11163 ">Certificates and keys</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11166 ">The certificates, public keys, and private keys that are used for
authentication and mutual TLS communication</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11163 ">Metrics data</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11166 ">Monitoring data gathered by UCP</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11163 ">Organizations</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11166 ">Your users, teams, and orgs</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11163 ">Volumes</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11166 ">All <a class="xref" href="https://docs.docker.com/datacenter/ucp/2.2/guides/architecture/#volumes-used-by-ucp" target="_blank">UCP named volumes</a>, which include all UCP component certs and
data</td>
</tr>
</tbody></table>
</div>


<p class="p">To make a backup of UCP, use <code class="ph codeph">playbook/backup_ucp.yml</code> as follows:</p>

    
<pre class="pre codeblock"><code># ansible-playbook -i vm_host playbooks/backup_ucp.yml</code></pre>

<p class="p">This playbook creates two archives in the folder specified by the variable
<code class="ph codeph">backup_dest</code> in <code class="ph codeph">group_vars/backups</code>. By default, the files
are named using the following pattern:</p>


<pre class="pre codeblock"><code>&lt;backup_dest&gt;/backup_ucp_&lt;ucpid&gt;_&lt;vmname&gt;_&lt;timestamp&gt;.tgz
&lt;backup_dest&gt;/backup_ucp_&lt;ucpid&gt;_&lt;vmname&gt;_&lt;timestamp&gt;.vars.tgz</code></pre>

<p class="p"><code class="ph codeph">&lt;ucpid&gt;</code> is the ID of the UCP instance,
<code class="ph codeph">&lt;vmname&gt;</code> is the name of the host (in the inventory) that was used to
take the backup, and <code class="ph codeph">&lt;timestamp&gt;</code> is the time at which the backup was
taken. The file with the extension <code class="ph codeph">.vars.tgz</code> contains information regarding the system which was backed up.</p>


<p class="p">You can override the generated file name by defining the variable <strong class="ph b">backup_name</strong> on the
command line when running the playbook. In the example below: </p>

    
<pre class="pre codeblock"><code># ansible-playbook -i vm_hosts playbooks/backup_ucp.yml -e backup_name=<strong class="ph b">my_ucp_backup</strong></code></pre>
    
<p class="p">The generated files won't have <code class="ph codeph">&lt;vmname&gt;</code> or
<code class="ph codeph">&lt;timestamp&gt;</code> appended:</p>


<pre class="pre codeblock"><code>&lt;backup_dest&gt;/my_ucp_backup.tgz
&lt;backup_dest&gt;/my_ucp_backup.vars.tgz</code></pre>

<hr /><div class="note warning"><span class="warningtitle"><b>Warning:</b></span> To create a consistent backup, the backup command <strong class="ph b">temporarily stops the
UCP containers running on the node where the backup is being performed</strong>. User resources, such
as services, containers, and stacks are not affected by this operation and will continue
to operate as expected. Any long-lasting <code class="ph codeph">docker exec</code>, <code class="ph codeph">docker logs</code>, <code class="ph codeph">docker events</code>, 
or <code class="ph codeph">docker attach</code> operations on the
affected manager node will be disconnected.</div><hr />


<p class="p">For more information on UCP backup, see the Docker documentation at
<a class="xref" href="https://docs.docker.com/datacenter/ucp/3.0/guides/admin/backups-and-disaster-recovery/" target="_blank">https://docs.docker.com/datacenter/ucp/3.0/guides/admin/backups-and-disaster-recovery/</a></p>
  


</div>

</div>
<div class="topic nested3" aria-labelledby="ariaid-title100" id="backup-dtr-meta">
<h4 class="title topictitle4" id="ariaid-title100">Backing up the Docker Trusted Registry (DTR)</h4>

<div class="body">

<p class="p">When you backup DTR, you save the data/metadata outlined in 
   <a class="xref" href="#backup-dtr-meta__backup-dtr-data-meta-table-content">Table 23</a>:</p>

    

<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="backup-dtr-meta__backup-dtr-data-meta-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 23. </span>DTR data backed up</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e11358">Data</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e11361">Backed up?</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e11364">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11358 ">Configurations</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11361 ">yes</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11364 ">DTR settings</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11358 ">Repository metadata</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11361 ">yes</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11364 ">Metadata like image architecture and size</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11358 ">Access control to repos and images</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11361 ">yes</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11364 ">Data about who has access to which images</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11358 ">Notary data</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11361 ">yes</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11364 ">Signatures and digests for images that are signed</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11358 ">Scan results</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11361 ">yes</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11364 ">Information about vulnerabilities in your images</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11358 ">Certificates and keys</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11361 ">yes</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11364 ">TLS certificates and keys used by DTR</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11358 ">Image content</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11361 ">no</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11364 ">Needs to be backed up separately, depends on DTR configuration</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11358 ">Users, orgs, teams</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11361 ">no</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11364 ">Create a UCP backup to backup this data</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11358 ">Vulnerability database</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11361 ">no</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11364 ">Can be re-downloaded after a restore</td>
</tr>  
</tbody></table>
</div>



<p class="p">To make a backup of DTR metadata, use <code class="ph codeph">playbook/backup_dtr_metadata.yml</code></p>

    
<pre class="pre codeblock"><code># ansible-playbook -i vm_host playbooks/backup_dtr_metadata.yml</code></pre>
    
<p class="p">This playbook creates two archives in the folder specified by the variable
<code class="ph codeph">backup_dest</code> in <code class="ph codeph">group_vars/backups</code>. By default, the file
is named using the following pattern:</p>


<pre class="pre codeblock"><code>&lt;backup_dest&gt;/backup_dtr_meta_&lt;replica_id&gt;_&lt;vmname&gt;_&lt;timestamp&gt;.tgz
&lt;backup_dest&gt;/backup_dtr_meta_&lt;replica_id&gt;_&lt;vmname&gt;_&lt;timestamp&gt;.vars.tgz</code></pre>

<p class="p"><code class="ph codeph">&lt;replica_id&gt;</code> is the ID of the DTR replica that was backed up,
<code class="ph codeph">&lt;vmname&gt;</code> is the name of the host (in the inventory) that was used to
take the backup, and <code class="ph codeph">&lt;timestamp&gt;</code> is the time at which the backup was
taken. The file with the extension <code class="ph codeph">.vars.tgz</code> contains information regarding the system that was backed up.</p>


<p class="p">You can override the generated file name by defining the variable <strong class="ph b">backup_name</strong> on the
command line when running the playbook. In the example below: </p>

    
<pre class="pre codeblock"><code># ansible-playbook -i vm_hosts playbooks/backup_dtr_metadata.yml -e backup_name=<strong class="ph b">my_dtr_metadata_backup</strong></code></pre>
    
<p class="p">The generated files won't have <code class="ph codeph">&lt;vmname&gt;</code> or
<code class="ph codeph">&lt;timestamp&gt;</code> appended:</p>


<pre class="pre codeblock"><code>&lt;backup_dest&gt;/my_dtr_metadata_backup.tgz
&lt;backup_dest&gt;/my_dtr_metadata_backup.vars.tgz</code></pre>

<p class="p">For more information on DTR backups, see the Docker documentation at 
<a class="xref" href="https://docs.docker.com/datacenter/dtr/2.5/guides/admin/backups-and-disaster-recovery/">https://docs.docker.com/datacenter/dtr/2.5/guides/admin/backups-and-disaster-recovery/</a></p>



</div>

</div>
<div class="topic nested3" aria-labelledby="ariaid-title101" id="backup-dtr-data">
<h4 class="title topictitle4" id="ariaid-title101">Backing up DTR data (images)</h4>

<div class="body">


<p class="p">To make a backup of the images that are inventoried in DTR and stored on the NFS server, use <code class="ph codeph">playbooks/backup_dtr_images.yml</code> </p>
  

<pre class="pre codeblock"><code># ansible-playbook -i vm_host playbooks/backup_dtr_images.yml</code></pre>  

<p class="p">This playbook creates two archives in the folder specified by the variable
<code class="ph codeph">backup_dest</code> in <code class="ph codeph">group_vars/backups</code>. By default, the files
are named using the following pattern:</p>


<pre class="pre codeblock"><code>&lt;backup_dest&gt;/backup_dtr_data_&lt;replica_id&gt;_&lt;vmname&gt;_&lt;timestamp&gt;.tgz
&lt;backup_dest&gt;/backup_dtr_data_&lt;replica_id&gt;_&lt;vmname&gt;_&lt;timestamp&gt;.vars.tgz</code></pre>

<p class="p"><code class="ph codeph">&lt;replica_id&gt;</code> is the ID of the DTR replica that was backed up,
<code class="ph codeph">&lt;vmname&gt;</code> is the name of the host (in the inventory) that was used to
take the backup, and <code class="ph codeph">&lt;timestamp&gt;</code> is the time at which the backup was
taken.</p>
  

<p class="p">You can override the generated file names by defining the variable <strong class="ph b">backup_name</strong> on the
command line when running the playbook, as shown in the example below: </p>

    
<pre class="pre codeblock"><code># ansible-playbook -i vm_hosts playbooks/backup_dtr_images.yml -e backup_name=<strong class="ph b">my_dtr_data_backup</strong></code></pre>
    
<p class="p">The generated files won't have <code class="ph codeph">&lt;vmname&gt;</code> or
<code class="ph codeph">&lt;timestamp&gt;</code> appended:</p>


<pre class="pre codeblock"><code>&lt;backup_dest&gt;/my_dtr_data_backup.tgz
&lt;backup_dest&gt;/my_dtr_data_backup.vars.tgz</code></pre>  

<p class="p">For more information on DTR backups, see the Docker documentation at 
<a class="xref" href="https://docs.docker.com/datacenter/dtr/2.5/guides/admin/backups-and-disaster-recovery/" target="_blank">https://docs.docker.com/datacenter/dtr/2.5/guides/admin/backups-and-disaster-recovery/</a></p>




</div>

</div>
<div class="topic nested3" aria-labelledby="ariaid-title102" id="backup-other">
<h4 class="title topictitle4" id="ariaid-title102">Backing up other metadata, including passwords</h4>

<div class="body">


<p class="p">The backup playbooks do not backup the sensitive data in your <code class="ph codeph">group_vars/vault</code> file. 
The information stored in the <code class="ph codeph">.vars.tgz</code> files includes backups of the following files:</p>


<ul class="ul">
<li class="li"><strong class="ph b">vm_hosts</strong>, a copy of the <code class="ph codeph">vm_hosts</code> file at the time the backup was taken</li>

<li class="li"><strong class="ph b">vars</strong>, a copy of your <code class="ph codeph">group_vars/vars</code> file at the time the backup was taken</li>

<li class="li"><strong class="ph b">meta.yml</strong>, a generated file containing information pertaining to the backup</li>
    
</ul>
    

<p class="p">The <strong class="ph b">meta.yml</strong> file contains the following information:</p>


<pre class="pre codeblock"><code>backup_node="&lt;node that took the backup&gt;"
replica_id="&lt;ID of DTR replica if DTR backup&gt;"
backup_source=""
ucp_version="&lt;UCP version if UCP backup&gt;"
dtr_version="&lt;DTR version of DTR backup&gt;"</code></pre>    



</div>

</div>
<div class="topic nested3" aria-labelledby="ariaid-title103" id="backup-utility">
<h4 class="title topictitle4" id="ariaid-title103">Backup Utility</h4>
 
<div class="body">

<p class="p">The script <code class="ph codeph">backup.sh</code> can be used to take a backup of the swarm, UCP, DTR metadata and the DTR images in one go. 
You can pass this script an argument (tag) that will be used to prefix the backup filenames, thereby overriding the default naming. 
<a class="xref" href="#backup-utility__backup-utility-table-content">Table 24</a> 
shows the file names produced by <code class="ph codeph">backup.sh</code> based on the argument passed in the command line.    
</p>



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="backup-utility__backup-utility-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 24. </span>Backup utility</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e11719">Example</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e11722">Command line</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e11725">Generated filenames</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11719 ">Default</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11722 "><code class="ph codeph">./backup.sh</code></td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11725 ">backup_swarm_&lt;vmname&gt;_&lt;timestamp&gt;.tgz, 
backup_ucp_&lt;ucpid&gt;_&lt;vmname&gt;_&lt;timestamp&gt;.tgz, 
backup_dtr_meta_&lt;replica_id&gt;_&lt;vmname&gt;_&lt;timestamp&gt;.tgz,
backup_dtr_data_&lt;replica_id&gt;_&lt;vmname&gt;_&lt;timestamp&gt;.tgz
and the corresponding <code class="ph codeph">.vars.tgz</code> files</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11719 ">Custom</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11722 "><code class="ph codeph">./backup.sh my_backup</code></td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11725 ">my_backup_swarm.tgz, my_backup_ucp.tgz, my_backup_dtr_meta.tgz, my_backup_dtr_data.tgz, and the corresponding <code class="ph codeph">.vars.tgz</code> files</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11719 ">Date</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11722 "><code class="ph codeph">./backup.sh $(date '+%Y_%m_%d_%H%M%S')</code></td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e11725 ">&lt;date&gt;_swarm.tgz, &lt;date&gt;_ucp.tgz, &lt;date&gt;_dtr_meta.tgz,  &lt;date&gt;_dtr_data.tgz, and the corresponding <code class="ph codeph">.vars.tgz</code> files</td>
</tr>    
</tbody></table>
</div>



<p class="p">In addition, the <code class="ph codeph">backup.sh</code> script accepts an optional switch that will let you specify the location of the password file that will 
    be passed to the <code class="ph codeph">ansible-playbook</code> commands in the script. This is required if you have encrypted the  <code class="ph codeph">group_vars/vault</code> file. 
    The general syntax for this script is as follows:
</p>


<pre class="pre codeblock"><code>./backup.sh [ -v &lt;Vault Password File&gt; ] [ tag ]</code></pre>

</div>

</div>
<div class="topic nested3" aria-labelledby="ariaid-title104" id="backup-playbooks">
<h4 class="title topictitle4" id="ariaid-title104">Related playbooks</h4>
  
<div class="body">


<ul class="ul">
<li class="li"><code class="ph codeph">playbooks/backup_swarm.yml</code> is used to back up the swarm data</li>

<li class="li"><code class="ph codeph">playbooks/backup_ucp.yml</code> is used to back up UCP</li>

<li class="li"><code class="ph codeph">playbooks/backup_dtr_meta.yml</code> is used to back up DTR metadata</li>

<li class="li"><code class="ph codeph">playbooks/backup_dtr_images.yml</code> is used to back up DTR images</li>

</ul>



</div>

</div>
</div>
<div class="topic nested2" aria-labelledby="ariaid-title105" id="restore-ucp-dtr">
<h3 class="title topictitle3" id="ariaid-title105">Restoring your cluster after a disaster</h3>
 
    
<div class="body">
<p class="p">The playbooks address a disaster recovery scenario where you have lost your entire cluster and all the VMs. 
Other scenarios and how to handle them are described in the Docker documentation including the following scenarios:</p>


<ul class="ul">
<li class="li">You have lost one UCP instance but your cluster still has the quorum. The easiest way is to recreate the missing UCP instance from scratch.</li>
 
<li class="li">You have lost the quorum in your UCP cluster but there is still one UCP instance running.</li>
 
<li class="li">You have lost one instance of DTR but still have a quorum of replicas. The easiest way is to recreate the missing DTR instance from scratch.</li>

<li class="li">You have lost the quorum of your DTR cluster but still have one DTR instance running.</li>
    
</ul>
     
</div>
   
<div class="topic nested3" aria-labelledby="ariaid-title106" id="before-restore">
<h4 class="title topictitle4" id="ariaid-title106">Before you restore</h4>
 
<div class="body">

<p class="p"><strong class="ph b">Step 1.</strong> Retrieve the backup files using your chosen backup solution and save them to a 
folder on your Ansible server. If you have used timestamps in the naming of your backup files, you can 
use them to determine the chronological order. If you used the <code class="ph codeph">backup.sh</code> script specifying a date prefix,
you can use that to identify the matching set of backup files. You should 
choose the files in the following reverse chronological order, from the most recent to the oldest file. 
Make sure you restore both the <code class="ph codeph">*.tgz</code> and the <code class="ph codeph">*.vars.tgz</code> files.</p>
 

<ol class="ol">
<li class="li">DTR images backup</li>

<li class="li">DTR metadata backup</li>

<li class="li">UCP backup</li>

<li class="li">Swarm backup</li>

</ol>


<p class="p">In this example, we will assume a set of backup files stored in <code class="ph codeph">/root/restore</code> that were created specifying a date prefix. These will have names like
<code class="ph codeph">2018_04_17_151734_swarm.tgz</code>, <code class="ph codeph">2018_04_17_151734_ucp.tgz</code>, etc and the corresponding 
<code class="ph codeph">.vars.tgz</code> files.</p>


<p class="p"><strong class="ph b">Step 2:</strong> Retrieve the DTR replica ID, the DTR version and the UCP version</p>

<p class="p">To retrieve the ID of the replica that was backed up, as well as the version of DTR, you need to extract the data from the <code class="ph codeph">.vars.tgz</code> file 
associated with the archive of the DTR metadata. You can retrieve this as follows:     
</p>


<pre class="pre codeblock"><code># tar -Oxf /root/restore/2018_04_17_151734_dtr_meta.vars.tgz meta.yml
backup_node="hpe-dtr01"
replica_id="<strong class="ph b">ad5204e8a4d0</strong>"
backup_source=""
ucp_version=""
dtr_version="<strong class="ph b">2.4.3</strong>"
</code></pre>

<pre class="pre codeblock"><code># tar -Oxf /root/restore/2018_04_17_151734_ucp.vars.tgz meta.yml
backup_node="hpe-ucp01"
replica_id=""
backup_source=""
ucp_version="<strong class="ph b">2.2.7</strong>"
dtr_version=""</code></pre>

<p class="p">Take note of the replica ID (<code class="ph codeph">ad5204e8a4d0</code>), the version of DTR (<code class="ph codeph">2.4.3</code>) and the version of UCP (<code class="ph codeph">2.2.7</code>).</p>


<p class="p"><strong class="ph b">Step 3:</strong> Populate the <code class="ph codeph">group_vars/backups</code> file</p>
   


<pre class="pre codeblock"><code>backup_swarm: "/root/restore/2018_04_17_151734_swarm.tgz"
backup_ucp: "/root/restore/2018_04_17_151734_ucp.tgz"
backup_dtr_meta: "/root/restore/2018_04_17_151734_dtr_meta.tgz"
backup_dtr_data: "/root/restore/2018_04_17_151734_dtr_data.tgz"
backup_dtr_id: "ad5204e8a4d0"
backup_dest: "/root/backups"
backup_server: &lt;IP of your ansible box&gt;</code></pre> 

<p class="p">You should populate your <code class="ph codeph">group_vars/backups</code> file as above, with the <code class="ph codeph">backup_dtr_id</code> variable
containing the value you retrieved in the preceding step as <code class="ph codeph">replica_id="<strong class="ph b">ad5204e8a4d0</strong>"</code>.</p>
    

<p class="p"><strong class="ph b">Step 4:</strong> Verify that your <code class="ph codeph">group_vars/vars</code> file specifies the correct versions of DTR and UCP.</p>
    

<p class="p">The playbooks use the versions of UCP and DTR as specified in your <code class="ph codeph">group_vars/vars</code> file to restore your backups.
You must ensure that the versions specified in your current <code class="ph codeph">group_vars/vars</code> file correspond to the versions in the backups as determined above.
</p>


<pre class="pre codeblock"><code># cat group_vars/vars | grep dtr_version
dtr_version: '2.4.3'</code></pre>
<pre class="pre codeblock"><code>
# cat group_vars/vars | grep ucp_version
ucp_version: '2.2.7'</code></pre>    

<p class="p"><strong class="ph b">Step 5:</strong> Restore UCP admin credentials if required</p>

<p class="p">You must ensure that the UCP admin credentials in your current <code class="ph codeph">group_vars/vars</code> file are those
that were in effect when you generated the backup files. If they have changed since then, you must restore the original 
credentials for the duration of the restore procedure.</p>


<p class="p"><strong class="ph b">Step 6:</strong> Restore your inventory (<code class="ph codeph">vm_hosts</code>)</p>
 

<p class="p">Your inventory must reflect the environment that was present when the backup files were created. 
You can find a copy of the inventory as it was when the backup was taken in the  <code class="ph codeph">*.vars.tgz</code> files. </p>
 

</div>

</div>
<div class="topic nested3" aria-labelledby="ariaid-title107" id="restore-ucp-dtr-details">
<h4 class="title topictitle4" id="ariaid-title107">Restore UCP and DTR</h4>
 
<div class="body">


<hr /><div class="note warning"><span class="warningtitle"><b>Warning:</b></span> This procedure is aimed at restoring a cluster after a disaster. It assumes you have lost all 
the VMs in your cluster and want to redeploy using data that you backed up earlier.

The solution follows Docker best practice, which means the swarm artifacts are not restored. 
You will need to restore your Docker volumes and your applications (stacks and services) when this procedure is complete.
</div><hr />


<p class="p"></p>


<ol class="ol">
<li class="li">Ensure that you have completed all the preliminary steps as outlined in the section <a class="xref" href="#before-restore">Before you restore</a> </li>

<li class="li">Run the restore playbook
<pre class="pre codeblock"><code>ansible-playbook -i vm_hosts restore.yml</code></pre>
<p class="p">You may need to modify <code class="ph codeph">restore.yml</code> before you run it, depending on what optional components you have deployed.
For example, if you have not deployed Splunk, you will need to comment out the line:</p>

<pre class="pre codeblock"><code>- import_playbook: playbooks/splunk_uf.yml</code></pre>
<p class="p">If not, you will get an error saying that Splunk components could not be restored when you run the playbook.</p>
    
</li>

<li class="li">If you are using the image scanning functionality in DTR, you will need to re-download the vulnerability database. 
For more information, see the Docker documentation  <a class="xref" href="https://docs.docker.com/datacenter/dtr/2.5/guides/admin/configure/set-up-vulnerability-scans/#get-the-security-scanning-license" target="_blank">here</a>.</li>

</ol>


<p class="p">You are now ready to restore your Docker volumes and your applications.</p>


</div>

</div>
<div class="topic nested3" aria-labelledby="ariaid-title108" id="restore-dtr-meta-images">
<h4 class="title topictitle4" id="ariaid-title108">Restore DTR metadata and DTR images</h4>
 
<div class="body">


<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> This procedure restores DTR metadata and images and assumes you have lost all the DTR VMs in your cluster.  It will redeploy using the DTR data that you backed up earlier and will also restore the images if the folder exported by the NFS VM is empty.</div><hr />



<ol class="ol">
<li class="li">Ensure that you have completed all the preliminary steps as outlined in the section <a class="xref" href="#before-restore">Before you restore</a>. 
In this scenario, you need the archives for the DTR metadata and the DTR images.
</li>

<li class="li">Ensure that all the DTR VMs listed in your inventory are destroyed, using the vSphere Web Client to delete them if required. If you want to restore the DTR images you should also delete the NFS VM.</li>

<li class="li">Remove the DTR nodes from the swarm by running the <code class="ph codeph">docker node rm &lt;DTR node&gt;</code> command on a UCP node for each DTR node in your cluster.
The following example shows the sequence of commands to use to remove the DTR nodes:

<pre class="pre codeblock"><code># docker node ls
ID         HOSTNAME                     STATUS              AVAILABILITY
aiz... *   hpe-ucp02.cloudra.local      Ready               Active
gvf...     hpe-dtr01.cloudra.local      Down                Active
ir4...     hpe-ucp03.cloudra.local      Ready               Active
mwf...     hpe-dtr02.cloudra.local      Down                Active
oqy...     hpe-ucp01.cloudra.local      Ready               Active
xqe...     hpe-worker01.cloudra.local   Ready               Active
zdu...     hpe-dtr03.cloudra.local      Down                Active
</code></pre>    


<pre class="pre codeblock"><code># docker node rm hpe-dtr01.cloudra.local
hpe-dtr01.cloudra.local
# docker node rm hpe-dtr02.cloudra.local
hpe-dtr02.cloudra.local
# docker node rm hpe-dtr03.cloudra.local
hpe-dtr03.cloudra.local
</code></pre>

<pre class="pre codeblock"><code># docker node ls
ID         HOSTNAME                     STATUS              AVAILABILITY        
aiz...     hpe-ucp02.cloudra.local      Ready               Active     
ir4...     hpe-ucp03.cloudra.local      Ready               Active    
oqy... *   hpe-ucp01.cloudra.local      Ready               Active    
xqe...     hpe-worker01.cloudra.local   Ready               Active</code></pre>

</li>


<li class="li">Run the restore script:
<pre class="pre codeblock"><code>./restore_dtr.sh</code></pre>        
</li>


<li class="li">If you are using the image scanning functionality in DTR, you will need to re-download the vulnerability database. 
For more information, see the Docker documentation  <a class="xref" href="https://docs.docker.com/datacenter/dtr/2.5/guides/admin/configure/set-up-vulnerability-scans/#get-the-security-scanning-license" target="_blank">here</a>.</li>



</ol>



</div>

</div>
<div class="topic nested3" aria-labelledby="ariaid-title109" id="restore-playbooks">
<h4 class="title topictitle4" id="ariaid-title109">Related playbooks</h4>
  
<div class="body">


<ul class="ul">
<li class="li"><code class="ph codeph">playbooks/restore_swarm.yml</code> is used to restore the swarm data</li>

<li class="li"><code class="ph codeph">playbooks/restore_dtr_meta.yml</code> is used to restore DTR metadata</li>

<li class="li"><code class="ph codeph">playbooks/restore_dtr_images.yml</code> is used to restore DTR images</li>

</ul>



</div>

</div>
</div>
</div>
<div class="topic nested1" aria-labelledby="ariaid-title110" id="simplivity-backups">
<h2 class="title topictitle2" id="ariaid-title110"><span class="ph"><span class="keyword">HPE SimpliVity</span></span> backups</h2>

<div class="body">

</div>

<div class="topic nested2" aria-labelledby="ariaid-title111" id="backup-restore-docker-volumes-svt">
<h3 class="title topictitle3" id="ariaid-title111">Backup and restore Docker persistent volumes with <span class="ph"><span class="keyword">HPE SimpliVity</span></span></h3>

<div class="body">
<p class="p">In order to restore a Docker volume, you need to restore a special VM that has been deployed for the sole 
  purpose of backing up Docker volumes. There is one such VM for each datastore defined in the datastores array 
  in the <code class="ph codeph">group_vars/vars</code> file. By default, a single datastore is specified in the playbooks:
</p>

<pre class="pre codeblock"><code>datastores: ['Docker_HPE']</code></pre>
  
<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> The use of a single datastore is recommended. If you have configured multiple datastores, you need to 
  understand and keep track of how your Docker volumes are distributed across the datastores.</div><hr />


<p class="p">The name of the special VM follows the pattern &lt;prefix&gt;-in-dockervols-&lt;Datastore&gt; where:</p>


<ul class="ul">
<li class="li">&lt;prefix&gt; is the value of the variable <code class="ph codeph">dummy_vm_prefix</code> from the file <code class="ph codeph">group_vars/vars</code></li>

<li class="li">&lt;Datastore&gt; is the name of the datastore</li>

</ul>



<p class="p">For example, based on the default values in the scripts, the VM name would be <code class="ph codeph">hpe-VM-in-dockervols-Docker_HPE</code></p>


<div class="section"><h4 class="title sectiontitle">Create a Docker volume</h4>
  
  
<p class="p">To see any existing Docker volumes created using the vSphere driver, use the <code class="ph codeph">docker volume ls</code> command and limit the results to 
  those volumes created using the vSphere driver. If you have already used the playbooks to install Prometheus for example, you may see 
a listing as follows:</p>

  
<pre class="pre codeblock"><code>
# docker volume ls | grep vsphere
vsphere:latest      prom_hpe-db-data@Docker_HPE
</code></pre>  
  
<p class="p">To create a Docker volume named <code class="ph codeph">test_01</code>, you can use the <code class="ph codeph">docker volume create</code> command 
  specifying the vSphere driver:</p>
  
  
<pre class="pre codeblock"><code>
# docker volume create -d vsphere test_01
test_01
</code></pre>  
  
<p class="p">You can check that the volume exists using the <code class="ph codeph">docker volume ls</code> command:</p>
  
  
<pre class="pre codeblock"><code>
# docker volume ls | grep vsphere
vsphere:latest      prom_hpe-db-data@Docker_HPE
vsphere:latest      test_01@Docker_HPE
</code></pre>  

<p class="p">You can attach a container to the volume and then add data to it by creating a text file with some arbitrary content:</p>

  
<pre class="pre codeblock"><code>
# docker run -it --rm -v test_01:/tmp alpine sh -c "echo some test data here &gt; /tmp/foo.txt"
</code></pre>  

<p class="p">If this is the first time you have used the <code class="ph codeph">alpine</code> image, you may see additional output relating to download of image layers:</p>

  
<pre class="pre codeblock"><code>
Unable to find image 'alpine:latest' locally
latest: Pulling from library/alpine
88286f41530e: Already exists
Digest: sha256:f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d
Status: Downloaded newer image for alpine:latest
</code></pre>  

<p class="p">The container will exit once the shell command has run and any unnamed volumes will be removed. 
  However, the named volume <code class="ph codeph">test_01:/tmp</code> will persist. 
  To check that the data is still available after the container exited, spin up a new container and try to retrieve the data:</p>


<pre class="pre codeblock"><code>
# docker run -it --rm -v test_01:/tmp alpine sh -c "cat /tmp/foo.txt"	
<strong class="ph b">some test data here</strong>
</code></pre>
  
</div>

  
<div class="section"><h4 class="title sectiontitle">Automated backup</h4>
  
  
<p class="p">By default, the special VM and any Docker volume in the <code class="ph codeph">dockvols</code> folder are backed up every hour. 
  This is controlled by the following settings in the <code class="ph codeph">group_vars/vars</code> file.</p>
  

<pre class="pre codeblock"><code>
backup_policies:
 - name: 'hpe-gold'
   days: 'All'
   start_time: '00:00'
   frequency: '60'
   retention: '43200'

dummy_vm_prefix: 'hpe-VM'

docker_volumes_policy: 'hpe-gold'
</code></pre>

<p class="p">The backup policy <code class="ph codeph">hpe-gold</code> is assigned to the special VM that is used to back up the Docker volumes. 
  This policy specifies that a backup is taken every hour (<code class="ph codeph">frequency: '60'</code> means sixty minutes) 
  while the backup is retained for one month (<code class="ph codeph">retention: '43200'</code> means 43200 minutes or thirty days).</p>
  
</div>
  

<div class="section"><h4 class="title sectiontitle">Manual backup</h4>


<p class="p">Rather than waiting for an automated backup to take place, you can create a backup immediately. 
  Right-click on the special VM, in this case <code class="ph codeph">hpe-VM-in-dockervols-Docker_HPE</code>, 
  select <code class="ph codeph">All <span class="ph"><span class="keyword">HPE SimpliVity</span></span> Actions</code> and choose <code class="ph codeph">Backup Virtual Machine</code> as shown in 
  <a class="xref" href="#backup-restore-docker-volumes-svt__backup-virtual-machine">Figure 22</a>.</p>


<div class="fig fignone" id="backup-restore-docker-volumes-svt__backup-virtual-machine">
 
<img class="image" src="media/backup-virtual-machine.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 22. </b></span>Backup virtual machine</span></div><p> </p></div>

 
<p class="p">You can specify a backup name, in this case <code class="ph codeph">manual_backup_test_01</code>, as shown in 
 <a class="xref" href="#backup-restore-docker-volumes-svt__backup-vm-details">Figure 23</a>.</p>



<div class="fig fignone" id="backup-restore-docker-volumes-svt__backup-vm-details">
 
<img class="image" src="media/backup-vm-details.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 23. </b></span>Backup virtual machine details</span></div><p> </p></div>


</div>


<div class="section"><h4 class="title sectiontitle">Restore</h4>
  
  
<p class="p">Right-click on the special VM, in this case hpe-VM-in-dockervols-Docker_HPE. 
  On the <code class="ph codeph">Configure</code> tab, select <code class="ph codeph"><span class="ph"><span class="keyword">HPE SimpliVity</span></span> Search Backups</code> as shown in 
<a class="xref" href="#backup-restore-docker-volumes-svt__search-backups">Figure 24</a>.
</p>
  
  
<div class="fig fignone" id="backup-restore-docker-volumes-svt__search-backups">
 
<img class="image" src="media/search-backups.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 24. </b></span>Search backups</span></div><p> </p></div>


<p class="p">You can narrow the search based on the time of the backup. If you are restoring from an automatic backup, 
  the name will be the timestamp of the backup. If you are restoring from a manual backup, the name will be the 
  one you specified earlier when creating the backup, in this case <code class="ph codeph">manual_backup_test_01</code>.</p>

  
<p class="p">Right-click on the backup you wish to restore, as shown in 
  <a class="xref" href="#backup-restore-docker-volumes-svt__restore-vm">Figure 25</a>, 
  and select <code class="ph codeph">Restore Virtual Machine</code>.</p>
  
  
<div class="fig fignone" id="backup-restore-docker-volumes-svt__restore-vm">
 
<img class="image" src="media/restore-vm.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 25. </b></span>Restore virtual machine</span></div><p> </p></div>


<p class="p">In the details screen, shown in 
  <a class="xref" href="#backup-restore-docker-volumes-svt__restore-vm-details">Figure 26</a>, 
  you can choose a name for the new virtual machine and specify the datastore.</p>


<div class="fig fignone" id="backup-restore-docker-volumes-svt__restore-vm-details">
 
<img class="image" src="media/restore-vm-details.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 26. </b></span>Restore virtual machine details</span></div><p> </p></div>


<p class="p">The name of the new virtual machine will default to a combination of the special VM name and a timestamp, 
  in this instance <code class="ph codeph">hpe-VM-in-dockervols-Docker_HPE-2018-11-26-20h47m01s</code>. 
  The datastore should be the one specified in the datastores array from the <code class="ph codeph">group_vars/vars</code> file. 
  Click OK to restore the virtual machine.</p>


<p class="p">Once the virtual machine has been restored, navigate to the datastore and locate the new VM in the file browser, as shown in 
<a class="xref" href="#backup-restore-docker-volumes-svt__browse-restored-vm">Figure 27</a>.
</p>


<div class="fig fignone" id="backup-restore-docker-volumes-svt__browse-restored-vm">
 
<img class="image" src="media/browse-restored-vm.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 27. </b></span>Browse for restored virtual machine</span></div><p> </p></div>


<p class="p">Navigate to the folder named <code class="ph codeph">1111111-1111-1111-1111-...</code> as shown in 
  <a class="xref" href="#backup-restore-docker-volumes-svt__vmdk-files">Figure 28</a>. 
  You will see files with names based on the Docker volume name that you used at the start, 
  in this instance <code class="ph codeph">test_01.vmdk</code> and <code class="ph codeph">test_01-478...f1f.vmfd</code></p>

  
<div class="fig fignone" id="backup-restore-docker-volumes-svt__vmdk-files">
 
<img class="image" src="media/vmdk-files.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 28. </b></span>Locate vmdk and vmfd files</span></div><p> </p></div>


<p class="p">You need to move these two files to the <code class="ph codeph">dockvols</code> sub-directory named <code class="ph codeph">1111111-1111-1111-1111-...</code> 
  in the same datastore. Right click on the <code class="ph codeph">.vmdk</code> file and choose <code class="ph codeph">Move to...</code> as shown in 
  <a class="xref" href="#backup-restore-docker-volumes-svt__move-to">Figure 29</a>.</p>



<div class="fig fignone" id="backup-restore-docker-volumes-svt__move-to">
 
<img class="image" src="media/move-to.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 29. </b></span>Move files</span></div><p> </p></div>

  
<p class="p">Set the destination folder to the <code class="ph codeph">dockvols</code> sub-directory named <code class="ph codeph">1111111-1111-1111-1111-...</code> 
  as shown in <a class="xref" href="#backup-restore-docker-volumes-svt__destination">Figure 30</a>.</p>
  
  
<div class="fig fignone" id="backup-restore-docker-volumes-svt__destination">
 
<img class="image" src="media/destination.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 30. </b></span>Move to destination</span></div><p> </p></div>

 
<p class="p">It is only necessary to move the <code class="ph codeph">.vmdk</code> file as the <code class="ph codeph">.vmfd</code> file will automatically follow. 
  The <code class="ph codeph">dockvols</code> sub-directory named <code class="ph codeph">1111111-1111-1111-1111-...</code> should now contain both files as shown in 
  <a class="xref" href="#backup-restore-docker-volumes-svt__moved">Figure 31</a>.</p>
 
  
<div class="fig fignone" id="backup-restore-docker-volumes-svt__moved">
 
<img class="image" src="media/moved.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 31. </b></span>Files moved to destination</span></div><p> </p></div>
  
  
  
</div>


<div class="section"><h4 class="title sectiontitle">Test the restore</h4>
  
  
<p class="p">You can check that the volume <code class="ph codeph">test_01</code> has been restored by using the <code class="ph codeph">docker volume ls</code> command again.</p>
  

<pre class="pre codeblock"><code>
# docker volume ls | grep vsphere
vsphere:latest      prom_hpe-db-data@Docker_HPE
vsphere:latest      <strong class="ph b">test_01@Docker_HPE</strong>
</code></pre>

<p class="p">You can verify that the volume contains the correct data by spinning up a container and running a shell command:</p>

  
<pre class="pre codeblock"><code>
# docker run -it --rm -v test_01:/tmp alpine sh -c "cat /tmp/foo.txt"
<strong class="ph b">some test data here</strong>
</code></pre>  

<p class="p">The data you entered in the text file before performing the backup and 
  deleting the volume is available once again after restoring the volume.</p>


</div>




  </div>

</div>
</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title112" id="lifecycle">
<h1 class="title topictitle1" id="ariaid-title112">Solution lifecycle management</h1>

<div class="body">
<div class="section">

<p class="p">Lifecycle management with respect to this solution refers to the maintenance and
management of software and hardware of various components that make up the solution
stack. Lifecycle management is required to keep the solution up-to-date and ensure
that the latest versions of the software are running to provide optimal performance,
security and to fix any existing defects within the product.</p>

    
<p class="p">In this section, we will cover life cycle management of the different components that
are used in this solution. The lifecycle of the following stacks need to be maintained and managed: </p>

    
<ul class="ul">
<li class="li">Monitoring Tools (Splunk or Prometheus and Grafana)</li>

<li class="li">Docker Enterprise Edition Environment</li>

<li class="li">Virtual Machine Operating systems</li>

<li class="li"><span class="ph"><span class="keyword">HPE SimpliVity</span></span> environment</li>

</ul>

    
<p class="p">The general practice and recommendation is to follow a bottom-up approach for
updating all components of the environment and making sure the dependencies are met.
In our solution, we would start with <span class="ph"><span class="keyword">HPE SimpliVity</span></span> and end with the monitoring
environment. If all components are not being updated at the same time, the same
approach can be followed – updating only the components that require updates while
adhering to the interdependencies for each component that is being updated.</p>

</div>


    

</div>

<div class="topic nested1" aria-labelledby="ariaid-title113" id="lifecycle-svt">
<h2 class="title topictitle2" id="ariaid-title113"><span class="ph"><span class="keyword">HPE SimpliVity</span></span> environment</h2>

<div class="body">

<div class="section">
<p class="p">The <span class="ph"><span class="keyword">HPE SimpliVity</span></span> environment is made up of proprietary <span class="ph"><span class="keyword">HPE SimpliVity</span></span> software, VMware software and HPE firmware. 
  There are interdependencies between the various components that need to be accounted for and are provided in the table below. 
  The components in <a class="xref" href="#lifecycle-svt__svt-components-table-content">Table 25</a> are part of the <span class="ph"><span class="keyword">HPE SimpliVity</span></span> environment that require lifecycle management.</p>


<p class="p">In general, ensure that the software bits for the Arbiter and vSphere extension corresponding to an OmniStack release are used.</p>

  

<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="lifecycle-svt__svt-components-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 25. </span><span class="ph"><span class="keyword">HPE SimpliVity</span></span>components</span></caption><colgroup><col /><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12726">Order</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12729">Component</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12732">Dependency (compatibility)</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12735">Download/Documentation</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12726 ">1.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12729 "><span class="ph"><span class="keyword">HPE SimpliVity</span></span> Arbiter</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12732 ">1. HPE OmniStack</td>
<td class="entry nocellnorowborder" rowspan="4" align="left" valign="top" headers="d29e12735 "><span class="ph"><span class="keyword">HPE SimpliVity</span></span> OmniStack for vSphere Upgrade Guide

<p class="p">Download software bits from HPE’s support website.</p>

  
<p class="p"><a class="xref" href="http://www.hpe.com/support" target="_blank">http://www.hpe.com/support</a></p>
  
</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12726 ">2.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12729 "><span class="ph"><span class="keyword">HPE SimpliVity</span></span> VMware Plug-in</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12732 ">1. <span class="ph"><span class="keyword">HPE SimpliVity</span></span> Arbiter 
<p class="p">2. HPE OmniStack</p>

</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12726 ">3.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12729 ">HPE Omnistack</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12732 ">1. <span class="ph"><span class="keyword">HPE SimpliVity</span></span> VMware Plug-in  
<p class="p">2. <span class="ph"><span class="keyword">HPE SimpliVity</span></span> Arbiter</p>

</td>
  
  
</tr>
</tbody></table>
</div>

      
</div>

  
<div class="section"><h3 class="title sectiontitle">VMware components</h3>
  
  
<p class="p">The <span class="ph"><span class="keyword">HPE SimpliVity</span></span> solution used in this deployment guide is built on VMware vSphere. VMware ESXi and vCenter 
  (see <a class="xref" href="#lifecycle-svt__vmware-components-table-content">Table 26</a>) 
  are the two components from VMware that are leveraged by the <span class="ph"><span class="keyword">HPE SimpliVity</span></span> software.</p>
  



<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="lifecycle-svt__vmware-components-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 26. </span>VMware components</span></caption><colgroup><col /><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12860">Order</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12863">Component</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12866">Dependency (compatibility)</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12869">Download/Documentation</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12860 ">1.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12863 ">VMware vCenter</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12866 ">1. HPE OmniStack
<p class="p">2. VMware ESXi</p>

</td>
<td class="entry nocellnorowborder" rowspan="3" align="left" valign="top" headers="d29e12869 "><a class="xref" href="https://support.hpe.com/hpsc/doc/public/display?sp4ts.oid=1010292248&amp;docLocale=en_US&amp;docId=emr_na-a00028056en_us" target="_blank">VMware Upgrade for SimpliVity</a>
</td>
</tr>
  
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12860 ">1.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12863 ">VMware ESXi</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12866 ">1. HPE OmniStack
<p class="p">2. VMware vCenter</p>

</td>  
</tr>
  
</tbody></table>
</div>

  

</div>
  
  
<div class="section"><h3 class="title sectiontitle">HPE server software</h3>


<p class="p"><span class="ph"><span class="keyword">HPE SimpliVity</span></span> servers are based on HPE server platforms and require a compatible firmware version to 
  function with HPE OmniStack Software, as shown in
  <a class="xref" href="#lifecycle-svt__hpe-components-table-content">Table 27</a>.</p>




<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="lifecycle-svt__hpe-components-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 27. </span>HPE server components</span></caption><colgroup><col /><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12951">Order</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12954">Component</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12957">Dependency (compatibility)</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e12960">Download/Documentation</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12951 ">1.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12954 ">HPE Firmware</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12957 ">1. HPE OmniStack
</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e12960 "><a class="xref" href="https://support.hpe.com/hpesc/public/home/result?qt=HPE+SimpliVity+380" target="_blank">Firmware Upgrade for <span class="ph"><span class="keyword">HPE SimpliVity</span></span></a>
</td>  
</tr>
</tbody></table>
</div>

  
</div>
  
  
  
  </div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title114" id="lifecycle-vdvs">
<h2 class="title topictitle2" id="ariaid-title114">vSphere Docker Volume Service Plug-in</h2>

<div class="body">
<div class="section">

<p class="p"> vSphere Docker Volume service plug-in is part of an open source project by VMware
that enables running stateful containers by providing persistent Docker volumes
leveraging existing storage technology from VMware. There are two parts to the
plug-in, namely, client software and server software (see <a class="xref" href="#lifecycle-vdvs__vdvs-components-table-conref">Table 28</a>).
Every version of the plug-in that is released includes both pieces of software and
it is imperative that the version number installed on the client side and server
side are the same. </p>

<p class="p">When updating the Docker Volume service plug-in, ensure the ESXi version you are
running is supported and that the client software is compatible with the operating
system.</p>


<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="lifecycle-vdvs__vdvs-components-table-conref" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 28. </span>vSphere Docker Volume service components</span></caption><colgroup><col /><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13037">Order</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13040">Component</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13043">Dependency (compatibility)</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13046">Download/Documentation</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13037 ">1.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13040 ">Server Software</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13043 "><ol class="ol"><li class="li">VMware ESXi</li>
<li class="li">Docker EE</li>
</ol>
</td>
<td class="entry nocellnorowborder" rowspan="2" align="left" valign="middle" headers="d29e13046 "><a class="xref" href="http://vmware.github.io/vsphere-storage-for-docker/documentation/index.html" target="_blank">vSphere Docker Volume Service on GitHub</a></td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13037 ">2.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13040 ">Client Software</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13043 "><ol class="ol"><li class="li">VM Operating System</li>
<li class="li">Docker EE</li>
</ol>
</td>
</tr>
</tbody></table>
</div>

</div>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title115" id="lifecycle-rhel">
<h2 class="title topictitle2" id="ariaid-title115">Red Hat Enterprise Linux operating system</h2>

<div class="body">
<div class="section">

<p class="p">This solution is built using Red Hat Enterprise Linux (see <a class="xref" href="#lifecycle-rhel__rhel-components-table-content">Table 29</a>) as the base operating system. When upgrading the operating system on the 
VMs, first verify that the OS version is compatible with Docker EE by looking at the Docker OS compatibility matrix.</p>




<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="lifecycle-rhel__rhel-components-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 29. </span>Operating system</span></caption><colgroup><col /><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13135">Order</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13138">Component</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13141">Dependency (compatibility)</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13144">Download/Documentation</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13135 ">1.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13138 ">Red Hat Enterprise Linux</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13141 "><ul class="ul"><li class="li">Docker EE</li>
<li class="li">vDVS client software plugin</li>
</ul>
</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13144 "><a class="xref" href="https://access.redhat.com/articles/11258" target="_blank">RHEL</a></td>
</tr>
</tbody></table>
</div>

    
    

</div>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title116" id="lifecycle-docker-ee">
<h2 class="title topictitle2" id="ariaid-title116">Docker EE Environment</h2>

<div class="body">
<div class="section">

<p class="p">Each release of Docker Enterprise Edition contains three technology components – UCP,
DTR and the Docker Daemon or Engine. It is imperative that the components belonging
to the same version are deployed or upgraded together – see <a class="xref" href="#lifecycle-docker-ee__dockeree-components-table-content">Table 30</a>
</p>

<p class="p">A banner will be displayed on the UI, as shown in <a class="xref" href="#lifecycle-docker-ee__docker-update">Figure 32</a>, when an update is
available for UCP or DTR. You can start the upgrade process by clicking on the
banner.</p>

<div class="fig fignone" id="lifecycle-docker-ee__docker-update">

<img class="image" src="media/dockerupdate.png" />
<div><span class="figcap"><span class="fig--title-label"><b>Figure 32. </b></span>Docker update notification</span></div><p> </p></div>


<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="lifecycle-docker-ee__dockeree-components-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 30. </span>Docker EE components</span></caption><colgroup><col /><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13232">Order</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13235">Component</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13238">Dependency (compatibility)</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13241">Download/Documentation</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13232 ">1.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13235 ">Docker Daemon/Engine</td>
<td class="entry nocellnorowborder" rowspan="3" align="left" valign="middle" headers="d29e13238 "><ol class="ol">
<li class="li">VM Operating System</li>

<li class="li">vDVS plugin</li>

<li class="li">Prometheus and Grafana</li>

</ol>
</td>
<td class="entry nocellnorowborder" rowspan="3" align="left" valign="middle" headers="d29e13241 "><ul class="ul">
<li class="li"><a class="xref" href="https://success.docker.com/Policies/Maintenance_Lifecycle" target="_blank">Docker Lifecycle Maintenance</a></li>

<li class="li"><a class="xref" href="https://success.docker.com/Policies/Compatibility_Matrix" target="_blank">Docker Compatibility Matrix</a></li>

</ul>

</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13232 ">2.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13235 ">Universal Control Plane</td>
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13232 ">3.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13235 ">Docker Trusted Registry</td>
</tr>
</tbody></table>
</div>

</div>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title117" id="lifecycle-monitoring">
<h2 class="title topictitle2" id="ariaid-title117">Monitoring Tools</h2>

<div class="body">
<div class="section">

    
<p class="p">To learn more about upgrading Splunk, see the relevant documentation at <a class="xref" href="http://docs.splunk.com/Documentation/Splunk/7.1.2/Installation/HowtoupgradeSplunk" target="_blank">How to upgrade Splunk Enterprise</a>.</p>
    

<p class="p">The Sysdig agent runs as a container and the latest version is pulled from the Docker hub on first installation. Re-run the <code class="ph codeph">install_sysdig.yml</code> playbook
to update to the newest version if required.</p>

    
    
<p class="p">Prometheus and Grafana monitoring tools (see <a class="xref" href="#lifecycle-monitoring__promgraf-table-content">Table 31</a>) run as containers within the
Docker environment. Newer versions of these tools can be deployed by pulling the
Docker images from Docker Hub. Verify that the version of Prometheus that is being
used is compatible with the version of Docker EE.</p>

    

<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="lifecycle-monitoring__promgraf-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 31. </span>Monitoring tools: Prometheus and Grafana</span></caption><colgroup><col /><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13362">Order</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13365">Component</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13368">Dependency (compatibility)</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13371">Download/Documentation</th>
</tr>
</thead><tbody class="tbody">  
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13362 ">1.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13365 ">Prometheus</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13368 "><ol class="ol"><li class="li">Grafana</li>
<li class="li">Docker EE</li>
</ol>
</td>
<td class="entry nocellnorowborder" rowspan="2" align="left" valign="middle" headers="d29e13371 "><ol class="ol"><li class="li">Prometheus Images on Docker Hub</li>
<li class="li"><a class="xref" href="http://docs.grafana.org/installation/upgrading/" target="_blank">Upgrading Grafana</a></li>

</ol>
</td>
</tr>    
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13362 ">2.</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13365 ">Grafana</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13368 "><ol class="ol"><li class="li">Prometheus</li>
<li class="li">Docker EE</li>
</ol>
</td>   
</tr>
</tbody></table>
</div>
    
    
    
</div>

</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title118" id="summary">
<h1 class="title topictitle1" id="ariaid-title118">Summary</h1>

<div class="body">
<p class="p">This document has described how to architect and deploy a Docker CaaS platform on <span class="ph"><span class="keyword">HPE SimpliVity</span></span>,
using Ansible playbooks to quickly install and deploy a production ready
container environment. This deployment includes a highly available container cluster with
backup services and persistent data support. This solution is ideal for customers looking to
run containers on VMs to take advantage of the resource efficient usage of virtual machines
for Docker containers, and having the ability to run legacy and new container applications
side-by-side. Customers deploying Docker containers on a large scale, on Linux and Microsoft
Windows, should consider <span class="ph"><span class="keyword">HPE SimpliVity</span></span> as the deployment infrastructure. </p>

</div>

</div>
<div class="topic nested0" aria-labelledby="ariaid-title119" id="bom">
<h1 class="title topictitle1" id="ariaid-title119">Appendix A</h1>

<div class="body">

  
</div>

<div class="topic nested1" aria-labelledby="ariaid-title120" id="software-licences-svt">
<h2 class="title topictitle2" id="ariaid-title120">Software Licenses</h2>

<div class="body">
<div class="section">

<p class="p">Licenses are required for the following software components:</p>


<ul class="ul">
<li class="li">VMware</li>

<li class="li">Red Hat Linux</li>

<li class="li">Docker EE</li>


<li class="li">Splunk (optional software)</li>

<li class="li">Sysdig (optional software)</li>

</ul>
  
  
</div>
  
</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title121" id="customer-certs">
<h1 class="title topictitle1" id="ariaid-title121">Appendix B: Using customer supplied certificates for UCP and DTR</h1>


<div class="body">
    
<p class="p"><a class="xref" href="#customer-certs__customer-certs-table-content">Table 32</a> lists the variables used when configuring 
customer supplied certificates for UCP and DTR.
</p>
    
    

<div class="tablenoborder">

<table cellpadding="4" cellspacing="0" summary="" id="customer-certs__customer-certs-table-content" class="table" frame="void" border="1" rules="all"><caption><span class="tablecap"><span class="table--title-label">Table 32. </span>Customer certs variables</span></caption><colgroup><col /><col /><col /></colgroup><thead class="thead" style="text-align:left;">
<tr class="row">
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13532">Variable</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13535">File</th>
<th class="entry nocellnorowborder" align="left" valign="top" id="d29e13538">Description</th>
</tr>
</thead><tbody class="tbody">
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13532 ">ucp_certs_dir</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13535 ">group_vars/vars</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13538 ">
<ul class="ul">
<li class="li">If <strong class="ph b">ucp_certs_dir</strong> is not defined, UCP is installed with self-signed
certificates and DTR is installed with the <code class="ph codeph">--ucp-insecure-tls</code>
switch</li>

<li class="li">If <strong class="ph b">ucp_certs_dir</strong> is defined, this is a folder on the Ansible
machine that must contain 3 files:
<ul class="ul">
<li class="li"><code class="ph codeph">ca.pem</code>, the root CA certificate in PEM format</li>

<li class="li"><code class="ph codeph">cert.pem</code>, the server certificate optionally followed by
intermediate CAs</li>

<li class="li"><code class="ph codeph">key.pem</code>,  the private key that comes with the <code class="ph codeph">cert.pem</code>
certificates</li>

</ul>
    
</li>

</ul>
    
</td> 
</tr>
<tr class="row">
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13532 ">dtr_certs_dir</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13535 ">group_vars/vars</td>
<td class="entry nocellnorowborder" align="left" valign="top" headers="d29e13538 ">    
<ul class="ul">
<li class="li">If <strong class="ph b">dtr_certs_dir</strong> is not defined, DTR is installed with self-signed
certificates</li>

<li class="li">If <strong class="ph b">dtr_certs_dir</strong> is defined, this is a folder on the
Ansible machine that must contain 3 files: 
<ul class="ul">
<li class="li"><code class="ph codeph">ca.pem</code>,  the root CA certificate in PEM format</li>

<li class="li"><code class="ph codeph">cert.pem</code>, the server certificate optionally followed by
intermediate CAs</li>

<li class="li"><code class="ph codeph">key.pem</code>, the private key that comes with the <code class="ph codeph">cert.pem</code>
certificates</li>

</ul>

</li>

</ul>
    
</td>
</tr>
</tbody></table>
</div>








<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> 
<p class="p">The installation will fail if the <code class="ph codeph">ca.pem</code>, <code class="ph codeph">cert.pem</code> and <code class="ph codeph">key.pem</code> files
cannot be found in the folders designated by <code class="ph codeph">dtr_certs_dir</code> and <code class="ph codeph">ucp_certs_dir</code> or if
they don't constitute valid certificates.</p>


<p class="p">The certificates should specify the names of the FQDNs of the load balancer and the FQDNs of the VMs themselves. 
This applies to both the UCP server certificate and the DTR server certificate.</p>

</div><hr />

    
    
    
    

</div>

<div class="topic nested1" aria-labelledby="ariaid-title122" id="customer-certs-generate-test">
<h2 class="title topictitle2" id="ariaid-title122">Generating and testing certificates</h2>

<div class="body">
<p class="p">In the example described here we have a root CA named <code class="ph codeph">Example root CA</code> and an 
  intermediate CA named <code class="ph codeph">Intermediate CA valid 3 years</code>. The intermediate CA signs the 
  server certificates for UCP and DTR. </p>


<p class="p">Below is the start of the output displayed by running the <code class="ph codeph">openssl x509</code> utility 
 against the <code class="ph codeph">ca.pem</code> file (the root CA certificate).</p>

<pre class="pre codeblock"><code>
[root@ansible ucp_certs]# openssl x509 -text -noout -in ca.pem|head -14
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            0d:07:ca:ea:00:37:77:6e:25:e0:18:3e:0e:db:80:0f:11:cb:1b:3f
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=Example Root CA
        Validity
            Not Before: Apr 24 20:12:01 2018 GMT
            Not After : Apr 21 20:12:30 2028 GMT
        Subject: CN=Example Root CA
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (4096 bit)</code></pre>  
  
  
<p class="p">Here is an excerpt from the example <code class="ph codeph">ca.pem</code> file:</p>


<pre class="pre codeblock"><code>-----BEGIN CERTIFICATE-----
MIIFJTCCAw2gAwIBAgIUDQfK6gA3d24l4Bg+DtuADxHLGz8wDQYJKoZIhvcNAQEL
BQAwGjEYMBYGA1UEAxMPRXhhbXBsZSBSb290IENBMB4XDTE4MDQyNDIwMTIwMVoX
...
...
uXzYbCtU6Jt9B3fayAeWWswQv+jQSzuuA3re0M1x838iIZWDx93f4yLJWLJl7xsY
btvKBmqKDCsAqsQLFLnNj/JyYq4e9a6Xxcyn9FXNpzuEsfjfNGHn+csY+w3f987T
MNviy376xZbyAc1CV5kgmnZzjU5bDkgT8Q==
-----END CERTIFICATE-----</code></pre>

<p class="p">The <code class="ph codeph">cert.pem</code> file should contain the server certificate itself, followed by your intermediate CA's certificate.
The following example shows how to extract the intermediate CA certificate from <code class="ph codeph">cert.pem</code> and 
 to save it to a file named <code class="ph codeph">intca.pem</code>. Using the <code class="ph codeph">openssl x509</code> utility, you can 
display the content of the <code class="ph codeph">intca.pem</code> file in human readable form.  This certificate was signed by 
the example CA above (<code class="ph codeph">Issuer = 'Example Root CA'</code>).
</p>


<pre class="pre codeblock"><code>[root@ansible ucp_certs]# openssl x509 -text -noout -in intca.pem|head -14
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            6b:1e:0c:86:20:cf:f0:88:d2:52:0d:5d:b9:56:fa:91:87:a0:49:18
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=Example Root CA
        Validity
            Not Before: Apr 24 20:12:09 2018 GMT
            Not After : Apr 23 20:12:39 2021 GMT
        Subject: CN=Intermediate CA valid 3 years
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (4096 bit)</code></pre>
  
<p class="p">Here is an excerpt from the <code class="ph codeph">incta.pem</code> file showing the example Intermediate CA certificate:</p>


<pre class="pre codeblock"><code>-----BEGIN CERTIFICATE-----
MIIFcjCCA1qgAwIBAgIUax4MhiDP8IjSUg1duVb6kYegSRgwDQYJKoZIhvcNAQEL
BQAwGjEYMBYGA1UEAxMPRXhhbXBsZSBSb290IENBMB4XDTE4MDQyNDIwMTIwOVoX
...
...
o2tL5nwR7ROiAr/kk9MIRzWrLNbc4cYth7jEjspU9dBqsXgsTozzWlwqI9ybZwvL
Ni1JnZandVlyQdoOaB2M/1DNFfKvwW3JeArKvDA9j95n/BWFTjoZ+YOz9pYit6T7
1GCGu3be
-----END CERTIFICATE-----</code></pre>
  
<p class="p">The <code class="ph codeph">openssl x509</code> utility will only decrypt the first certificate found in <code class="ph codeph">cert.pem</code>,  
  so you don't need to extract the server certificate from <code class="ph codeph">cert.pem</code>.
In this example, the server certificate is signed by the intermediate CA above. Note the <code class="ph codeph">Subject Alternate Names: hpe-ucp.cloudra.local</code> 
  is the FQDN of the UCP load balancer, and the other names are those of the UCP instances 
  (<code class="ph codeph">hpe-ucp01.cloudra.local</code>, <code class="ph codeph">hpe-ucp02.clodura.local</code>, <code class="ph codeph">hpe-ucp03.cloudra.local</code>).
</p>

  
<pre class="pre codeblock"><code>[root@ansible ucp_certs]# openssl x509 -text -noout -in server.pem
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            25:d9:f8:1d:9b:1d:23:f1:21:56:54:f2:43:cc:4f:0e:73:22:be:ec
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=Intermediate CA valid 3 years
        Validity
            Not Before: Apr 24 20:17:30 2018 GMT
            Not After : Apr 24 20:18:00 2019 GMT
        Subject: O=HPE, OU=CloudRA Team, CN=hpe-ucp.cloudra.local
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                CA Issuers - URI:http://localhost:8200/v1/intca
      ( portions removed )
 
            X509v3 Subject Alternative Name:
                DNS:hpe-ucp.cloudra.local, DNS:hpe-ucp01.cloudra.local, DNS:hpe-ucp02.cloudra.local, DNS:hpe-ucp03.cloudra.local</code></pre>  
 

<p class="p">The following excerpts from <code class="ph codeph">cert.pem</code> show the first certificate which is the server certificate itself
  and the second certificate which is the intermediate CA's certificate. </p>

  
<pre class="pre codeblock"><code>-----BEGIN CERTIFICATE-----
MIIFGTCCAwGgAwIBAgIUJdn4HZsdI/EhVlTyQ8xPDnMivuwwDQYJKoZIhvcNAQEL
BQAwKDEmMCQGA1UEAxMdSW50ZXJtZWRpYXRlIENBIHZhbGlkIDMgeWVhcnMwHhcN
...
...
s0R4I3Qnc50oNISng5l7wW1d4RMMwmXQhG1H5QKAUjHfJXH4bNtIzKxw/zGTVr4Z
llYKbEwJcgAvvfkn+w==
-----END CERTIFICATE-----</code></pre>  
  
<pre class="pre codeblock"><code>-----BEGIN CERTIFICATE-----
MIIFcjCCA1qgAwIBAgIUax4MhiDP8IjSUg1duVb6kYegSRgwDQYJKoZIhvcNAQEL
BQAwGjEYMBYGA1UEAxMPRXhhbXBsZSBSb290IENBMB4XDTE4MDQyNDIwMTIwOVoX
...
...
Ni1JnZandVlyQdoOaB2M/1DNFfKvwW3JeArKvDA9j95n/BWFTjoZ+YOz9pYit6T7
1GCGu3be
-----END CERTIFICATE-----</code></pre>

<p class="p">Finally, here is an excerpt from <code class="ph codeph">key.pem</code>, the private key that goes with the server certificate.</p>


<pre class="pre codeblock"><code>-----BEGIN RSA PRIVATE KEY-----
MIIEpQIBAAKCAQEA5rmmb52ufE80a3cXhY2HSRZNazb7/fipXY1rZ+U5+rJv9BN5
d/X3NTroSE8/PvoS/maGkHCnURGNqbu/G2umKN/tm/eSpDY861YnGWxj+bc0gtiU
...
...
A0SGidSMk3hFX1Iaftgx4EUGbrzZO7I8M5RO64U1aMFNFyj4XghJ2mZTdNelwNBw
pr/fYulyi5lYPalQHYH3OyvNqQQ3arEbTbZp8hEyY0gxtZRXmmaoqOY=
-----END RSA PRIVATE KEY-----</code></pre>

</div>

</div>
<div class="topic nested1" aria-labelledby="ariaid-title123" id="verify-certs">
<h2 class="title topictitle2" id="ariaid-title123">Verify your certificates</h2>

<div class="body">
<p class="p">The playbooks do not verify the validity of the certificate files you supply so  you should verify them manually 
before you start your deployment.</p>


<div class="section"><h3 class="title sectiontitle">Verify that the private and the server cert match</h3>
 
<p class="p">On the Ansible box, run the following commands:</p>
  
 
<pre class="pre codeblock"><code>ckcert=$(openssl x509 -noout -modulus -in cert.pem | openssl md5)
ckkey=$(openssl rsa -noout -modulus -in key.pem| openssl md5)
if [ "$ckkey" == "$ckcert" ] ; then echo "Private key and Certificate match" ; else echo "STOP! Private Key and Certificate don't match" ; fi
</code></pre>


</div>

  
<div class="section"><h3 class="title sectiontitle">Verify that the server certificate was signed by the CA</h3>


<p class="p">Extract all but the first certificate from <code class="ph codeph">cert.pem</code> (
  i.e. extract the certs for the intermediate CA authorities) into the file <code class="ph codeph">int.pem</code></p>

  
<pre class="pre codeblock"><code>sed -e '1,/-----END CERTIFICATE-----/d' cert.pem &gt;intca.pem</code></pre>  
  
<p class="p">Combine <code class="ph codeph">intca.pem</code> and <code class="ph codeph">ca.pem</code> to form <code class="ph codeph">cachain.pem</code>:</p>
  
  
<pre class="pre codeblock"><code>cat intca.pem ca.pem &gt; cachain.pem</code></pre>  
  
<p class="p">Finally, verify that <code class="ph codeph">cert.pem</code> was signed by the CA or by an intermediate CA:</p>
 
  
<pre class="pre codeblock"><code>openssl verify -verbose -CAfile cachain.pem  cert.pem</code></pre>  
  
<p class="p">A successful check will generate output similar to:</p>

  
<pre class="pre codeblock"><code>[root@ansible ucp_certs]# cat intca.pem ca.pem &gt; cachain.pem
[root@ansible ucp_certs]# openssl verify -verbose -CAfile cachain.pem  cert.pem
cert.pem: OK</code></pre> 
  
<p class="p">An unsuccessful check will generate output similar to:</p>
 
  
<pre class="pre codeblock"><code>[root@ansible ucp_certs]# openssl verify -verbose -CAfile cachain.pem  certsignedbyanotherca.pem
certsignedbyanotherca.pem: O = HPE, OU = CloudRA Team, CN = hpe-ucp.cloudra.local
error 20 at 0 depth lookup:unable to get local issuer certificate</code></pre>  


</div>
  
</div>

</div>
</div>
<div class="topic nested0" aria-labelledby="ariaid-title124" id="splunk-ssl">
<h1 class="title topictitle1" id="ariaid-title124">Appendix C: Enabling SSL between the universal forwarders and the Splunk indexers using your
certificates</h1>

<div class="body">
    
<div class="section">

 
<p class="p">The procedure for enabling SSL between the universal forwarders and the Splunk indexers using your
certificates is described below. In summary, the following steps are required:</p>


<ol class="ol">
<li class="li">Set the variable <code class="ph codeph">splunk_ssl</code> to <code class="ph codeph">yes</code> in <code class="ph codeph">group_vars/vars</code></li>

  
<li class="li">Put your root CA certificate and your server certificate files in
<pre class="pre codeblock"><code>/root/<span class="ph"><span class="keyword">Docker-SimpliVity</span></span>/ops/files/splunk/linux/SPLUNK_HOME/etc/mycerts</code></pre></li>

  
<li class="li">Uncomment the <code class="ph codeph">[sslConfig]</code> stanza in the file 
<code class="ph codeph">/files/splunk/linux/SPLUNK_HOME/etc/system/local/server.conf</code>

</li>

</ol>


</div>
    
    
    
<div class="section"><h2 class="title sectiontitle">Limitations</h2>
    


<p class="p">SSL only works with Linux worker nodes. The Universal Forwarders verify that the indexers they connect to have a certificate signed
by the configured root CA and that the Common Name in the certificate presented by the
indexer matches its FQDN as listed by the variable</p>

 
<code class="ph codeph">splunk_architecture_forward_servers</code>.

</div>


<div class="section"><h2 class="title sectiontitle">Prerequisites</h2>

    
<p class="p">Configure your indexers to use SSL on port 9998. The following is an example <code class="ph codeph">inputs.conf</code>
file located in <code class="ph codeph">$SPLUNK_HOME/etc/system/local</code> that enables SSL on port 9998 and configures
the certificate file for use by the indexer itself, in this instance <code class="ph codeph">/opt/splunk/etc/mycerts/indexer.pem</code>. </p>

    
<pre class="pre codeblock"><code>[splunktcp-ssl://9998]
disabled=0
connection_host = ip

[SSL]
serverCert=/opt/splunk/etc/mycerts/indexer.pem
#requireClientCert = true
#sslAltNameToCheck = forwarder,forwarder.cloudra.local

[tcp://1514]
connection_host = dns
sourcetype = ucp
</code></pre>


<p class="p">For more information, see the documentation at 
<a class="xref" href="https://docs.splunk.com/Documentation/Splunk/7.1.2/Security/ConfigureSplunkforwardingtousesignedcertificates" target="_blank">https://docs.splunk.com/Documentation/Splunk/7.1.2/Security/ConfigureSplunkforwardingtousesignedcertificates</a>.
In addition, you can see how to create your own certificates and
the content of the file designated with <code class="ph codeph">serverCert</code> at 
<a class="xref" href="http://docs.splunk.com/Documentation/Splunk/7.1.2/Security/Howtoself-signcertificates" target="_blank">http://docs.splunk.com/Documentation/Splunk/7.1.2/Security/Howtoself-signcertificates</a>. 
</p>

<p class="p">In this instance, the folder <code class="ph codeph">mycerts</code> was
created under <code class="ph codeph">/opt/splunk/etc</code> and the file <code class="ph codeph">indexer.pem</code>  was copied to this folder.</p>

    
    
    
<p class="p">Indexers are configured with the Root CA cert used to sign all certificates. This can
be achieved by editing the file <code class="ph codeph">server.conf</code> in <code class="ph codeph">$SPLUNK_HOME/etc/system/local</code> on your
indexer(s). The following code block shows the relevant portion of this file where
<code class="ph codeph">sssRootCaPath</code> is pointing to the root CA certificate. 
</p>


<pre class="pre codeblock"><code>[sslConfig]
sslRootCAPath = /opt/splunk/etc/mycerts/ca.pem</code></pre>
    
<hr /><div class="note note"><span class="notetitle"><b>Note:</b></span> In order to be able to download and install additional applications, you
may want to append the file <code class="ph codeph">$SPLUNK_HOME/auth/appsCA.pem</code> to your <code class="ph codeph">ca.pem</code> file. If you
don't do this, the Splunk UI will make this suggestion when you attempt to <code class="ph codeph">Find more
apps</code>. </div><hr />

 
    
<p class="p">Splunk should be restarted on the indexers if you had to make these changes (see the
Splunk documentation for more information).</p>



</div>


<div class="section"><h2 class="title sectiontitle">Before you deploy</h2>


<p class="p">Generate the forwarder certificate and  name it <code class="ph codeph">forwarder.pem</code>. Make sure that you
copy the root CA certificate to <code class="ph codeph">ca.pem</code></p>

    
<p class="p">Copy both the <code class="ph codeph">ca.pem</code> and the <code class="ph codeph">forwarder.pem</code> files to <code class="ph codeph">files/splunk/linux/SPLUNK_HOME/etc/mycerts/</code>
(overwriting any existing files).</p>

    
<p class="p">Edit the file <code class="ph codeph">server.conf</code> in the folder <code class="ph codeph">files/splunk/linux/SPLUNK_HOME/etc/system/local</code> and
uncomment the last two lines as suggested in the file itself. Your file should look like
this:
</p>


<pre class="pre codeblock"><code>#
# uncomment the section below if you want to enable SSL
#
[sslConfig]
sslRootCAPath = /opt/splunkforwarder/etc/mycerts/ca.pem</code></pre>
    
    
<p class="p">Set <code class="ph codeph">splunk_ssl</code> to <code class="ph codeph">yes</code> in the file <code class="ph codeph">group_vars/vars</code>, uncommenting the line if required.
Make sure that the <code class="ph codeph">splunk_architecture_forward_servers</code> list specifies all your indexers together with the
port that was configured to accept SSL:</p>

    
<pre class="pre codeblock"><code>monitoring_stack: splunk
splunk_ssl: yes
splunk_architecture_forward_servers:
- indexer1.cloudra.local:9998
- indexer2.cloudra.local:9998</code></pre>

</div>




<div class="section"><h2 class="title sectiontitle">Hybrid environment Linux / Windows</h2>

<p class="p">Currently, you cannot deploy your own certificates for use by the Universal Forwarders
deployed on Windows machines. If you want to have your Linux machines in a hybrid deployment to use SSL, proceed as
follows.</p>

    
    
<p class="p">Comment out the <code class="ph codeph">splunk_architecture_forward_servers</code> variable (and its values)
from <code class="ph codeph">group_vars/vars</code></p>

<pre class="pre codeblock"><code>monitoring_stack: splunk
splunk_ssl: yes
#splunk_architecture_forward_servers:
#  - hpe2-ansible.cloudra.local:9998</code></pre>

<p class="p">Create a file named <code class="ph codeph">vms.yml</code> in the folder <code class="ph codeph">group_vars</code> and specify the list of forward
servers to use by the Linux servers. This list is typically the same as the one used for
Windows servers but specifies a TCP port that enables SSL.</p>

    
<pre class="pre codeblock"><code>splunk_architecture_forward_servers:
- hpe2-ansible.cloudra.local:9998</code></pre>

    
<div class="p">Edit the <code class="ph codeph">group_vars/win_worker.yml</code> file and specify the list of forward servers to be used by the Windows servers.
 This list is typically the same as the one used for Linux servers  but specifies a TCP port that does not enable SSL.
 
<pre class="pre codeblock"><code>splunk_architecture_forward_servers:
- hpe2-ansible.cloudra.local:9997</code></pre>
</div>



</div>


</div>

</div>
<div class="topic nested0" aria-labelledby="ariaid-title125" id="post-deployment-check-certs">
<h1 class="title topictitle1" id="ariaid-title125">Appendix D: How to check that certs were deployed correctly</h1>

<div class="body">
<p class="p">The following commands should return the CA certificates used by
        UCP / DTR. This certificate is the same as the one pointed to
        by the <code class="ph codeph">--cacert</code> switch. </p>


<pre class="pre codeblock"><code># curl --cacert &lt;ucp_certs_dir&gt;/ca.pem https://&lt;your ucp fqdn&gt;/ca 
# curl --cacert &lt;dtr_certs_dir&gt;/ca.pem https://&lt;your dtr fqdn&gt;/ca
</code></pre>

<p class="p"></p>

<p class="p"><strong class="ph b">Output 1</strong>: certificates successfully deployed
        (content will depend on your own CA certificate)</p>


<pre class="pre codeblock"><code>-----BEGIN CERTIFICATE-----
MIIDyTCCArGgAwIBAgIUUeo+H6xGSB7/9gqq9T2SUwJPLggwDQYJKoZIhvcNAQEL
BQAwbDELMAkGA1UEBhMCRlIxFTATBgNVBAcTDFRoZSBJbnRlcm5ldDETMBEGA1UE
ChMKQ2hyaXN0b3BoZTEUMBIGA1UECxMLQ0EgU2VydmljZXMxGzAZBgNVBAMTEkNo
...
XkJ8WcsHocJO8J9J3RaWsM2BQc7wRntJc0kA7ooTH13OtQTP1jFcQp5xNdI4J3Mz
j9BAYERjkGqu7v9tfOem99oVGUal20pu4r73eWUm1mL948xuw6PgiRSLZrXhn/RS
uvFVnS/vPYJozOXIZA==
-----END CERTIFICATE-----</code></pre>

<p class="p">If the deployment was not
        successful, <code class="ph codeph">curl</code> will output something like <strong class="ph b">Output
                2.</strong></p>

        
<p class="p"><strong class="ph b">Output 2</strong>: certificates were not successfully
        deployed</p>


<pre class="pre codeblock"><code>
curl: (60) Peer's Certificate issuer is not recognized.
More details here: http://curl.haxx.se/docs/sslcerts.html
...
</code></pre>

<p class="p"></p>

</div>


<div class="topic nested1" aria-labelledby="ariaid-title126" id="enable-certs">
<h2 class="title topictitle2" id="ariaid-title126">Enable certs for browser (Windows 2016 example)</h2>

<div class="body">
<p class="p">Choose <code class="ph codeph">Manage computer certificates</code> in the control panel as shown in
<a class="xref" href="#enable-certs__manage-computer-certificates">Figure 33</a>
</p>

	
<div class="fig fignone" id="enable-certs__manage-computer-certificates">

<br /><img class="image" src="media/manage-computer-certificates.png" /><br />	
<div><span class="figcap"><span class="fig--title-label"><b>Figure 33. </b></span>Manage computer certificates</span></div><p> </p></div>
	

<p class="p">Import the <code class="ph codeph">ca.pem</code> for UCP into the Trusted Root Certification Authorities, as shown in
<a class="xref" href="#enable-certs__import-ca-pem">Figure 34</a>
</p>


<div class="fig fignone" id="enable-certs__import-ca-pem">
	
<br /><img class="image" src="media/import-ca-pem.png" /><br />	
<div><span class="figcap"><span class="fig--title-label"><b>Figure 34. </b></span>Import the ca.pem</span></div><p> </p></div>

	

	


<p class="p">It should now show up in the list of certificates. You may need to restart your browser to see the green, 
 secure lock symbol as shown in <a class="xref" href="#enable-certs__green-https">Figure 35</a></p>

	
<div class="fig fignone" id="enable-certs__green-https">

<br /><img class="image" src="media/green-https.png" /><br />	
<div><span class="figcap"><span class="fig--title-label"><b>Figure 35. </b></span>Secure HTTPS</span></div><p> </p></div>

	
	
</div>

</div>

  
</div>
<div class="topic nested0" aria-labelledby="ariaid-title127" id="resources-links">
<h1 class="title topictitle1" id="ariaid-title127">Resources and additional links</h1>

  
<div class="body">
<p class="p">HPE Reference Architectures, <a class="xref" href="http://hpe.com/info/ra" target="_blank">hpe.com/info/ra</a></p>



<p class="p"><span class="ph"><span class="keyword">HPE SimpliVity</span></span>, <a class="xref" href="http://hpe.com/info/simplivity" target="_blank">hpe.com/info/simplivity</a></p>
    
<p class="p">HPE Servers, <a class="xref" href="http://hpe.com/servers" target="_blank">hpe.com/servers</a></p>

<p class="p">HPE Storage, <a class="xref" href="http://hpe.com/storage" target="_blank">hpe.com/storage</a></p>

<p class="p">HPE Networking, <a class="xref" href="http://hpe.com/networking" target="_blank">hpe.com/networking</a></p>

<p class="p">HPE Technology Consulting Services, <a class="xref" href="http://hpe.com/us/en/services/consulting.html" target="_blank">hpe.com/us/en/services/consulting.html</a></p>

<p class="p">Docker Reference Architectures, <a class="xref" href="https://success.docker.com/architectures" target="_blank">https://success.docker.com/architectures</a></p>

<p class="p">Splunk Validate Architectures, <a class="xref" href="https://www.splunk.com/pdfs/white-papers/splunk-validated-architectures.pdf" target="_blank">https://www.splunk.com/pdfs/white-papers/splunk-validated-architectures.pdf</a></p>

<p class="p">Sysdig Resources, <a class="xref" href="https://sysdig.com/resources/" target="_blank">https://sysdig.com/resources/</a>    
</p>

</div>

</div>
</body>
</html>